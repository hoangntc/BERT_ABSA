{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "474c718a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJ_PATH=/home/hoang/github/BERT_ABSA\n"
     ]
    }
   ],
   "source": [
    "import os, sys, re, datetime, random, gzip, json\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from itertools import accumulate\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "\n",
    "from time import time\n",
    "from math import ceil\n",
    "from multiprocessing import Pool\n",
    "from sentence_transformers import SentenceTransformer, models, losses, InputExample\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.trainer.trainer import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "\n",
    "PROJ_PATH = Path(os.path.join(re.sub(\"/BERT_ABSA.*$\", '', os.getcwd()), 'BERT_ABSA'))\n",
    "print(f'PROJ_PATH={PROJ_PATH}')\n",
    "sys.path.insert(1, str(PROJ_PATH))\n",
    "sys.path.insert(1, str(PROJ_PATH/'src'))\n",
    "import utils\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0cd3c37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glob.glob('../model/restaurants/*.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b86824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f2bcacb",
   "metadata": {},
   "source": [
    "## XML Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5a5e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64b40ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseXML(data_path):\n",
    "    tree = ET.ElementTree(file=data_path)\n",
    "    objs = list()\n",
    "    for sentence in tree.getroot():\n",
    "        obj = dict()\n",
    "        obj['id'] = sentence.attrib['id']\n",
    "        for item in sentence:\n",
    "            if item.tag == 'text':\n",
    "                obj['text'] = item.text\n",
    "            elif item.tag == 'aspectTerms':\n",
    "                obj['aspects'] = list()\n",
    "                for aspectTerm in item:\n",
    "                    if aspectTerm.attrib['polarity'] != 'conflict':\n",
    "                        obj['aspects'].append(aspectTerm.attrib)\n",
    "            elif item.tag == 'aspectCategories':\n",
    "                obj['category'] = list()\n",
    "                for category in item:\n",
    "                    obj['category'].append(category.attrib)\n",
    "        if 'aspects' in obj and len(obj['aspects']):\n",
    "            objs.append(obj)\n",
    "    return objs\n",
    "\n",
    "def convert_to_dataframe(objs):\n",
    "    output = []\n",
    "    for sentence in objs:\n",
    "        id = sentence['id']\n",
    "        text = sentence['text']\n",
    "        aspects = sentence['aspects']\n",
    "        for aspect in aspects:\n",
    "            term = aspect['term']\n",
    "            label = aspect['polarity']\n",
    "            output.append([id, text, term, label])\n",
    "    output = sorted(output, key=lambda x: x[0])\n",
    "    df = pd.DataFrame(output, columns=['id', 'text', 'term', 'label'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4ed995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.ElementTree(file= str(PROJ_PATH/ 'dataset/raw_data' / 'Restaurants_Train.xml'))\n",
    "objs = list()\n",
    "for sentence in tree.getroot():\n",
    "    obj = dict()\n",
    "    obj['id'] = sentence.attrib['id']\n",
    "    for item in sentence:\n",
    "        if item.tag == 'text':\n",
    "            obj['text'] = item.text\n",
    "        elif item.tag == 'aspectTerms':\n",
    "            obj['aspects'] = list()\n",
    "            for aspectTerm in item:\n",
    "                if aspectTerm.attrib['polarity'] != 'conflict':\n",
    "                    obj['aspects'].append(aspectTerm.attrib)\n",
    "        elif item.tag == 'aspectCategories':\n",
    "            obj['category'] = list()\n",
    "            for category in item:\n",
    "                obj['category'].append(category.attrib)\n",
    "#     if 'aspects' in obj and len(obj['aspects']):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb95a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_files = {\n",
    "    'restaurant': {\n",
    "        'train': 'Restaurants_Train.xml',\n",
    "        'test': 'Restaurants_Test.xml',\n",
    "        'trial': 'Restaurants_Trial.xml'\n",
    "    },\n",
    "    'laptop': {\n",
    "        'train': 'Laptops_Train.xml',\n",
    "        'test': 'Laptops_Test.xml',\n",
    "        'trial': 'Laptops_Trial.xml'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2a4c280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load: /home/hoang/github/BERT_ABSA/dataset/raw_data/Restaurants_Train.xml\n",
      "Save: /home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Restaurants_Train.pkl\n",
      "\n",
      "Load: /home/hoang/github/BERT_ABSA/dataset/raw_data/Restaurants_Test.xml\n",
      "Save: /home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Restaurants_Test.pkl\n",
      "\n",
      "Load: /home/hoang/github/BERT_ABSA/dataset/raw_data/Restaurants_Trial.xml\n",
      "Save: /home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Restaurants_Trial.pkl\n",
      "\n",
      "Load: /home/hoang/github/BERT_ABSA/dataset/raw_data/Laptops_Train.xml\n",
      "Save: /home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Laptops_Train.pkl\n",
      "\n",
      "Load: /home/hoang/github/BERT_ABSA/dataset/raw_data/Laptops_Test.xml\n",
      "Save: /home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Laptops_Test.pkl\n",
      "\n",
      "Load: /home/hoang/github/BERT_ABSA/dataset/raw_data/Laptops_Trial.xml\n",
      "Save: /home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Laptops_Trial.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for dsname, fnames in dataset_files.items():\n",
    "#     for g, fname in fnames.items():\n",
    "#         input_path = str(PROJ_PATH/ 'dataset/raw_data' / fname)\n",
    "#         output_path01 = str(PROJ_PATH/ 'dataset/preprocessed_data' / fname.replace('.xml', '.pkl'))\n",
    "#         output_path02 = str(PROJ_PATH/ 'dataset/preprocessed_data' / fname.replace('.xml', '.csv'))\n",
    "#         print(f'Load: {input_path}')\n",
    "#         print(f'Save: {output_path01}\\n')\n",
    "#         objs = parseXML(input_path)\n",
    "#         df = convert_to_dataframe(objs)\n",
    "#         pd.to_pickle(objs, output_path01)\n",
    "#         df.to_csv(output_path02, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95e5d482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>term</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>The food is good, especially their more basic ...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>The food is good, especially their more basic ...</td>\n",
       "      <td>dishes</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>The food is good, especially their more basic ...</td>\n",
       "      <td>drinks</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1002</td>\n",
       "      <td>The view is spectacular, and the food is great.</td>\n",
       "      <td>view</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1002</td>\n",
       "      <td>The view is spectacular, and the food is great.</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text    term     label\n",
       "0  1000  The food is good, especially their more basic ...    food  positive\n",
       "1  1000  The food is good, especially their more basic ...  dishes  positive\n",
       "2  1000  The food is good, especially their more basic ...  drinks  positive\n",
       "3  1002    The view is spectacular, and the food is great.    view  positive\n",
       "4  1002    The view is spectacular, and the food is great.    food  positive"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Restaurants_Train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d00e8b",
   "metadata": {},
   "source": [
    "## Tree Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92b4ad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import pickle\n",
    "import tqdm\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "import re\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import json\n",
    "nlps = StanfordCoreNLP(str(PROJ_PATH / 'misc/stanford-corenlp-4.3.2'))\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba15c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_and_depparse(text):\n",
    "#     text+=' '\n",
    "#     text = re.sub(r'\\. ',' . ',text).strip()\n",
    "#     text = re.sub(r' {2,}',' ',text)\n",
    "#     nlp_properties = {\n",
    "#         'annotators': 'depparse',\n",
    "# #         'tokenize.options': 'splitHyphenated=false,normalizeParentheses=false',\n",
    "#         'tokenize.whitespace': True,  # all tokens have been tokenized before\n",
    "#         'ssplit.isOneSentence': False,\n",
    "#         'outputFormat': 'json',\n",
    "#     }\n",
    "    \n",
    "#     try:\n",
    "#         parsed = json.loads(nlps.annotate(text.strip(), nlp_properties))\n",
    "#     except:\n",
    "#         print('ewewerror')\n",
    "        \n",
    "#     parsed = parsed['sentences']\n",
    "#     tokens = []\n",
    "#     tokens_dict = {}\n",
    "#     tuples = []\n",
    "#     tmplen = 0\n",
    "#     for item in parsed:\n",
    "#         for ite in item['tokens']:\n",
    "#             tokens.extend([ite['word']])\n",
    "#             tokens_dict[ite['index']] = ite['word']\n",
    "# #         tokens.extend([ite['word'] for ite in item['tokens']])\n",
    "#         tuples.extend([\n",
    "#             (\n",
    "#                 ite['dep'],\n",
    "#                 ite['governor']-1+tmplen,\n",
    "#                 ite['dependent']-1+tmplen\n",
    "#             ) for ite in item['basicDependencies'] if ite['dep']!='ROOT'\n",
    "#         ])\n",
    "#         tmplen=len(tokens)\n",
    "        \n",
    "# #     return tokens, tokens_dict, tuples\n",
    "#     return tokens, tuples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29b11e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_depparse(text):\n",
    "    '''\n",
    "    # to_conll(10) will return the result in a format as follows:\n",
    "    # id word lemma ctag tag feats head(head's id) rel(syntactic relation)\n",
    "    # return values that is unknown will be shown as '_'\n",
    "    # tag and ctag are considered to be equal\n",
    "    '''\n",
    "    parser = CoreNLPDependencyParser(url='http://localhost:9000')\n",
    "    dep_parsed_sentence = parser.raw_parse(text)\n",
    "    deps = dep_parsed_sentence.__next__()\n",
    "    \n",
    "    lines = deps.to_conll(10).split('\\n')\n",
    "    tokens = []\n",
    "    tuples = []\n",
    "    for line in lines:\n",
    "        if line != '':\n",
    "            result = line.split('\\t')\n",
    "            # id word lemma ctag tag feats head(head's id) rel(syntactic relation)\n",
    "            tokens.append(result[1])\n",
    "            if result[7] != 'ROOT':\n",
    "                tuples.append((result[7], int(result[6])-1 , int(result[0])-1))   \n",
    "    return tokens, tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6607848",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('/home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Restaurants_Train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79ce7a62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "89dd1383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '671', 'text': 'Each table has a pot of boiling water sunken into its surface, and you get platters of thin sliced meats, various vegetables, and rice and glass noodles.', 'aspects': [{'term': 'table', 'polarity': 'neutral', 'from': '5', 'to': '10'}, {'term': 'pot of boiling water', 'polarity': 'neutral', 'from': '17', 'to': '37'}, {'term': 'meats', 'polarity': 'neutral', 'from': '99', 'to': '104'}, {'term': 'vegetables', 'polarity': 'neutral', 'from': '114', 'to': '124'}, {'term': 'rice', 'polarity': 'neutral', 'from': '130', 'to': '134'}, {'term': 'glass noodles', 'polarity': 'neutral', 'from': '139', 'to': '152'}], 'category': [{'category': 'food', 'polarity': 'neutral'}]}\n"
     ]
    }
   ],
   "source": [
    "inp = data[len(data)-1]\n",
    "print(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9279ef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = inp['text']\n",
    "text = \"I'm waiting... It's 9am now.\"\n",
    "# text = \"Hello, I'm Nguyen. I'm 29 years old.\"\n",
    "\n",
    "# text = \"This is Sam's bicycle.\"\n",
    "# text = \"My parents' friends came for dinner.\"\n",
    "# text = \"They're my parents.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "019e6f31",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'waiting',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " '9',\n",
       " '##am',\n",
       " 'now',\n",
       " '.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2ab261a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['I', \"'m\", 'waiting', '...', 'It', \"'s\", '9', 'am', 'now', '.'],\n",
       " [('nsubj', 2, 0),\n",
       "  ('aux', 2, 1),\n",
       "  ('punct', 2, 3),\n",
       "  ('nsubj', 7, 4),\n",
       "  ('cop', 7, 5),\n",
       "  ('nummod', 7, 6),\n",
       "  ('parataxis', 2, 7),\n",
       "  ('advmod', 7, 8),\n",
       "  ('punct', 2, 9)])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_depparse(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051e81cf",
   "metadata": {},
   "source": [
    "## Dep parse tree builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ddbf36a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output\n",
    "## text\n",
    "## bert token index\n",
    "## aspect term index\n",
    "## edge index of dependency graph\n",
    "#### edge index example: 1 --> 2, 2 --> 3, 3 --> 4\n",
    "#### edge_idx = [[1, 2, 3], [2, 3, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "62a22af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aspect term: redeeming\n",
    "# Corenlp: redeeming (12) --> A (16)\n",
    "# Bert: 'red','##eem','##ing' (12, 13, 14) --> A (18)\n",
    "# Edge index: [[12, 13, 14], [18, 18, 18]]\n",
    "# aspect term index [12, 13, 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6875ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep token i == '.' if token i+1 != '.'\n",
    "# all other token, trimming \".,()\", and remove if empty\n",
    "# if token[i] == \"'\" -> group to token i-1\n",
    "# if token[i] == \"'\" and token[i+1] == \"s\" | \"re\" | \"m\" | \"ve\" -> group token i and i+1\n",
    "# if token[i] starts witht \"##\" -> group with token i-1\n",
    "def process_bert_tokens(tokens, DEBUG=False):\n",
    "    \n",
    "    to_strip_chars = \".,\\(\\)\"\n",
    "    token_check_list_1 = [\"s\", \"re\", \"m\", \"ve\", \"ll\", \"d\"]\n",
    "    \n",
    "    tokens += [\"\"]\n",
    "    \n",
    "    current_token_group = []\n",
    "    output = []\n",
    "    last_token = None\n",
    "    for token_idx, token in enumerate(tokens):\n",
    "        \n",
    "        next_token = None\n",
    "        if token_idx + 1 < len(tokens):\n",
    "            next_token = tokens[token_idx + 1]\n",
    "\n",
    "        reset = True\n",
    "        if token == \"'\":\n",
    "            reset = False\n",
    "        elif token.startswith(\"##\"):\n",
    "            reset = False\n",
    "        elif last_token is not None:\n",
    "            if token in token_check_list_1:\n",
    "                reset = False\n",
    "\n",
    "        keep = True\n",
    "        if (token == '.') and (next_token is None):\n",
    "            keep = True\n",
    "        elif (token == '.') and (next_token is not None) and (next_token != '.'):\n",
    "            keep = True\n",
    "        else:\n",
    "            token = token.strip(to_strip_chars)\n",
    "            if token == \"\":\n",
    "                keep = False\n",
    "\n",
    "        if reset:\n",
    "            if len(current_token_group) > 0:\n",
    "                output += [current_token_group]\n",
    "            current_token_group = []\n",
    "        if keep:\n",
    "            if DEBUG:\n",
    "                current_token_group += [token]\n",
    "            else:\n",
    "                current_token_group += [token_idx]\n",
    "            last_token = token\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ad3fdfca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm\", 'waiting', '.', \"It's\", '9', 'am', 'now', '.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep '.', '..', '...'\n",
    "# all other token, trimming \".,()\", and remove if empty\n",
    "# if token starts with \"'\" -> group to previous token\n",
    "# if token ends with \".\" -> insert idx -1 or \"\" for DEBUG mod\n",
    "def process_core_nlp_tokens(tokens, DEBUG=False):\n",
    "    \n",
    "    to_strip_chars = \".,\\(\\)\"\n",
    "    \n",
    "    output = []\n",
    "    for token_idx, token in enumerate(tokens):\n",
    "        \n",
    "        point_stripped_token = token.strip(\".\")\n",
    "        if point_stripped_token == \"\":\n",
    "            if DEBUG:\n",
    "                output += [\".\"]\n",
    "            else: \n",
    "                output += [token_idx]\n",
    "        elif token.startswith(\"'\"):\n",
    "            if DEBUG:\n",
    "                output[-1] += token\n",
    "        else:\n",
    "            isTokenEndsWithPoint = token.endswith(\".\")\n",
    "            \n",
    "            token = token.strip(to_strip_chars)\n",
    "            if token != \"\":\n",
    "                if DEBUG:\n",
    "                    output += [token]\n",
    "                    if isTokenEndsWithPoint:\n",
    "                        output += [\"\"]\n",
    "                else: \n",
    "                    output += [token_idx]\n",
    "                    if isTokenEndsWithPoint:\n",
    "                        output += [-1]\n",
    "\n",
    "    return output\n",
    "\n",
    "process_core_nlp_tokens(['I', \"'m\", 'waiting', '...', 'It', \"'s\", '9', 'am', 'now', '.'])\n",
    "process_core_nlp_tokens(['I', \"'m\", 'waiting', '...', 'It', \"'s\", '9', 'am', 'now', '.'], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ebd9a2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm ['i', \"'\", 'm']\n",
      "waiting ['waiting']\n",
      ". ['.']\n",
      ". ['it', \"'\", 's']\n",
      "It's ['9', '##am']\n",
      "9am ['now']\n",
      "now ['.']\n"
     ]
    }
   ],
   "source": [
    "corenlp_tokens = [\"I'm\", 'waiting', '..', '.', \"It's\", '9am', 'now', '.']\n",
    "bert_tokens = ['i', \"'\", 'm', 'waiting', '.', '.', '.', 'it', \"'\", 's', '9', '##am', 'now', '.']\n",
    "processed_core_nlp_tokens = process_core_nlp_tokens(corenlp_tokens, True)\n",
    "processed_bert_tokens = process_bert_tokens(bert_tokens, True)\n",
    "for (processed_core_nlp_token, processed_bert_token) in zip(processed_core_nlp_tokens, processed_bert_tokens):\n",
    "    print(processed_core_nlp_token, processed_bert_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3412e741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1980"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f006db42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dv4'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_bert_token_whole(bert_token):\n",
    "    result = \"\"\n",
    "    for bert_sub_token in bert_token:\n",
    "        if bert_sub_token.startswith(\"##\"):\n",
    "            result += bert_sub_token[2:]\n",
    "        else:\n",
    "            result += bert_sub_token\n",
    "    return result\n",
    "\n",
    "build_bert_token_whole(['d', '##v', '##4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9702a18d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def TEST_corenlp_to_bert_mapping():\n",
    "#     match_sample_count = 0\n",
    "\n",
    "#     for sample_idx, sample in enumerate(data[:1000]):\n",
    "#         text = sample[\"text\"]\n",
    "\n",
    "#         bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#         bert_tokens = bert_tokenizer.tokenize(text)\n",
    "#         (corenlp_tokens, parse_tree_corenlp) = tokenize_and_depparse(text)\n",
    "\n",
    "#         processed_core_nlp_tokens = process_core_nlp_tokens(corenlp_tokens, True)\n",
    "#         processed_bert_tokens = process_bert_tokens(bert_tokens, True)\n",
    "\n",
    "#         total_count = max(len(processed_core_nlp_tokens), len(processed_bert_tokens))\n",
    "#         match_count = 0\n",
    "#         for (processed_core_nlp_token, processed_bert_token) in zip(processed_core_nlp_tokens, processed_bert_tokens):\n",
    "# #             print(processed_core_nlp_token, processed_bert_token)\n",
    "#             processed_bert_token_whole = build_bert_token_whole(processed_bert_token)\n",
    "            \n",
    "#             lower_corenlp_token = processed_core_nlp_token.lower()\n",
    "#             lower_bert_token = processed_bert_token_whole.lower()\n",
    "#             if lower_corenlp_token.startswith(lower_bert_token) or lower_corenlp_token.endswith(lower_bert_token) or lower_bert_token.startswith(lower_corenlp_token) or lower_bert_token.endswith(lower_corenlp_token):\n",
    "#                 match_count += 1\n",
    "\n",
    "#         ratio = match_count/total_count\n",
    "#         if ratio == 1:\n",
    "#             match_sample_count += 1\n",
    "#         else:\n",
    "#             print(text)\n",
    "            \n",
    "#         cur_ratio = match_sample_count / (sample_idx + 1)\n",
    "#         print(\"{}/{} - {}\".format(sample_idx,len(data), cur_ratio))\n",
    "\n",
    "#     print(match_sample_count / len(data))\n",
    "    \n",
    "# TEST_corenlp_to_bert_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a961825b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0, 1, 2],\n",
       " 1: [3],\n",
       " 2: [6],\n",
       " 3: [6],\n",
       " 4: [7, 8, 9],\n",
       " 5: [10, 11],\n",
       " 6: [12],\n",
       " 7: [13]}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_corenlp_to_bert_from_indexes(corenlp_processed_indexes, bert_processed_indexes):\n",
    "    output = {}\n",
    "    for (corenlp_processed_index, bert_processed_index) in zip(corenlp_processed_indexes, bert_processed_indexes):\n",
    "        output[corenlp_processed_index] = bert_processed_index\n",
    "    return output\n",
    "\n",
    "def map_corenlp_to_bert_from_indexes_2(corenlp_tokens, bert_tokens, corenlp_processed_indexes, bert_processed_indexes):\n",
    "\n",
    "    output = {}\n",
    "    \n",
    "    bert_run_idx_global = 0\n",
    "    for corenlp_idx in corenlp_processed_indexes:\n",
    "        for bert_run_idx, bert_idx_group in enumerate(bert_processed_indexes[bert_run_idx_global:]):\n",
    "        \n",
    "            corenlp_token = corenlp_tokens[corenlp_idx]\n",
    "\n",
    "            bert_token_group = map(lambda bert_idx: bert_tokens[bert_idx], bert_idx_group)\n",
    "            bert_token = build_bert_token_whole(bert_token_group)\n",
    "\n",
    "            lower_corenlp_token = corenlp_token.lower()\n",
    "            lower_bert_token = bert_token.lower()\n",
    "            if lower_corenlp_token.startswith(lower_bert_token) or lower_corenlp_token.endswith(lower_bert_token) or lower_bert_token.startswith(lower_corenlp_token) or lower_bert_token.endswith(lower_corenlp_token):\n",
    "                bert_run_idx_global = bert_run_idx + 1\n",
    "                output[corenlp_idx] = bert_idx_group\n",
    "                break;\n",
    "        \n",
    "    return output\n",
    "        \n",
    "def map_corenlp_to_bert(corenlp_tokens, bert_tokens, DEBUG=False):\n",
    "    corenlp_processed_indexes = process_core_nlp_tokens(corenlp_tokens, DEBUG)\n",
    "    bert_processed_indexes = process_bert_tokens(bert_tokens, DEBUG)\n",
    "#     return map_corenlp_to_bert_from_indexes(corenlp_processed_indexes, bert_processed_indexes)\n",
    "    return map_corenlp_to_bert_from_indexes_2(corenlp_tokens, bert_tokens, corenlp_processed_indexes, bert_processed_indexes)\n",
    "\n",
    "corenlp_tokens = [\"I'm\", 'waiting', '..', '.', \"It's\", '9am', 'now', '.']\n",
    "bert_tokens = ['i', \"'\", 'm', 'waiting', '.', '.', '.', 'it', \"'\", 's', '9', '##am', 'now', '.']\n",
    "map_corenlp_to_bert(corenlp_tokens, bert_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "035298c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: [0, 1, 2],\n",
       "  2: [3],\n",
       "  3: [6],\n",
       "  4: [7, 8, 9],\n",
       "  6: [10, 11],\n",
       "  7: [10, 11],\n",
       "  8: [12],\n",
       "  9: [13]},\n",
       " [('I', ['i', \"'\", 'm']),\n",
       "  ('waiting', ['waiting']),\n",
       "  ('...', ['.']),\n",
       "  ('It', ['it', \"'\", 's']),\n",
       "  ('9', ['9', '##am']),\n",
       "  ('am', ['9', '##am']),\n",
       "  ('now', ['now']),\n",
       "  ('.', ['.'])])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_corenlp_to_bert_map(corenlp_tokens, bert_tokens, DEBUG=True):\n",
    "    corenlp_to_bert_map = map_corenlp_to_bert(corenlp_tokens, bert_tokens, False)\n",
    "    if DEBUG:\n",
    "        output = []\n",
    "        for corenlp_idx, bert_idx_group in corenlp_to_bert_map.items():\n",
    "            corenlp_token = corenlp_tokens[corenlp_idx]\n",
    "            bert_token_group = list(map(lambda bert_idx: bert_tokens[bert_idx], bert_idx_group))\n",
    "            output += [(corenlp_token, bert_token_group)]\n",
    "        return corenlp_to_bert_map, output\n",
    "    else:\n",
    "        return corenlp_to_bert_map\n",
    "\n",
    "def build_corenlp_to_bert_map_from_text(text, DEBUG=True):\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    bert_tokens = bert_tokenizer.tokenize(text)\n",
    "    (corenlp_tokens, parse_tree_corenlp) = tokenize_and_depparse(text)\n",
    "    \n",
    "#     print(corenlp_tokens)\n",
    "#     print(bert_tokens)\n",
    "\n",
    "    return build_corenlp_to_bert_map(corenlp_tokens, bert_tokens, DEBUG)\n",
    "\n",
    "build_corenlp_to_bert_map_from_text(\n",
    "#     \"Lahore is a great place to duck into late-night when you need some really tasty food on the cheap -- you'll likely have trouble finishing the amount of food you get for FOUR DOLLARS.\", \n",
    "    \"I'm waiting ... It's 9am now.\",\n",
    "    True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "218289d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corenlp_to_bert_map_matching_rate(corenlp_tokens, bert_tokens, corenlp_to_bert_map):\n",
    "    match_count = 0\n",
    "    for corenlp_idx, bert_idx_group in corenlp_to_bert_map.items():\n",
    "        corenlp_token = corenlp_tokens[corenlp_idx]\n",
    "\n",
    "        bert_token_group = list(map(lambda bert_idx: bert_tokens[bert_idx], bert_idx_group))\n",
    "        bert_token = build_bert_token_whole(bert_token_group)\n",
    "        \n",
    "        lower_corenlp_token = corenlp_token.lower()\n",
    "        lower_bert_token = bert_token.lower()\n",
    "        if lower_corenlp_token.startswith(lower_bert_token) or lower_corenlp_token.endswith(lower_bert_token) or lower_bert_token.startswith(lower_corenlp_token) or lower_bert_token.endswith(lower_corenlp_token):\n",
    "            match_count += 1\n",
    "        \n",
    "    return match_count / len(corenlp_to_bert_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9facb0ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1980 - 1.0\n",
      "1/1980 - 1.0\n",
      "2/1980 - 1.0\n",
      "3/1980 - 1.0\n",
      "4/1980 - 1.0\n",
      "5/1980 - 1.0\n",
      "6/1980 - 1.0\n",
      "7/1980 - 1.0\n",
      "8/1980 - 1.0\n",
      "9/1980 - 1.0\n",
      "10/1980 - 1.0\n",
      "11/1980 - 1.0\n",
      "12/1980 - 1.0\n",
      "13/1980 - 1.0\n",
      "14/1980 - 1.0\n",
      "15/1980 - 1.0\n",
      "16/1980 - 1.0\n",
      "17/1980 - 1.0\n",
      "18/1980 - 1.0\n",
      "19/1980 - 1.0\n",
      "20/1980 - 1.0\n",
      "21/1980 - 1.0\n",
      "22/1980 - 1.0\n",
      "23/1980 - 1.0\n",
      "24/1980 - 1.0\n",
      "25/1980 - 1.0\n",
      "26/1980 - 1.0\n",
      "27/1980 - 1.0\n",
      "28/1980 - 1.0\n",
      "29/1980 - 1.0\n",
      "30/1980 - 1.0\n",
      "31/1980 - 1.0\n",
      "32/1980 - 1.0\n",
      "33/1980 - 1.0\n",
      "34/1980 - 1.0\n",
      "35/1980 - 1.0\n",
      "36/1980 - 1.0\n",
      "37/1980 - 1.0\n",
      "38/1980 - 1.0\n",
      "39/1980 - 1.0\n",
      "40/1980 - 1.0\n",
      "41/1980 - 1.0\n",
      "42/1980 - 1.0\n",
      "43/1980 - 1.0\n",
      "44/1980 - 1.0\n",
      "45/1980 - 1.0\n",
      "46/1980 - 1.0\n",
      "47/1980 - 1.0\n",
      "48/1980 - 1.0\n",
      "49/1980 - 1.0\n",
      "50/1980 - 1.0\n",
      "51/1980 - 1.0\n",
      "52/1980 - 1.0\n",
      "53/1980 - 1.0\n",
      "54/1980 - 1.0\n",
      "55/1980 - 1.0\n",
      "56/1980 - 1.0\n",
      "57/1980 - 1.0\n",
      "58/1980 - 1.0\n",
      "59/1980 - 1.0\n",
      "60/1980 - 1.0\n",
      "61/1980 - 1.0\n",
      "62/1980 - 1.0\n",
      "63/1980 - 1.0\n",
      "64/1980 - 1.0\n",
      "65/1980 - 1.0\n",
      "66/1980 - 1.0\n",
      "67/1980 - 1.0\n",
      "68/1980 - 1.0\n",
      "69/1980 - 1.0\n",
      "70/1980 - 1.0\n",
      "71/1980 - 1.0\n",
      "72/1980 - 1.0\n",
      "73/1980 - 1.0\n",
      "74/1980 - 1.0\n",
      "75/1980 - 1.0\n",
      "76/1980 - 1.0\n",
      "77/1980 - 1.0\n",
      "78/1980 - 1.0\n",
      "79/1980 - 1.0\n",
      "80/1980 - 1.0\n",
      "81/1980 - 1.0\n",
      "82/1980 - 1.0\n",
      "83/1980 - 1.0\n",
      "84/1980 - 1.0\n",
      "85/1980 - 1.0\n",
      "86/1980 - 1.0\n",
      "87/1980 - 1.0\n",
      "88/1980 - 1.0\n",
      "89/1980 - 1.0\n",
      "90/1980 - 1.0\n",
      "91/1980 - 1.0\n",
      "92/1980 - 1.0\n",
      "93/1980 - 1.0\n",
      "94/1980 - 1.0\n",
      "95/1980 - 1.0\n",
      "96/1980 - 1.0\n",
      "97/1980 - 1.0\n",
      "98/1980 - 1.0\n",
      "99/1980 - 1.0\n",
      "100/1980 - 1.0\n",
      "101/1980 - 1.0\n",
      "102/1980 - 1.0\n",
      "103/1980 - 1.0\n",
      "104/1980 - 1.0\n",
      "105/1980 - 1.0\n",
      "106/1980 - 1.0\n",
      "107/1980 - 1.0\n",
      "108/1980 - 1.0\n",
      "109/1980 - 1.0\n",
      "110/1980 - 1.0\n",
      "111/1980 - 1.0\n",
      "112/1980 - 1.0\n",
      "113/1980 - 1.0\n",
      "114/1980 - 1.0\n",
      "115/1980 - 1.0\n",
      "116/1980 - 1.0\n",
      "117/1980 - 1.0\n",
      "118/1980 - 1.0\n",
      "119/1980 - 1.0\n",
      "120/1980 - 1.0\n",
      "121/1980 - 1.0\n",
      "122/1980 - 1.0\n",
      "123/1980 - 1.0\n",
      "124/1980 - 1.0\n",
      "125/1980 - 1.0\n",
      "126/1980 - 1.0\n",
      "127/1980 - 1.0\n",
      "128/1980 - 1.0\n",
      "129/1980 - 1.0\n",
      "130/1980 - 1.0\n",
      "131/1980 - 1.0\n",
      "132/1980 - 1.0\n",
      "133/1980 - 1.0\n",
      "134/1980 - 1.0\n",
      "135/1980 - 1.0\n",
      "136/1980 - 1.0\n",
      "137/1980 - 1.0\n",
      "138/1980 - 1.0\n",
      "139/1980 - 1.0\n",
      "140/1980 - 1.0\n",
      "141/1980 - 1.0\n",
      "142/1980 - 1.0\n",
      "143/1980 - 1.0\n",
      "144/1980 - 1.0\n",
      "145/1980 - 1.0\n",
      "146/1980 - 1.0\n",
      "147/1980 - 1.0\n",
      "148/1980 - 1.0\n",
      "149/1980 - 1.0\n",
      "150/1980 - 1.0\n",
      "151/1980 - 1.0\n",
      "152/1980 - 1.0\n",
      "153/1980 - 1.0\n",
      "154/1980 - 1.0\n",
      "155/1980 - 1.0\n",
      "156/1980 - 1.0\n",
      "157/1980 - 1.0\n",
      "158/1980 - 1.0\n",
      "159/1980 - 1.0\n",
      "160/1980 - 1.0\n",
      "161/1980 - 1.0\n",
      "162/1980 - 1.0\n",
      "163/1980 - 1.0\n",
      "164/1980 - 1.0\n",
      "165/1980 - 1.0\n",
      "166/1980 - 1.0\n",
      "167/1980 - 1.0\n",
      "168/1980 - 1.0\n",
      "169/1980 - 1.0\n",
      "170/1980 - 1.0\n",
      "171/1980 - 1.0\n",
      "172/1980 - 1.0\n",
      "173/1980 - 1.0\n",
      "174/1980 - 1.0\n",
      "175/1980 - 1.0\n",
      "176/1980 - 1.0\n",
      "177/1980 - 1.0\n",
      "178/1980 - 1.0\n",
      "179/1980 - 1.0\n",
      "180/1980 - 1.0\n",
      "181/1980 - 1.0\n",
      "182/1980 - 1.0\n",
      "183/1980 - 1.0\n",
      "184/1980 - 1.0\n",
      "185/1980 - 1.0\n",
      "186/1980 - 1.0\n",
      "187/1980 - 1.0\n",
      "188/1980 - 1.0\n",
      "189/1980 - 1.0\n",
      "190/1980 - 1.0\n",
      "191/1980 - 1.0\n",
      "192/1980 - 1.0\n",
      "193/1980 - 1.0\n",
      "194/1980 - 1.0\n",
      "195/1980 - 1.0\n",
      "196/1980 - 1.0\n",
      "197/1980 - 1.0\n",
      "198/1980 - 1.0\n",
      "199/1980 - 1.0\n",
      "200/1980 - 1.0\n",
      "201/1980 - 1.0\n",
      "202/1980 - 1.0\n",
      "203/1980 - 1.0\n",
      "204/1980 - 1.0\n",
      "205/1980 - 1.0\n",
      "206/1980 - 1.0\n",
      "207/1980 - 1.0\n",
      "208/1980 - 1.0\n",
      "209/1980 - 1.0\n",
      "210/1980 - 1.0\n",
      "211/1980 - 1.0\n",
      "212/1980 - 1.0\n",
      "213/1980 - 1.0\n",
      "214/1980 - 1.0\n",
      "215/1980 - 1.0\n",
      "216/1980 - 1.0\n",
      "217/1980 - 1.0\n",
      "218/1980 - 1.0\n",
      "219/1980 - 1.0\n",
      "220/1980 - 1.0\n",
      "221/1980 - 1.0\n",
      "222/1980 - 1.0\n",
      "223/1980 - 1.0\n",
      "224/1980 - 1.0\n",
      "225/1980 - 1.0\n",
      "226/1980 - 1.0\n",
      "227/1980 - 1.0\n",
      "228/1980 - 1.0\n",
      "229/1980 - 1.0\n",
      "230/1980 - 1.0\n",
      "231/1980 - 1.0\n",
      "232/1980 - 1.0\n",
      "233/1980 - 1.0\n",
      "234/1980 - 1.0\n",
      "235/1980 - 1.0\n",
      "236/1980 - 1.0\n",
      "237/1980 - 1.0\n",
      "238/1980 - 1.0\n",
      "239/1980 - 1.0\n",
      "240/1980 - 1.0\n",
      "241/1980 - 1.0\n",
      "242/1980 - 1.0\n",
      "243/1980 - 1.0\n",
      "244/1980 - 1.0\n",
      "245/1980 - 1.0\n",
      "246/1980 - 1.0\n",
      "247/1980 - 1.0\n",
      "248/1980 - 1.0\n",
      "249/1980 - 1.0\n",
      "250/1980 - 1.0\n",
      "251/1980 - 1.0\n",
      "252/1980 - 1.0\n",
      "253/1980 - 1.0\n",
      "254/1980 - 1.0\n",
      "255/1980 - 1.0\n",
      "256/1980 - 1.0\n",
      "257/1980 - 1.0\n",
      "258/1980 - 1.0\n",
      "259/1980 - 1.0\n",
      "260/1980 - 1.0\n",
      "261/1980 - 1.0\n",
      "262/1980 - 1.0\n",
      "263/1980 - 1.0\n",
      "264/1980 - 1.0\n",
      "265/1980 - 1.0\n",
      "266/1980 - 1.0\n",
      "267/1980 - 1.0\n",
      "268/1980 - 1.0\n",
      "269/1980 - 1.0\n",
      "270/1980 - 1.0\n",
      "271/1980 - 1.0\n",
      "272/1980 - 1.0\n",
      "273/1980 - 1.0\n",
      "274/1980 - 1.0\n",
      "275/1980 - 1.0\n",
      "276/1980 - 1.0\n",
      "277/1980 - 1.0\n",
      "278/1980 - 1.0\n",
      "279/1980 - 1.0\n",
      "280/1980 - 1.0\n",
      "281/1980 - 1.0\n",
      "282/1980 - 1.0\n",
      "283/1980 - 1.0\n",
      "284/1980 - 1.0\n",
      "285/1980 - 1.0\n",
      "286/1980 - 1.0\n",
      "287/1980 - 1.0\n",
      "288/1980 - 1.0\n",
      "289/1980 - 1.0\n",
      "290/1980 - 1.0\n",
      "291/1980 - 1.0\n",
      "292/1980 - 1.0\n",
      "293/1980 - 1.0\n",
      "294/1980 - 1.0\n",
      "295/1980 - 1.0\n",
      "296/1980 - 1.0\n",
      "297/1980 - 1.0\n",
      "298/1980 - 1.0\n",
      "299/1980 - 1.0\n",
      "300/1980 - 1.0\n",
      "301/1980 - 1.0\n",
      "302/1980 - 1.0\n",
      "303/1980 - 1.0\n",
      "304/1980 - 1.0\n",
      "305/1980 - 1.0\n",
      "306/1980 - 1.0\n",
      "307/1980 - 1.0\n",
      "308/1980 - 1.0\n",
      "309/1980 - 1.0\n",
      "310/1980 - 1.0\n",
      "311/1980 - 1.0\n",
      "312/1980 - 1.0\n",
      "313/1980 - 1.0\n",
      "314/1980 - 1.0\n",
      "315/1980 - 1.0\n",
      "316/1980 - 1.0\n",
      "317/1980 - 1.0\n",
      "318/1980 - 1.0\n",
      "319/1980 - 1.0\n",
      "320/1980 - 1.0\n",
      "321/1980 - 1.0\n",
      "322/1980 - 1.0\n",
      "323/1980 - 1.0\n",
      "324/1980 - 1.0\n",
      "325/1980 - 1.0\n",
      "326/1980 - 1.0\n",
      "327/1980 - 1.0\n",
      "328/1980 - 1.0\n",
      "329/1980 - 1.0\n",
      "330/1980 - 1.0\n",
      "331/1980 - 1.0\n",
      "332/1980 - 1.0\n",
      "333/1980 - 1.0\n",
      "334/1980 - 1.0\n",
      "335/1980 - 1.0\n",
      "336/1980 - 1.0\n",
      "337/1980 - 1.0\n",
      "338/1980 - 1.0\n",
      "339/1980 - 1.0\n",
      "340/1980 - 1.0\n",
      "341/1980 - 1.0\n",
      "342/1980 - 1.0\n",
      "343/1980 - 1.0\n",
      "344/1980 - 1.0\n",
      "345/1980 - 1.0\n",
      "346/1980 - 1.0\n",
      "347/1980 - 1.0\n",
      "348/1980 - 1.0\n",
      "349/1980 - 1.0\n",
      "350/1980 - 1.0\n",
      "351/1980 - 1.0\n",
      "352/1980 - 1.0\n",
      "353/1980 - 1.0\n",
      "354/1980 - 1.0\n",
      "355/1980 - 1.0\n",
      "356/1980 - 1.0\n",
      "357/1980 - 1.0\n",
      "358/1980 - 1.0\n",
      "359/1980 - 1.0\n",
      "360/1980 - 1.0\n",
      "361/1980 - 1.0\n",
      "362/1980 - 1.0\n",
      "363/1980 - 1.0\n",
      "364/1980 - 1.0\n",
      "365/1980 - 1.0\n",
      "366/1980 - 1.0\n",
      "367/1980 - 1.0\n",
      "368/1980 - 1.0\n",
      "369/1980 - 1.0\n",
      "370/1980 - 1.0\n",
      "371/1980 - 1.0\n",
      "372/1980 - 1.0\n",
      "373/1980 - 1.0\n",
      "374/1980 - 1.0\n",
      "375/1980 - 1.0\n",
      "376/1980 - 1.0\n",
      "377/1980 - 1.0\n",
      "378/1980 - 1.0\n",
      "379/1980 - 1.0\n",
      "380/1980 - 1.0\n",
      "381/1980 - 1.0\n",
      "382/1980 - 1.0\n",
      "383/1980 - 1.0\n",
      "384/1980 - 1.0\n",
      "385/1980 - 1.0\n",
      "386/1980 - 1.0\n",
      "387/1980 - 1.0\n",
      "388/1980 - 1.0\n",
      "389/1980 - 1.0\n",
      "390/1980 - 1.0\n",
      "391/1980 - 1.0\n",
      "392/1980 - 1.0\n",
      "393/1980 - 1.0\n",
      "394/1980 - 1.0\n",
      "395/1980 - 1.0\n",
      "396/1980 - 1.0\n",
      "397/1980 - 1.0\n",
      "398/1980 - 1.0\n",
      "399/1980 - 1.0\n",
      "400/1980 - 1.0\n",
      "401/1980 - 1.0\n",
      "402/1980 - 1.0\n",
      "403/1980 - 1.0\n",
      "404/1980 - 1.0\n",
      "405/1980 - 1.0\n",
      "406/1980 - 1.0\n",
      "407/1980 - 1.0\n",
      "408/1980 - 1.0\n",
      "409/1980 - 1.0\n",
      "410/1980 - 1.0\n",
      "411/1980 - 1.0\n",
      "412/1980 - 1.0\n",
      "413/1980 - 1.0\n",
      "414/1980 - 1.0\n",
      "415/1980 - 1.0\n",
      "416/1980 - 1.0\n",
      "417/1980 - 1.0\n",
      "418/1980 - 1.0\n",
      "419/1980 - 1.0\n",
      "420/1980 - 1.0\n",
      "421/1980 - 1.0\n",
      "422/1980 - 1.0\n",
      "423/1980 - 1.0\n",
      "424/1980 - 1.0\n",
      "425/1980 - 1.0\n",
      "426/1980 - 1.0\n",
      "427/1980 - 1.0\n",
      "428/1980 - 1.0\n",
      "429/1980 - 1.0\n",
      "430/1980 - 1.0\n",
      "431/1980 - 1.0\n",
      "432/1980 - 1.0\n",
      "433/1980 - 1.0\n",
      "434/1980 - 1.0\n",
      "435/1980 - 1.0\n",
      "436/1980 - 1.0\n",
      "437/1980 - 1.0\n",
      "438/1980 - 1.0\n",
      "439/1980 - 1.0\n",
      "440/1980 - 1.0\n",
      "441/1980 - 1.0\n",
      "442/1980 - 1.0\n",
      "443/1980 - 1.0\n",
      "444/1980 - 1.0\n",
      "445/1980 - 1.0\n",
      "446/1980 - 1.0\n",
      "447/1980 - 1.0\n",
      "448/1980 - 1.0\n",
      "449/1980 - 1.0\n",
      "450/1980 - 1.0\n",
      "451/1980 - 1.0\n",
      "452/1980 - 1.0\n",
      "453/1980 - 1.0\n",
      "454/1980 - 1.0\n",
      "455/1980 - 1.0\n",
      "456/1980 - 1.0\n",
      "457/1980 - 1.0\n",
      "458/1980 - 1.0\n",
      "459/1980 - 1.0\n",
      "460/1980 - 1.0\n",
      "461/1980 - 1.0\n",
      "462/1980 - 1.0\n",
      "463/1980 - 1.0\n",
      "464/1980 - 1.0\n",
      "465/1980 - 1.0\n",
      "466/1980 - 1.0\n",
      "467/1980 - 1.0\n",
      "468/1980 - 1.0\n",
      "469/1980 - 1.0\n",
      "470/1980 - 1.0\n",
      "471/1980 - 1.0\n",
      "472/1980 - 1.0\n",
      "473/1980 - 1.0\n",
      "474/1980 - 1.0\n",
      "475/1980 - 1.0\n",
      "476/1980 - 1.0\n",
      "477/1980 - 1.0\n",
      "478/1980 - 1.0\n",
      "479/1980 - 1.0\n",
      "480/1980 - 1.0\n",
      "481/1980 - 1.0\n",
      "482/1980 - 1.0\n",
      "483/1980 - 1.0\n",
      "484/1980 - 1.0\n",
      "485/1980 - 1.0\n",
      "486/1980 - 1.0\n",
      "487/1980 - 1.0\n",
      "488/1980 - 1.0\n",
      "489/1980 - 1.0\n",
      "490/1980 - 1.0\n",
      "491/1980 - 1.0\n",
      "492/1980 - 1.0\n",
      "493/1980 - 1.0\n",
      "494/1980 - 1.0\n",
      "495/1980 - 1.0\n",
      "496/1980 - 1.0\n",
      "497/1980 - 1.0\n",
      "498/1980 - 1.0\n",
      "499/1980 - 1.0\n",
      "500/1980 - 1.0\n",
      "501/1980 - 1.0\n",
      "502/1980 - 1.0\n",
      "503/1980 - 1.0\n",
      "504/1980 - 1.0\n",
      "505/1980 - 1.0\n",
      "506/1980 - 1.0\n",
      "507/1980 - 1.0\n",
      "508/1980 - 1.0\n",
      "509/1980 - 1.0\n",
      "510/1980 - 1.0\n",
      "511/1980 - 1.0\n",
      "512/1980 - 1.0\n",
      "513/1980 - 1.0\n",
      "514/1980 - 1.0\n",
      "515/1980 - 1.0\n",
      "516/1980 - 1.0\n",
      "517/1980 - 1.0\n",
      "518/1980 - 1.0\n",
      "519/1980 - 1.0\n",
      "520/1980 - 1.0\n",
      "521/1980 - 1.0\n",
      "522/1980 - 1.0\n",
      "523/1980 - 1.0\n",
      "524/1980 - 1.0\n",
      "525/1980 - 1.0\n",
      "526/1980 - 1.0\n",
      "527/1980 - 1.0\n",
      "528/1980 - 1.0\n",
      "529/1980 - 1.0\n",
      "530/1980 - 1.0\n",
      "531/1980 - 1.0\n",
      "532/1980 - 1.0\n",
      "533/1980 - 1.0\n",
      "534/1980 - 1.0\n",
      "535/1980 - 1.0\n",
      "536/1980 - 1.0\n",
      "537/1980 - 1.0\n",
      "538/1980 - 1.0\n",
      "539/1980 - 1.0\n",
      "540/1980 - 1.0\n",
      "541/1980 - 1.0\n",
      "542/1980 - 1.0\n",
      "543/1980 - 1.0\n",
      "544/1980 - 1.0\n",
      "545/1980 - 1.0\n",
      "546/1980 - 1.0\n",
      "547/1980 - 1.0\n",
      "548/1980 - 1.0\n",
      "549/1980 - 1.0\n",
      "550/1980 - 1.0\n",
      "551/1980 - 1.0\n",
      "552/1980 - 1.0\n",
      "553/1980 - 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/1980 - 1.0\n",
      "555/1980 - 1.0\n",
      "556/1980 - 1.0\n",
      "557/1980 - 1.0\n",
      "558/1980 - 1.0\n",
      "559/1980 - 1.0\n",
      "560/1980 - 1.0\n",
      "561/1980 - 1.0\n",
      "562/1980 - 1.0\n",
      "563/1980 - 1.0\n",
      "564/1980 - 1.0\n",
      "565/1980 - 1.0\n",
      "566/1980 - 1.0\n",
      "567/1980 - 1.0\n",
      "568/1980 - 1.0\n",
      "569/1980 - 1.0\n",
      "570/1980 - 1.0\n",
      "571/1980 - 1.0\n",
      "572/1980 - 1.0\n",
      "573/1980 - 1.0\n",
      "574/1980 - 1.0\n",
      "575/1980 - 1.0\n",
      "576/1980 - 1.0\n",
      "577/1980 - 1.0\n",
      "578/1980 - 1.0\n",
      "579/1980 - 1.0\n",
      "580/1980 - 1.0\n",
      "581/1980 - 1.0\n",
      "582/1980 - 1.0\n",
      "583/1980 - 1.0\n",
      "584/1980 - 1.0\n",
      "585/1980 - 1.0\n",
      "586/1980 - 1.0\n",
      "587/1980 - 1.0\n",
      "588/1980 - 1.0\n",
      "589/1980 - 1.0\n",
      "590/1980 - 1.0\n",
      "591/1980 - 1.0\n",
      "592/1980 - 1.0\n",
      "593/1980 - 1.0\n",
      "594/1980 - 1.0\n",
      "595/1980 - 1.0\n",
      "596/1980 - 1.0\n",
      "597/1980 - 1.0\n",
      "598/1980 - 1.0\n",
      "599/1980 - 1.0\n",
      "600/1980 - 1.0\n",
      "601/1980 - 1.0\n",
      "602/1980 - 1.0\n",
      "603/1980 - 1.0\n",
      "604/1980 - 1.0\n",
      "605/1980 - 1.0\n",
      "606/1980 - 1.0\n",
      "607/1980 - 1.0\n",
      "608/1980 - 1.0\n",
      "609/1980 - 1.0\n",
      "610/1980 - 1.0\n",
      "611/1980 - 1.0\n",
      "612/1980 - 1.0\n",
      "613/1980 - 1.0\n",
      "614/1980 - 1.0\n",
      "615/1980 - 1.0\n",
      "616/1980 - 1.0\n",
      "617/1980 - 1.0\n",
      "618/1980 - 1.0\n",
      "619/1980 - 1.0\n",
      "620/1980 - 1.0\n",
      "621/1980 - 1.0\n",
      "622/1980 - 1.0\n",
      "623/1980 - 1.0\n",
      "624/1980 - 1.0\n",
      "625/1980 - 1.0\n",
      "626/1980 - 1.0\n",
      "627/1980 - 1.0\n",
      "628/1980 - 1.0\n",
      "629/1980 - 1.0\n",
      "630/1980 - 1.0\n",
      "631/1980 - 1.0\n",
      "632/1980 - 1.0\n",
      "633/1980 - 1.0\n",
      "634/1980 - 1.0\n",
      "635/1980 - 1.0\n",
      "636/1980 - 1.0\n",
      "637/1980 - 1.0\n",
      "638/1980 - 1.0\n",
      "639/1980 - 1.0\n",
      "640/1980 - 1.0\n",
      "641/1980 - 1.0\n",
      "642/1980 - 1.0\n",
      "643/1980 - 1.0\n",
      "644/1980 - 1.0\n",
      "645/1980 - 1.0\n",
      "646/1980 - 1.0\n",
      "647/1980 - 1.0\n",
      "648/1980 - 1.0\n",
      "649/1980 - 1.0\n",
      "650/1980 - 1.0\n",
      "651/1980 - 1.0\n",
      "652/1980 - 1.0\n",
      "653/1980 - 1.0\n",
      "654/1980 - 1.0\n",
      "655/1980 - 1.0\n",
      "656/1980 - 1.0\n",
      "657/1980 - 1.0\n",
      "658/1980 - 1.0\n",
      "659/1980 - 1.0\n",
      "660/1980 - 1.0\n",
      "661/1980 - 1.0\n",
      "662/1980 - 1.0\n",
      "663/1980 - 1.0\n",
      "664/1980 - 1.0\n",
      "665/1980 - 1.0\n",
      "666/1980 - 1.0\n",
      "667/1980 - 1.0\n",
      "668/1980 - 1.0\n",
      "669/1980 - 1.0\n",
      "670/1980 - 1.0\n",
      "671/1980 - 1.0\n",
      "672/1980 - 1.0\n",
      "673/1980 - 1.0\n",
      "674/1980 - 1.0\n",
      "675/1980 - 1.0\n",
      "676/1980 - 1.0\n",
      "677/1980 - 1.0\n",
      "678/1980 - 1.0\n",
      "679/1980 - 1.0\n",
      "680/1980 - 1.0\n",
      "681/1980 - 1.0\n",
      "682/1980 - 1.0\n",
      "683/1980 - 1.0\n",
      "684/1980 - 1.0\n",
      "685/1980 - 1.0\n",
      "686/1980 - 1.0\n",
      "687/1980 - 1.0\n",
      "688/1980 - 1.0\n",
      "689/1980 - 1.0\n",
      "690/1980 - 1.0\n",
      "691/1980 - 1.0\n",
      "692/1980 - 1.0\n",
      "693/1980 - 1.0\n",
      "694/1980 - 1.0\n",
      "695/1980 - 1.0\n",
      "696/1980 - 1.0\n",
      "697/1980 - 1.0\n",
      "698/1980 - 1.0\n",
      "699/1980 - 1.0\n",
      "700/1980 - 1.0\n",
      "701/1980 - 1.0\n",
      "702/1980 - 1.0\n",
      "703/1980 - 1.0\n",
      "704/1980 - 1.0\n",
      "705/1980 - 1.0\n",
      "706/1980 - 1.0\n",
      "707/1980 - 1.0\n",
      "708/1980 - 1.0\n",
      "709/1980 - 1.0\n",
      "710/1980 - 1.0\n",
      "711/1980 - 1.0\n",
      "712/1980 - 1.0\n",
      "713/1980 - 1.0\n",
      "714/1980 - 1.0\n",
      "715/1980 - 1.0\n",
      "716/1980 - 1.0\n",
      "717/1980 - 1.0\n",
      "718/1980 - 1.0\n",
      "719/1980 - 1.0\n",
      "720/1980 - 1.0\n",
      "721/1980 - 1.0\n",
      "722/1980 - 1.0\n",
      "723/1980 - 1.0\n",
      "724/1980 - 1.0\n",
      "725/1980 - 1.0\n",
      "726/1980 - 1.0\n",
      "727/1980 - 1.0\n",
      "728/1980 - 1.0\n",
      "729/1980 - 1.0\n",
      "730/1980 - 1.0\n",
      "731/1980 - 1.0\n",
      "732/1980 - 1.0\n",
      "733/1980 - 1.0\n",
      "734/1980 - 1.0\n",
      "735/1980 - 1.0\n",
      "736/1980 - 1.0\n",
      "737/1980 - 1.0\n",
      "738/1980 - 1.0\n",
      "739/1980 - 1.0\n",
      "740/1980 - 1.0\n",
      "741/1980 - 1.0\n",
      "742/1980 - 1.0\n",
      "743/1980 - 1.0\n",
      "744/1980 - 1.0\n",
      "745/1980 - 1.0\n",
      "746/1980 - 1.0\n",
      "747/1980 - 1.0\n",
      "748/1980 - 1.0\n",
      "749/1980 - 1.0\n",
      "750/1980 - 1.0\n",
      "751/1980 - 1.0\n",
      "752/1980 - 1.0\n",
      "753/1980 - 1.0\n",
      "754/1980 - 1.0\n",
      "755/1980 - 1.0\n",
      "756/1980 - 1.0\n",
      "757/1980 - 1.0\n",
      "758/1980 - 1.0\n",
      "759/1980 - 1.0\n",
      "760/1980 - 1.0\n",
      "761/1980 - 1.0\n",
      "762/1980 - 1.0\n",
      "763/1980 - 1.0\n",
      "764/1980 - 1.0\n",
      "765/1980 - 1.0\n",
      "766/1980 - 1.0\n",
      "767/1980 - 1.0\n",
      "768/1980 - 1.0\n",
      "769/1980 - 1.0\n",
      "770/1980 - 1.0\n",
      "771/1980 - 1.0\n",
      "772/1980 - 1.0\n",
      "773/1980 - 1.0\n",
      "774/1980 - 1.0\n",
      "775/1980 - 1.0\n",
      "776/1980 - 1.0\n",
      "777/1980 - 1.0\n",
      "778/1980 - 1.0\n",
      "779/1980 - 1.0\n",
      "780/1980 - 1.0\n",
      "781/1980 - 1.0\n",
      "782/1980 - 1.0\n",
      "783/1980 - 1.0\n",
      "784/1980 - 1.0\n",
      "785/1980 - 1.0\n",
      "786/1980 - 1.0\n",
      "787/1980 - 1.0\n",
      "788/1980 - 1.0\n",
      "789/1980 - 1.0\n",
      "790/1980 - 1.0\n",
      "791/1980 - 1.0\n",
      "792/1980 - 1.0\n",
      "793/1980 - 1.0\n",
      "794/1980 - 1.0\n",
      "795/1980 - 1.0\n",
      "796/1980 - 1.0\n",
      "797/1980 - 1.0\n",
      "798/1980 - 1.0\n",
      "799/1980 - 1.0\n",
      "800/1980 - 1.0\n",
      "801/1980 - 1.0\n",
      "802/1980 - 1.0\n",
      "803/1980 - 1.0\n",
      "804/1980 - 1.0\n",
      "805/1980 - 1.0\n",
      "806/1980 - 1.0\n",
      "807/1980 - 1.0\n",
      "808/1980 - 1.0\n",
      "809/1980 - 1.0\n",
      "810/1980 - 1.0\n",
      "811/1980 - 1.0\n",
      "812/1980 - 1.0\n",
      "813/1980 - 1.0\n",
      "814/1980 - 1.0\n",
      "815/1980 - 1.0\n",
      "816/1980 - 1.0\n",
      "817/1980 - 1.0\n",
      "818/1980 - 1.0\n",
      "819/1980 - 1.0\n",
      "820/1980 - 1.0\n",
      "821/1980 - 1.0\n",
      "822/1980 - 1.0\n",
      "823/1980 - 1.0\n",
      "824/1980 - 1.0\n",
      "825/1980 - 1.0\n",
      "826/1980 - 1.0\n",
      "827/1980 - 1.0\n",
      "828/1980 - 1.0\n",
      "829/1980 - 1.0\n",
      "830/1980 - 1.0\n",
      "831/1980 - 1.0\n",
      "832/1980 - 1.0\n",
      "833/1980 - 1.0\n",
      "834/1980 - 1.0\n",
      "835/1980 - 1.0\n",
      "836/1980 - 1.0\n",
      "837/1980 - 1.0\n",
      "838/1980 - 1.0\n",
      "839/1980 - 1.0\n",
      "840/1980 - 1.0\n",
      "841/1980 - 1.0\n",
      "842/1980 - 1.0\n",
      "843/1980 - 1.0\n",
      "844/1980 - 1.0\n",
      "845/1980 - 1.0\n",
      "846/1980 - 1.0\n",
      "847/1980 - 1.0\n",
      "848/1980 - 1.0\n",
      "849/1980 - 1.0\n",
      "850/1980 - 1.0\n",
      "851/1980 - 1.0\n",
      "852/1980 - 1.0\n",
      "853/1980 - 1.0\n",
      "854/1980 - 1.0\n",
      "855/1980 - 1.0\n",
      "856/1980 - 1.0\n",
      "857/1980 - 1.0\n",
      "858/1980 - 1.0\n",
      "859/1980 - 1.0\n",
      "860/1980 - 1.0\n",
      "861/1980 - 1.0\n",
      "862/1980 - 1.0\n",
      "863/1980 - 1.0\n",
      "864/1980 - 1.0\n",
      "865/1980 - 1.0\n",
      "866/1980 - 1.0\n",
      "867/1980 - 1.0\n",
      "868/1980 - 1.0\n",
      "869/1980 - 1.0\n",
      "870/1980 - 1.0\n",
      "871/1980 - 1.0\n",
      "872/1980 - 1.0\n",
      "873/1980 - 1.0\n",
      "874/1980 - 1.0\n",
      "875/1980 - 1.0\n",
      "876/1980 - 1.0\n",
      "877/1980 - 1.0\n",
      "878/1980 - 1.0\n",
      "879/1980 - 1.0\n",
      "880/1980 - 1.0\n",
      "881/1980 - 1.0\n",
      "882/1980 - 1.0\n",
      "883/1980 - 1.0\n",
      "884/1980 - 1.0\n",
      "885/1980 - 1.0\n",
      "886/1980 - 1.0\n",
      "887/1980 - 1.0\n",
      "888/1980 - 1.0\n",
      "889/1980 - 1.0\n",
      "890/1980 - 1.0\n",
      "891/1980 - 1.0\n",
      "892/1980 - 1.0\n",
      "893/1980 - 1.0\n",
      "894/1980 - 1.0\n",
      "895/1980 - 1.0\n",
      "896/1980 - 1.0\n",
      "897/1980 - 1.0\n",
      "898/1980 - 1.0\n",
      "899/1980 - 1.0\n",
      "900/1980 - 1.0\n",
      "901/1980 - 1.0\n",
      "902/1980 - 1.0\n",
      "903/1980 - 1.0\n",
      "904/1980 - 1.0\n",
      "905/1980 - 1.0\n",
      "906/1980 - 1.0\n",
      "907/1980 - 1.0\n",
      "908/1980 - 1.0\n",
      "909/1980 - 1.0\n",
      "910/1980 - 1.0\n",
      "911/1980 - 1.0\n",
      "912/1980 - 1.0\n",
      "913/1980 - 1.0\n",
      "914/1980 - 1.0\n",
      "915/1980 - 1.0\n",
      "916/1980 - 1.0\n",
      "917/1980 - 1.0\n",
      "918/1980 - 1.0\n",
      "919/1980 - 1.0\n",
      "920/1980 - 1.0\n",
      "921/1980 - 1.0\n",
      "922/1980 - 1.0\n",
      "923/1980 - 1.0\n",
      "924/1980 - 1.0\n",
      "925/1980 - 1.0\n",
      "926/1980 - 1.0\n",
      "927/1980 - 1.0\n",
      "928/1980 - 1.0\n",
      "929/1980 - 1.0\n",
      "930/1980 - 1.0\n",
      "931/1980 - 1.0\n",
      "932/1980 - 1.0\n",
      "933/1980 - 1.0\n",
      "934/1980 - 1.0\n",
      "935/1980 - 1.0\n",
      "936/1980 - 1.0\n",
      "937/1980 - 1.0\n",
      "938/1980 - 1.0\n",
      "939/1980 - 1.0\n",
      "940/1980 - 1.0\n",
      "941/1980 - 1.0\n",
      "942/1980 - 1.0\n",
      "943/1980 - 1.0\n",
      "944/1980 - 1.0\n",
      "945/1980 - 1.0\n",
      "946/1980 - 1.0\n",
      "947/1980 - 1.0\n",
      "948/1980 - 1.0\n",
      "949/1980 - 1.0\n",
      "950/1980 - 1.0\n",
      "951/1980 - 1.0\n",
      "952/1980 - 1.0\n",
      "953/1980 - 1.0\n",
      "954/1980 - 1.0\n",
      "955/1980 - 1.0\n",
      "956/1980 - 1.0\n",
      "957/1980 - 1.0\n",
      "958/1980 - 1.0\n",
      "959/1980 - 1.0\n",
      "960/1980 - 1.0\n",
      "961/1980 - 1.0\n",
      "962/1980 - 1.0\n",
      "963/1980 - 1.0\n",
      "964/1980 - 1.0\n",
      "965/1980 - 1.0\n",
      "966/1980 - 1.0\n",
      "967/1980 - 1.0\n",
      "968/1980 - 1.0\n",
      "969/1980 - 1.0\n",
      "970/1980 - 1.0\n",
      "971/1980 - 1.0\n",
      "972/1980 - 1.0\n",
      "973/1980 - 1.0\n",
      "974/1980 - 1.0\n",
      "975/1980 - 1.0\n",
      "976/1980 - 1.0\n",
      "977/1980 - 1.0\n",
      "978/1980 - 1.0\n",
      "979/1980 - 1.0\n",
      "980/1980 - 1.0\n",
      "981/1980 - 1.0\n",
      "982/1980 - 1.0\n",
      "983/1980 - 1.0\n",
      "984/1980 - 1.0\n",
      "985/1980 - 1.0\n",
      "986/1980 - 1.0\n",
      "987/1980 - 1.0\n",
      "988/1980 - 1.0\n",
      "989/1980 - 1.0\n",
      "990/1980 - 1.0\n",
      "991/1980 - 1.0\n",
      "992/1980 - 1.0\n",
      "993/1980 - 1.0\n",
      "994/1980 - 1.0\n",
      "995/1980 - 1.0\n",
      "996/1980 - 1.0\n",
      "997/1980 - 1.0\n",
      "998/1980 - 1.0\n",
      "999/1980 - 1.0\n",
      "1000/1980 - 1.0\n",
      "1001/1980 - 1.0\n",
      "1002/1980 - 1.0\n",
      "1003/1980 - 1.0\n",
      "1004/1980 - 1.0\n",
      "1005/1980 - 1.0\n",
      "1006/1980 - 1.0\n",
      "1007/1980 - 1.0\n",
      "1008/1980 - 1.0\n",
      "1009/1980 - 1.0\n",
      "1010/1980 - 1.0\n",
      "1011/1980 - 1.0\n",
      "1012/1980 - 1.0\n",
      "1013/1980 - 1.0\n",
      "1014/1980 - 1.0\n",
      "1015/1980 - 1.0\n",
      "1016/1980 - 1.0\n",
      "1017/1980 - 1.0\n",
      "1018/1980 - 1.0\n",
      "1019/1980 - 1.0\n",
      "1020/1980 - 1.0\n",
      "1021/1980 - 1.0\n",
      "1022/1980 - 1.0\n",
      "1023/1980 - 1.0\n",
      "1024/1980 - 1.0\n",
      "1025/1980 - 1.0\n",
      "1026/1980 - 1.0\n",
      "1027/1980 - 1.0\n",
      "1028/1980 - 1.0\n",
      "1029/1980 - 1.0\n",
      "1030/1980 - 1.0\n",
      "1031/1980 - 1.0\n",
      "1032/1980 - 1.0\n",
      "1033/1980 - 1.0\n",
      "1034/1980 - 1.0\n",
      "1035/1980 - 1.0\n",
      "1036/1980 - 1.0\n",
      "1037/1980 - 1.0\n",
      "1038/1980 - 1.0\n",
      "1039/1980 - 1.0\n",
      "1040/1980 - 1.0\n",
      "1041/1980 - 1.0\n",
      "1042/1980 - 1.0\n",
      "1043/1980 - 1.0\n",
      "1044/1980 - 1.0\n",
      "1045/1980 - 1.0\n",
      "1046/1980 - 1.0\n",
      "1047/1980 - 1.0\n",
      "1048/1980 - 1.0\n",
      "1049/1980 - 1.0\n",
      "1050/1980 - 1.0\n",
      "1051/1980 - 1.0\n",
      "1052/1980 - 1.0\n",
      "1053/1980 - 1.0\n",
      "1054/1980 - 1.0\n",
      "1055/1980 - 1.0\n",
      "1056/1980 - 1.0\n",
      "1057/1980 - 1.0\n",
      "1058/1980 - 1.0\n",
      "1059/1980 - 1.0\n",
      "1060/1980 - 1.0\n",
      "1061/1980 - 1.0\n",
      "1062/1980 - 1.0\n",
      "1063/1980 - 1.0\n",
      "1064/1980 - 1.0\n",
      "1065/1980 - 1.0\n",
      "1066/1980 - 1.0\n",
      "1067/1980 - 1.0\n",
      "1068/1980 - 1.0\n",
      "1069/1980 - 1.0\n",
      "1070/1980 - 1.0\n",
      "1071/1980 - 1.0\n",
      "1072/1980 - 1.0\n",
      "1073/1980 - 1.0\n",
      "1074/1980 - 1.0\n",
      "1075/1980 - 1.0\n",
      "1076/1980 - 1.0\n",
      "1077/1980 - 1.0\n",
      "1078/1980 - 1.0\n",
      "1079/1980 - 1.0\n",
      "1080/1980 - 1.0\n",
      "1081/1980 - 1.0\n",
      "1082/1980 - 1.0\n",
      "1083/1980 - 1.0\n",
      "1084/1980 - 1.0\n",
      "1085/1980 - 1.0\n",
      "1086/1980 - 1.0\n",
      "1087/1980 - 1.0\n",
      "1088/1980 - 1.0\n",
      "1089/1980 - 1.0\n",
      "1090/1980 - 1.0\n",
      "1091/1980 - 1.0\n",
      "1092/1980 - 1.0\n",
      "1093/1980 - 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1094/1980 - 1.0\n",
      "1095/1980 - 1.0\n",
      "1096/1980 - 1.0\n",
      "1097/1980 - 1.0\n",
      "1098/1980 - 1.0\n",
      "1099/1980 - 1.0\n",
      "1100/1980 - 1.0\n",
      "1101/1980 - 1.0\n",
      "1102/1980 - 1.0\n",
      "1103/1980 - 1.0\n",
      "1104/1980 - 1.0\n",
      "1105/1980 - 1.0\n",
      "1106/1980 - 1.0\n",
      "1107/1980 - 1.0\n",
      "1108/1980 - 1.0\n",
      "1109/1980 - 1.0\n",
      "1110/1980 - 1.0\n",
      "1111/1980 - 1.0\n",
      "1112/1980 - 1.0\n",
      "1113/1980 - 1.0\n",
      "1114/1980 - 1.0\n",
      "1115/1980 - 1.0\n",
      "1116/1980 - 1.0\n",
      "1117/1980 - 1.0\n",
      "1118/1980 - 1.0\n",
      "1119/1980 - 1.0\n",
      "1120/1980 - 1.0\n",
      "1121/1980 - 1.0\n",
      "1122/1980 - 1.0\n",
      "1123/1980 - 1.0\n",
      "1124/1980 - 1.0\n",
      "1125/1980 - 1.0\n",
      "1126/1980 - 1.0\n",
      "1127/1980 - 1.0\n",
      "1128/1980 - 1.0\n",
      "1129/1980 - 1.0\n",
      "1130/1980 - 1.0\n",
      "1131/1980 - 1.0\n",
      "1132/1980 - 1.0\n",
      "1133/1980 - 1.0\n",
      "1134/1980 - 1.0\n",
      "1135/1980 - 1.0\n",
      "1136/1980 - 1.0\n",
      "1137/1980 - 1.0\n",
      "1138/1980 - 1.0\n",
      "1139/1980 - 1.0\n",
      "1140/1980 - 1.0\n",
      "1141/1980 - 1.0\n",
      "1142/1980 - 1.0\n",
      "1143/1980 - 1.0\n",
      "1144/1980 - 1.0\n",
      "1145/1980 - 1.0\n",
      "1146/1980 - 1.0\n",
      "1147/1980 - 1.0\n",
      "1148/1980 - 1.0\n",
      "1149/1980 - 1.0\n",
      "1150/1980 - 1.0\n",
      "1151/1980 - 1.0\n",
      "1152/1980 - 1.0\n",
      "1153/1980 - 1.0\n",
      "1154/1980 - 1.0\n",
      "1155/1980 - 1.0\n",
      "1156/1980 - 1.0\n",
      "1157/1980 - 1.0\n",
      "1158/1980 - 1.0\n",
      "1159/1980 - 1.0\n",
      "1160/1980 - 1.0\n",
      "1161/1980 - 1.0\n",
      "1162/1980 - 1.0\n",
      "1163/1980 - 1.0\n",
      "1164/1980 - 1.0\n",
      "1165/1980 - 1.0\n",
      "1166/1980 - 1.0\n",
      "1167/1980 - 1.0\n",
      "1168/1980 - 1.0\n",
      "1169/1980 - 1.0\n",
      "1170/1980 - 1.0\n",
      "1171/1980 - 1.0\n",
      "1172/1980 - 1.0\n",
      "1173/1980 - 1.0\n",
      "1174/1980 - 1.0\n",
      "1175/1980 - 1.0\n",
      "1176/1980 - 1.0\n",
      "1177/1980 - 1.0\n",
      "1178/1980 - 1.0\n",
      "1179/1980 - 1.0\n",
      "1180/1980 - 1.0\n",
      "1181/1980 - 1.0\n",
      "1182/1980 - 1.0\n",
      "1183/1980 - 1.0\n",
      "1184/1980 - 1.0\n",
      "1185/1980 - 1.0\n",
      "1186/1980 - 1.0\n",
      "1187/1980 - 1.0\n",
      "1188/1980 - 1.0\n",
      "1189/1980 - 1.0\n",
      "1190/1980 - 1.0\n",
      "1191/1980 - 1.0\n",
      "1192/1980 - 1.0\n",
      "1193/1980 - 1.0\n",
      "1194/1980 - 1.0\n",
      "1195/1980 - 1.0\n",
      "1196/1980 - 1.0\n",
      "1197/1980 - 1.0\n",
      "1198/1980 - 1.0\n",
      "1199/1980 - 1.0\n",
      "1200/1980 - 1.0\n",
      "1201/1980 - 1.0\n",
      "1202/1980 - 1.0\n",
      "1203/1980 - 1.0\n",
      "1204/1980 - 1.0\n",
      "1205/1980 - 1.0\n",
      "1206/1980 - 1.0\n",
      "1207/1980 - 1.0\n",
      "1208/1980 - 1.0\n",
      "1209/1980 - 1.0\n",
      "1210/1980 - 1.0\n",
      "1211/1980 - 1.0\n",
      "1212/1980 - 1.0\n",
      "1213/1980 - 1.0\n",
      "1214/1980 - 1.0\n",
      "1215/1980 - 1.0\n",
      "1216/1980 - 1.0\n",
      "1217/1980 - 1.0\n",
      "1218/1980 - 1.0\n",
      "1219/1980 - 1.0\n",
      "1220/1980 - 1.0\n",
      "1221/1980 - 1.0\n",
      "1222/1980 - 1.0\n",
      "1223/1980 - 1.0\n",
      "1224/1980 - 1.0\n",
      "1225/1980 - 1.0\n",
      "1226/1980 - 1.0\n",
      "1227/1980 - 1.0\n",
      "1228/1980 - 1.0\n",
      "1229/1980 - 1.0\n",
      "1230/1980 - 1.0\n",
      "1231/1980 - 1.0\n",
      "1232/1980 - 1.0\n",
      "1233/1980 - 1.0\n",
      "1234/1980 - 1.0\n",
      "1235/1980 - 1.0\n",
      "1236/1980 - 1.0\n",
      "1237/1980 - 1.0\n",
      "1238/1980 - 1.0\n",
      "1239/1980 - 1.0\n",
      "1240/1980 - 1.0\n",
      "1241/1980 - 1.0\n",
      "1242/1980 - 1.0\n",
      "1243/1980 - 1.0\n",
      "1244/1980 - 1.0\n",
      "1245/1980 - 1.0\n",
      "1246/1980 - 1.0\n",
      "1247/1980 - 1.0\n",
      "1248/1980 - 1.0\n",
      "1249/1980 - 1.0\n",
      "1250/1980 - 1.0\n",
      "1251/1980 - 1.0\n",
      "1252/1980 - 1.0\n",
      "1253/1980 - 1.0\n",
      "1254/1980 - 1.0\n",
      "1255/1980 - 1.0\n",
      "1256/1980 - 1.0\n",
      "1257/1980 - 1.0\n",
      "1258/1980 - 1.0\n",
      "1259/1980 - 1.0\n",
      "1260/1980 - 1.0\n",
      "1261/1980 - 1.0\n",
      "1262/1980 - 1.0\n",
      "1263/1980 - 1.0\n",
      "1264/1980 - 1.0\n",
      "1265/1980 - 1.0\n",
      "1266/1980 - 1.0\n",
      "1267/1980 - 1.0\n",
      "1268/1980 - 1.0\n",
      "1269/1980 - 1.0\n",
      "1270/1980 - 1.0\n",
      "1271/1980 - 1.0\n",
      "1272/1980 - 1.0\n",
      "1273/1980 - 1.0\n",
      "1274/1980 - 1.0\n",
      "1275/1980 - 1.0\n",
      "1276/1980 - 1.0\n",
      "1277/1980 - 1.0\n",
      "1278/1980 - 1.0\n",
      "1279/1980 - 1.0\n",
      "1280/1980 - 1.0\n",
      "1281/1980 - 1.0\n",
      "1282/1980 - 1.0\n",
      "1283/1980 - 1.0\n",
      "1284/1980 - 1.0\n",
      "1285/1980 - 1.0\n",
      "1286/1980 - 1.0\n",
      "1287/1980 - 1.0\n",
      "1288/1980 - 1.0\n",
      "1289/1980 - 1.0\n",
      "1290/1980 - 1.0\n",
      "1291/1980 - 1.0\n",
      "1292/1980 - 1.0\n",
      "1293/1980 - 1.0\n",
      "1294/1980 - 1.0\n",
      "1295/1980 - 1.0\n",
      "1296/1980 - 1.0\n",
      "1297/1980 - 1.0\n",
      "1298/1980 - 1.0\n",
      "1299/1980 - 1.0\n",
      "1300/1980 - 1.0\n",
      "1301/1980 - 1.0\n",
      "1302/1980 - 1.0\n",
      "1303/1980 - 1.0\n",
      "1304/1980 - 1.0\n",
      "1305/1980 - 1.0\n",
      "1306/1980 - 1.0\n",
      "1307/1980 - 1.0\n",
      "1308/1980 - 1.0\n",
      "1309/1980 - 1.0\n",
      "1310/1980 - 1.0\n",
      "1311/1980 - 1.0\n",
      "1312/1980 - 1.0\n",
      "1313/1980 - 1.0\n",
      "1314/1980 - 1.0\n",
      "1315/1980 - 1.0\n",
      "1316/1980 - 1.0\n",
      "1317/1980 - 1.0\n",
      "1318/1980 - 1.0\n",
      "1319/1980 - 1.0\n",
      "1320/1980 - 1.0\n",
      "1321/1980 - 1.0\n",
      "1322/1980 - 1.0\n",
      "1323/1980 - 1.0\n",
      "1324/1980 - 1.0\n",
      "1325/1980 - 1.0\n",
      "1326/1980 - 1.0\n",
      "1327/1980 - 1.0\n",
      "1328/1980 - 1.0\n",
      "1329/1980 - 1.0\n",
      "1330/1980 - 1.0\n",
      "1331/1980 - 1.0\n",
      "1332/1980 - 1.0\n",
      "1333/1980 - 1.0\n",
      "1334/1980 - 1.0\n",
      "1335/1980 - 1.0\n",
      "1336/1980 - 1.0\n",
      "1337/1980 - 1.0\n",
      "1338/1980 - 1.0\n",
      "1339/1980 - 1.0\n",
      "1340/1980 - 1.0\n",
      "1341/1980 - 1.0\n",
      "1342/1980 - 1.0\n",
      "1343/1980 - 1.0\n",
      "1344/1980 - 1.0\n",
      "1345/1980 - 1.0\n",
      "1346/1980 - 1.0\n",
      "1347/1980 - 1.0\n",
      "1348/1980 - 1.0\n",
      "1349/1980 - 1.0\n",
      "1350/1980 - 1.0\n",
      "1351/1980 - 1.0\n",
      "1352/1980 - 1.0\n",
      "1353/1980 - 1.0\n",
      "1354/1980 - 1.0\n",
      "1355/1980 - 1.0\n",
      "1356/1980 - 1.0\n",
      "1357/1980 - 1.0\n",
      "1358/1980 - 1.0\n",
      "1359/1980 - 1.0\n",
      "1360/1980 - 1.0\n",
      "1361/1980 - 1.0\n",
      "1362/1980 - 1.0\n",
      "1363/1980 - 1.0\n",
      "1364/1980 - 1.0\n",
      "1365/1980 - 1.0\n",
      "1366/1980 - 1.0\n",
      "1367/1980 - 1.0\n",
      "1368/1980 - 1.0\n",
      "1369/1980 - 1.0\n",
      "1370/1980 - 1.0\n",
      "1371/1980 - 1.0\n",
      "1372/1980 - 1.0\n",
      "1373/1980 - 1.0\n",
      "1374/1980 - 1.0\n",
      "1375/1980 - 1.0\n",
      "1376/1980 - 1.0\n",
      "1377/1980 - 1.0\n",
      "1378/1980 - 1.0\n",
      "1379/1980 - 1.0\n",
      "1380/1980 - 1.0\n",
      "1381/1980 - 1.0\n",
      "1382/1980 - 1.0\n",
      "1383/1980 - 1.0\n",
      "1384/1980 - 1.0\n",
      "1385/1980 - 1.0\n",
      "1386/1980 - 1.0\n",
      "1387/1980 - 1.0\n",
      "1388/1980 - 1.0\n",
      "1389/1980 - 1.0\n",
      "1390/1980 - 1.0\n",
      "1391/1980 - 1.0\n",
      "1392/1980 - 1.0\n",
      "1393/1980 - 1.0\n",
      "1394/1980 - 1.0\n",
      "1395/1980 - 1.0\n",
      "1396/1980 - 1.0\n",
      "1397/1980 - 1.0\n",
      "1398/1980 - 1.0\n",
      "1399/1980 - 1.0\n",
      "1400/1980 - 1.0\n",
      "1401/1980 - 1.0\n",
      "1402/1980 - 1.0\n",
      "1403/1980 - 1.0\n",
      "1404/1980 - 1.0\n",
      "1405/1980 - 1.0\n",
      "1406/1980 - 1.0\n",
      "1407/1980 - 1.0\n",
      "1408/1980 - 1.0\n",
      "1409/1980 - 1.0\n",
      "1410/1980 - 1.0\n",
      "1411/1980 - 1.0\n",
      "1412/1980 - 1.0\n",
      "1413/1980 - 1.0\n",
      "1414/1980 - 1.0\n",
      "1415/1980 - 1.0\n",
      "1416/1980 - 1.0\n",
      "1417/1980 - 1.0\n",
      "1418/1980 - 1.0\n",
      "1419/1980 - 1.0\n",
      "1420/1980 - 1.0\n",
      "1421/1980 - 1.0\n",
      "1422/1980 - 1.0\n",
      "1423/1980 - 1.0\n",
      "1424/1980 - 1.0\n",
      "1425/1980 - 1.0\n",
      "1426/1980 - 1.0\n",
      "1427/1980 - 1.0\n",
      "1428/1980 - 1.0\n",
      "1429/1980 - 1.0\n",
      "1430/1980 - 1.0\n",
      "1431/1980 - 1.0\n",
      "1432/1980 - 1.0\n",
      "1433/1980 - 1.0\n",
      "1434/1980 - 1.0\n",
      "1435/1980 - 1.0\n",
      "1436/1980 - 1.0\n",
      "1437/1980 - 1.0\n",
      "1438/1980 - 1.0\n",
      "1439/1980 - 1.0\n",
      "1440/1980 - 1.0\n",
      "1441/1980 - 1.0\n",
      "1442/1980 - 1.0\n",
      "1443/1980 - 1.0\n",
      "1444/1980 - 1.0\n",
      "1445/1980 - 1.0\n",
      "1446/1980 - 1.0\n",
      "1447/1980 - 1.0\n",
      "1448/1980 - 1.0\n",
      "1449/1980 - 1.0\n",
      "1450/1980 - 1.0\n",
      "1451/1980 - 1.0\n",
      "1452/1980 - 1.0\n",
      "1453/1980 - 1.0\n",
      "1454/1980 - 1.0\n",
      "1455/1980 - 1.0\n",
      "1456/1980 - 1.0\n",
      "1457/1980 - 1.0\n",
      "1458/1980 - 1.0\n",
      "1459/1980 - 1.0\n",
      "1460/1980 - 1.0\n",
      "1461/1980 - 1.0\n",
      "1462/1980 - 1.0\n",
      "1463/1980 - 1.0\n",
      "1464/1980 - 1.0\n",
      "1465/1980 - 1.0\n",
      "1466/1980 - 1.0\n",
      "1467/1980 - 1.0\n",
      "1468/1980 - 1.0\n",
      "1469/1980 - 1.0\n",
      "1470/1980 - 1.0\n",
      "1471/1980 - 1.0\n",
      "1472/1980 - 1.0\n",
      "1473/1980 - 1.0\n",
      "1474/1980 - 1.0\n",
      "1475/1980 - 1.0\n",
      "1476/1980 - 1.0\n",
      "1477/1980 - 1.0\n",
      "1478/1980 - 1.0\n",
      "1479/1980 - 1.0\n",
      "1480/1980 - 1.0\n",
      "1481/1980 - 1.0\n",
      "1482/1980 - 1.0\n",
      "1483/1980 - 1.0\n",
      "1484/1980 - 1.0\n",
      "1485/1980 - 1.0\n",
      "1486/1980 - 1.0\n",
      "1487/1980 - 1.0\n",
      "1488/1980 - 1.0\n",
      "1489/1980 - 1.0\n",
      "1490/1980 - 1.0\n",
      "1491/1980 - 1.0\n",
      "1492/1980 - 1.0\n",
      "1493/1980 - 1.0\n",
      "1494/1980 - 1.0\n",
      "1495/1980 - 1.0\n",
      "1496/1980 - 1.0\n",
      "1497/1980 - 1.0\n",
      "1498/1980 - 1.0\n",
      "1499/1980 - 1.0\n",
      "1500/1980 - 1.0\n",
      "1501/1980 - 1.0\n",
      "1502/1980 - 1.0\n",
      "1503/1980 - 1.0\n",
      "1504/1980 - 1.0\n",
      "1505/1980 - 1.0\n",
      "1506/1980 - 1.0\n",
      "1507/1980 - 1.0\n",
      "1508/1980 - 1.0\n",
      "1509/1980 - 1.0\n",
      "1510/1980 - 1.0\n",
      "1511/1980 - 1.0\n",
      "1512/1980 - 1.0\n",
      "1513/1980 - 1.0\n",
      "1514/1980 - 1.0\n",
      "1515/1980 - 1.0\n",
      "1516/1980 - 1.0\n",
      "1517/1980 - 1.0\n",
      "1518/1980 - 1.0\n",
      "1519/1980 - 1.0\n",
      "1520/1980 - 1.0\n",
      "1521/1980 - 1.0\n",
      "1522/1980 - 1.0\n",
      "1523/1980 - 1.0\n",
      "1524/1980 - 1.0\n",
      "1525/1980 - 1.0\n",
      "1526/1980 - 1.0\n",
      "1527/1980 - 1.0\n",
      "1528/1980 - 1.0\n",
      "1529/1980 - 1.0\n",
      "1530/1980 - 1.0\n",
      "1531/1980 - 1.0\n",
      "1532/1980 - 1.0\n",
      "1533/1980 - 1.0\n",
      "1534/1980 - 1.0\n",
      "1535/1980 - 1.0\n",
      "1536/1980 - 1.0\n",
      "1537/1980 - 1.0\n",
      "1538/1980 - 1.0\n",
      "1539/1980 - 1.0\n",
      "1540/1980 - 1.0\n",
      "1541/1980 - 1.0\n",
      "1542/1980 - 1.0\n",
      "1543/1980 - 1.0\n",
      "1544/1980 - 1.0\n",
      "1545/1980 - 1.0\n",
      "1546/1980 - 1.0\n",
      "1547/1980 - 1.0\n",
      "1548/1980 - 1.0\n",
      "1549/1980 - 1.0\n",
      "1550/1980 - 1.0\n",
      "1551/1980 - 1.0\n",
      "1552/1980 - 1.0\n",
      "1553/1980 - 1.0\n",
      "1554/1980 - 1.0\n",
      "1555/1980 - 1.0\n",
      "1556/1980 - 1.0\n",
      "1557/1980 - 1.0\n",
      "1558/1980 - 1.0\n",
      "1559/1980 - 1.0\n",
      "1560/1980 - 1.0\n",
      "1561/1980 - 1.0\n",
      "1562/1980 - 1.0\n",
      "1563/1980 - 1.0\n",
      "1564/1980 - 1.0\n",
      "1565/1980 - 1.0\n",
      "1566/1980 - 1.0\n",
      "1567/1980 - 1.0\n",
      "1568/1980 - 1.0\n",
      "1569/1980 - 1.0\n",
      "1570/1980 - 1.0\n",
      "1571/1980 - 1.0\n",
      "1572/1980 - 1.0\n",
      "1573/1980 - 1.0\n",
      "1574/1980 - 1.0\n",
      "1575/1980 - 1.0\n",
      "1576/1980 - 1.0\n",
      "1577/1980 - 1.0\n",
      "1578/1980 - 1.0\n",
      "1579/1980 - 1.0\n",
      "1580/1980 - 1.0\n",
      "1581/1980 - 1.0\n",
      "1582/1980 - 1.0\n",
      "1583/1980 - 1.0\n",
      "1584/1980 - 1.0\n",
      "1585/1980 - 1.0\n",
      "1586/1980 - 1.0\n",
      "1587/1980 - 1.0\n",
      "1588/1980 - 1.0\n",
      "1589/1980 - 1.0\n",
      "1590/1980 - 1.0\n",
      "1591/1980 - 1.0\n",
      "1592/1980 - 1.0\n",
      "1593/1980 - 1.0\n",
      "1594/1980 - 1.0\n",
      "1595/1980 - 1.0\n",
      "1596/1980 - 1.0\n",
      "1597/1980 - 1.0\n",
      "1598/1980 - 1.0\n",
      "1599/1980 - 1.0\n",
      "1600/1980 - 1.0\n",
      "1601/1980 - 1.0\n",
      "1602/1980 - 1.0\n",
      "1603/1980 - 1.0\n",
      "1604/1980 - 1.0\n",
      "1605/1980 - 1.0\n",
      "1606/1980 - 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1607/1980 - 1.0\n",
      "1608/1980 - 1.0\n",
      "1609/1980 - 1.0\n",
      "1610/1980 - 1.0\n",
      "1611/1980 - 1.0\n",
      "1612/1980 - 1.0\n",
      "1613/1980 - 1.0\n",
      "1614/1980 - 1.0\n",
      "1615/1980 - 1.0\n",
      "1616/1980 - 1.0\n",
      "1617/1980 - 1.0\n",
      "1618/1980 - 1.0\n",
      "1619/1980 - 1.0\n",
      "1620/1980 - 1.0\n",
      "1621/1980 - 1.0\n",
      "1622/1980 - 1.0\n",
      "1623/1980 - 1.0\n",
      "1624/1980 - 1.0\n",
      "1625/1980 - 1.0\n",
      "1626/1980 - 1.0\n",
      "1627/1980 - 1.0\n",
      "1628/1980 - 1.0\n",
      "1629/1980 - 1.0\n",
      "1630/1980 - 1.0\n",
      "1631/1980 - 1.0\n",
      "1632/1980 - 1.0\n",
      "1633/1980 - 1.0\n",
      "1634/1980 - 1.0\n",
      "1635/1980 - 1.0\n",
      "1636/1980 - 1.0\n",
      "1637/1980 - 1.0\n",
      "1638/1980 - 1.0\n",
      "1639/1980 - 1.0\n",
      "1640/1980 - 1.0\n",
      "1641/1980 - 1.0\n",
      "1642/1980 - 1.0\n",
      "1643/1980 - 1.0\n",
      "1644/1980 - 1.0\n",
      "1645/1980 - 1.0\n",
      "1646/1980 - 1.0\n",
      "1647/1980 - 1.0\n",
      "1648/1980 - 1.0\n",
      "1649/1980 - 1.0\n",
      "1650/1980 - 1.0\n",
      "1651/1980 - 1.0\n",
      "1652/1980 - 1.0\n",
      "1653/1980 - 1.0\n",
      "1654/1980 - 1.0\n",
      "1655/1980 - 1.0\n",
      "1656/1980 - 1.0\n",
      "1657/1980 - 1.0\n",
      "1658/1980 - 1.0\n",
      "1659/1980 - 1.0\n",
      "1660/1980 - 1.0\n",
      "1661/1980 - 1.0\n",
      "1662/1980 - 1.0\n",
      "1663/1980 - 1.0\n",
      "1664/1980 - 1.0\n",
      "1665/1980 - 1.0\n",
      "1666/1980 - 1.0\n",
      "1667/1980 - 1.0\n",
      "1668/1980 - 1.0\n",
      "1669/1980 - 1.0\n",
      "1670/1980 - 1.0\n",
      "1671/1980 - 1.0\n",
      "1672/1980 - 1.0\n",
      "1673/1980 - 1.0\n",
      "1674/1980 - 1.0\n",
      "1675/1980 - 1.0\n",
      "1676/1980 - 1.0\n",
      "1677/1980 - 1.0\n",
      "1678/1980 - 1.0\n",
      "1679/1980 - 1.0\n",
      "1680/1980 - 1.0\n",
      "1681/1980 - 1.0\n",
      "1682/1980 - 1.0\n",
      "1683/1980 - 1.0\n",
      "1684/1980 - 1.0\n",
      "1685/1980 - 1.0\n",
      "1686/1980 - 1.0\n",
      "1687/1980 - 1.0\n",
      "1688/1980 - 1.0\n",
      "1689/1980 - 1.0\n",
      "1690/1980 - 1.0\n",
      "1691/1980 - 1.0\n",
      "1692/1980 - 1.0\n",
      "1693/1980 - 1.0\n",
      "1694/1980 - 1.0\n",
      "1695/1980 - 1.0\n",
      "1696/1980 - 1.0\n",
      "1697/1980 - 1.0\n",
      "1698/1980 - 1.0\n",
      "1699/1980 - 1.0\n",
      "1700/1980 - 1.0\n",
      "1701/1980 - 1.0\n",
      "1702/1980 - 1.0\n",
      "1703/1980 - 1.0\n",
      "1704/1980 - 1.0\n",
      "1705/1980 - 1.0\n",
      "1706/1980 - 1.0\n",
      "1707/1980 - 1.0\n",
      "1708/1980 - 1.0\n",
      "1709/1980 - 1.0\n",
      "1710/1980 - 1.0\n",
      "1711/1980 - 1.0\n",
      "1712/1980 - 1.0\n",
      "1713/1980 - 1.0\n",
      "1714/1980 - 1.0\n",
      "1715/1980 - 1.0\n",
      "1716/1980 - 1.0\n",
      "1717/1980 - 1.0\n",
      "1718/1980 - 1.0\n",
      "1719/1980 - 1.0\n",
      "1720/1980 - 1.0\n",
      "1721/1980 - 1.0\n",
      "1722/1980 - 1.0\n",
      "1723/1980 - 1.0\n",
      "1724/1980 - 1.0\n",
      "1725/1980 - 1.0\n",
      "1726/1980 - 1.0\n",
      "1727/1980 - 1.0\n",
      "1728/1980 - 1.0\n",
      "1729/1980 - 1.0\n",
      "1730/1980 - 1.0\n",
      "1731/1980 - 1.0\n",
      "1732/1980 - 1.0\n",
      "1733/1980 - 1.0\n",
      "1734/1980 - 1.0\n",
      "1735/1980 - 1.0\n",
      "1736/1980 - 1.0\n",
      "1737/1980 - 1.0\n",
      "1738/1980 - 1.0\n",
      "1739/1980 - 1.0\n",
      "1740/1980 - 1.0\n",
      "1741/1980 - 1.0\n",
      "1742/1980 - 1.0\n",
      "1743/1980 - 1.0\n",
      "1744/1980 - 1.0\n",
      "1745/1980 - 1.0\n",
      "1746/1980 - 1.0\n",
      "1747/1980 - 1.0\n",
      "1748/1980 - 1.0\n",
      "1749/1980 - 1.0\n",
      "1750/1980 - 1.0\n",
      "1751/1980 - 1.0\n",
      "1752/1980 - 1.0\n",
      "1753/1980 - 1.0\n",
      "1754/1980 - 1.0\n",
      "1755/1980 - 1.0\n",
      "1756/1980 - 1.0\n",
      "1757/1980 - 1.0\n",
      "1758/1980 - 1.0\n",
      "1759/1980 - 1.0\n",
      "1760/1980 - 1.0\n",
      "1761/1980 - 1.0\n",
      "1762/1980 - 1.0\n",
      "1763/1980 - 1.0\n",
      "1764/1980 - 1.0\n",
      "1765/1980 - 1.0\n",
      "1766/1980 - 1.0\n",
      "1767/1980 - 1.0\n",
      "1768/1980 - 1.0\n",
      "1769/1980 - 1.0\n",
      "1770/1980 - 1.0\n",
      "1771/1980 - 1.0\n",
      "1772/1980 - 1.0\n",
      "1773/1980 - 1.0\n",
      "1774/1980 - 1.0\n",
      "1775/1980 - 1.0\n",
      "1776/1980 - 1.0\n",
      "1777/1980 - 1.0\n",
      "1778/1980 - 1.0\n",
      "1779/1980 - 1.0\n",
      "1780/1980 - 1.0\n",
      "1781/1980 - 1.0\n",
      "1782/1980 - 1.0\n",
      "1783/1980 - 1.0\n",
      "1784/1980 - 1.0\n",
      "1785/1980 - 1.0\n",
      "1786/1980 - 1.0\n",
      "1787/1980 - 1.0\n",
      "1788/1980 - 1.0\n",
      "1789/1980 - 1.0\n",
      "1790/1980 - 1.0\n",
      "1791/1980 - 1.0\n",
      "1792/1980 - 1.0\n",
      "1793/1980 - 1.0\n",
      "1794/1980 - 1.0\n",
      "1795/1980 - 1.0\n",
      "1796/1980 - 1.0\n",
      "1797/1980 - 1.0\n",
      "1798/1980 - 1.0\n",
      "1799/1980 - 1.0\n",
      "1800/1980 - 1.0\n",
      "1801/1980 - 1.0\n",
      "1802/1980 - 1.0\n",
      "1803/1980 - 1.0\n",
      "1804/1980 - 1.0\n",
      "1805/1980 - 1.0\n",
      "1806/1980 - 1.0\n",
      "1807/1980 - 1.0\n",
      "1808/1980 - 1.0\n",
      "1809/1980 - 1.0\n",
      "1810/1980 - 1.0\n",
      "1811/1980 - 1.0\n",
      "1812/1980 - 1.0\n",
      "1813/1980 - 1.0\n",
      "1814/1980 - 1.0\n",
      "1815/1980 - 1.0\n",
      "1816/1980 - 1.0\n",
      "1817/1980 - 1.0\n",
      "1818/1980 - 1.0\n",
      "1819/1980 - 1.0\n",
      "1820/1980 - 1.0\n",
      "1821/1980 - 1.0\n",
      "1822/1980 - 1.0\n",
      "1823/1980 - 1.0\n",
      "1824/1980 - 1.0\n",
      "1825/1980 - 1.0\n",
      "1826/1980 - 1.0\n",
      "1827/1980 - 1.0\n",
      "1828/1980 - 1.0\n",
      "1829/1980 - 1.0\n",
      "1830/1980 - 1.0\n",
      "1831/1980 - 1.0\n",
      "1832/1980 - 1.0\n",
      "1833/1980 - 1.0\n",
      "1834/1980 - 1.0\n",
      "1835/1980 - 1.0\n",
      "1836/1980 - 1.0\n",
      "1837/1980 - 1.0\n",
      "1838/1980 - 1.0\n",
      "1839/1980 - 1.0\n",
      "1840/1980 - 1.0\n",
      "1841/1980 - 1.0\n",
      "1842/1980 - 1.0\n",
      "1843/1980 - 1.0\n",
      "1844/1980 - 1.0\n",
      "1845/1980 - 1.0\n",
      "1846/1980 - 1.0\n",
      "1847/1980 - 1.0\n",
      "1848/1980 - 1.0\n",
      "1849/1980 - 1.0\n",
      "1850/1980 - 1.0\n",
      "1851/1980 - 1.0\n",
      "1852/1980 - 1.0\n",
      "1853/1980 - 1.0\n",
      "1854/1980 - 1.0\n",
      "1855/1980 - 1.0\n",
      "1856/1980 - 1.0\n",
      "1857/1980 - 1.0\n",
      "1858/1980 - 1.0\n",
      "1859/1980 - 1.0\n",
      "1860/1980 - 1.0\n",
      "1861/1980 - 1.0\n",
      "1862/1980 - 1.0\n",
      "1863/1980 - 1.0\n",
      "1864/1980 - 1.0\n",
      "1865/1980 - 1.0\n",
      "1866/1980 - 1.0\n",
      "1867/1980 - 1.0\n",
      "1868/1980 - 1.0\n",
      "1869/1980 - 1.0\n",
      "1870/1980 - 1.0\n",
      "1871/1980 - 1.0\n",
      "1872/1980 - 1.0\n",
      "1873/1980 - 1.0\n",
      "1874/1980 - 1.0\n",
      "1875/1980 - 1.0\n",
      "1876/1980 - 1.0\n",
      "1877/1980 - 1.0\n",
      "1878/1980 - 1.0\n",
      "1879/1980 - 1.0\n",
      "1880/1980 - 1.0\n",
      "1881/1980 - 1.0\n",
      "1882/1980 - 1.0\n",
      "1883/1980 - 1.0\n",
      "1884/1980 - 1.0\n",
      "1885/1980 - 1.0\n",
      "1886/1980 - 1.0\n",
      "1887/1980 - 1.0\n",
      "1888/1980 - 1.0\n",
      "1889/1980 - 1.0\n",
      "1890/1980 - 1.0\n",
      "1891/1980 - 1.0\n",
      "1892/1980 - 1.0\n",
      "1893/1980 - 1.0\n",
      "1894/1980 - 1.0\n",
      "1895/1980 - 1.0\n",
      "1896/1980 - 1.0\n",
      "1897/1980 - 1.0\n",
      "1898/1980 - 1.0\n",
      "1899/1980 - 1.0\n",
      "1900/1980 - 1.0\n",
      "1901/1980 - 1.0\n",
      "1902/1980 - 1.0\n",
      "1903/1980 - 1.0\n",
      "1904/1980 - 1.0\n",
      "1905/1980 - 1.0\n",
      "1906/1980 - 1.0\n",
      "1907/1980 - 1.0\n",
      "1908/1980 - 1.0\n",
      "1909/1980 - 1.0\n",
      "1910/1980 - 1.0\n",
      "1911/1980 - 1.0\n",
      "1912/1980 - 1.0\n",
      "1913/1980 - 1.0\n",
      "1914/1980 - 1.0\n",
      "1915/1980 - 1.0\n",
      "1916/1980 - 1.0\n",
      "1917/1980 - 1.0\n",
      "1918/1980 - 1.0\n",
      "1919/1980 - 1.0\n",
      "1920/1980 - 1.0\n",
      "1921/1980 - 1.0\n",
      "1922/1980 - 1.0\n",
      "1923/1980 - 1.0\n",
      "1924/1980 - 1.0\n",
      "1925/1980 - 1.0\n",
      "1926/1980 - 1.0\n",
      "1927/1980 - 1.0\n",
      "1928/1980 - 1.0\n",
      "1929/1980 - 1.0\n",
      "1930/1980 - 1.0\n",
      "1931/1980 - 1.0\n",
      "1932/1980 - 1.0\n",
      "1933/1980 - 1.0\n",
      "1934/1980 - 1.0\n",
      "1935/1980 - 1.0\n",
      "1936/1980 - 1.0\n",
      "1937/1980 - 1.0\n",
      "1938/1980 - 1.0\n",
      "1939/1980 - 1.0\n",
      "1940/1980 - 1.0\n",
      "1941/1980 - 1.0\n",
      "1942/1980 - 1.0\n",
      "1943/1980 - 1.0\n",
      "1944/1980 - 1.0\n",
      "1945/1980 - 1.0\n",
      "1946/1980 - 1.0\n",
      "1947/1980 - 1.0\n",
      "1948/1980 - 1.0\n",
      "1949/1980 - 1.0\n",
      "1950/1980 - 1.0\n",
      "1951/1980 - 1.0\n",
      "1952/1980 - 1.0\n",
      "1953/1980 - 1.0\n",
      "1954/1980 - 1.0\n",
      "1955/1980 - 1.0\n",
      "1956/1980 - 1.0\n",
      "1957/1980 - 1.0\n",
      "1958/1980 - 1.0\n",
      "1959/1980 - 1.0\n",
      "1960/1980 - 1.0\n",
      "1961/1980 - 1.0\n",
      "1962/1980 - 1.0\n",
      "1963/1980 - 1.0\n",
      "1964/1980 - 1.0\n",
      "1965/1980 - 1.0\n",
      "1966/1980 - 1.0\n",
      "1967/1980 - 1.0\n",
      "1968/1980 - 1.0\n",
      "1969/1980 - 1.0\n",
      "1970/1980 - 1.0\n",
      "1971/1980 - 1.0\n",
      "1972/1980 - 1.0\n",
      "1973/1980 - 1.0\n",
      "1974/1980 - 1.0\n",
      "1975/1980 - 1.0\n",
      "1976/1980 - 1.0\n",
      "1977/1980 - 1.0\n",
      "1978/1980 - 1.0\n",
      "1979/1980 - 1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def TEST_corenlp_to_bert_mapping():\n",
    "    match_sample_count = 0\n",
    "\n",
    "    for sample_idx, sample in enumerate(data):\n",
    "        text = sample[\"text\"]\n",
    "        \n",
    "        bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        bert_tokens = bert_tokenizer.tokenize(text)\n",
    "        (corenlp_tokens, parse_tree_corenlp) = tokenize_and_depparse(text)\n",
    "\n",
    "        corenlp_to_bert_map = build_corenlp_to_bert_map(corenlp_tokens, bert_tokens, False)\n",
    "        rate = get_corenlp_to_bert_map_matching_rate(corenlp_tokens, bert_tokens, corenlp_to_bert_map)\n",
    "\n",
    "        if rate == 1:\n",
    "            match_sample_count += 1\n",
    "        else:\n",
    "            print(text)\n",
    "            \n",
    "        cur_ratio = match_sample_count / (sample_idx + 1)\n",
    "        print(\"{}/{} - {}\".format(sample_idx,len(data), cur_ratio))\n",
    "\n",
    "    print(match_sample_count / len(data))\n",
    "    \n",
    "TEST_corenlp_to_bert_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "66f75600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3, 0, 0, 3, 10, 10, 7, 7, 10, 3, 10, 3],\n",
       " [0, 1, 2, 6, 7, 11, 8, 9, 10, 10, 12, 13],\n",
       " ['nsubj',\n",
       "  'sprwrd',\n",
       "  'sprwrd',\n",
       "  'punct',\n",
       "  'nsubj',\n",
       "  'sprwrd',\n",
       "  'sprwrd',\n",
       "  'sprwrd',\n",
       "  'nummod',\n",
       "  'parataxis',\n",
       "  'advmod',\n",
       "  'punct'])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_dep_parse_tree(text, verbose=False):\n",
    "    '''\n",
    "    Parse dependency tree and map CoreNLP index to BERT index\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        output_bert_v1s: list of source node indexes\n",
    "        output_bert_v2s: list of target node indexes\n",
    "        types: list of dependency relation\n",
    "        \n",
    "    Usage:\n",
    "    ----------\n",
    "        build_dep_parse_tree(\"I'm waiting ... It's 9am now.\", True)\n",
    "    '''\n",
    "    check_existed_dict = {}\n",
    "    \n",
    "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    bert_tokens = bert_tokenizer.tokenize(text)\n",
    "    (corenlp_tokens, parse_tree_corenlp) = tokenize_and_depparse(text)\n",
    "    \n",
    "    corenlp_to_bert_map = map_corenlp_to_bert(corenlp_tokens, bert_tokens)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'BERT tokens: {bert_tokens}')\n",
    "        print(f'CoreNLP tokens: {corenlp_tokens}')\n",
    "        print(f'CoreNLP dependency tree: {parse_tree_corenlp}')\n",
    "        print(f'CoreNLP to BERT: {corenlp_to_bert_map}')\n",
    "        \n",
    "    output_bert_v1s = []\n",
    "    output_bert_v2s = []\n",
    "    types = []\n",
    "    for edge in parse_tree_corenlp:\n",
    "        (t, corenlp_v1, corenlp_v2) = edge\n",
    "        \n",
    "        if (corenlp_v1 not in corenlp_to_bert_map) or (corenlp_v2 not in corenlp_to_bert_map):\n",
    "            continue;\n",
    "        \n",
    "        bert_v1 = corenlp_to_bert_map[corenlp_v1]\n",
    "        bert_v2 = corenlp_to_bert_map[corenlp_v2]\n",
    "\n",
    "        if (len(bert_v1) > 0) and (len(bert_v2) > 0):\n",
    "            bert_v1_super = bert_v1[0]\n",
    "            bert_v2_super = bert_v2[0]\n",
    "            \n",
    "            output_bert_v1s.append(bert_v1_super)\n",
    "            output_bert_v2s.append(bert_v2_super)\n",
    "            types.append(t)\n",
    "            \n",
    "            for bert_v1_sub in bert_v1[1:]:\n",
    "                if (bert_v1_super, bert_v1_sub, \"sprwrd\") not in check_existed_dict:\n",
    "                    output_bert_v1s.append(bert_v1_super)\n",
    "                    output_bert_v2s.append(bert_v1_sub)\n",
    "                    types.append(\"sprwrd\")\n",
    "                    check_existed_dict[(bert_v1_super, bert_v1_sub, \"sprwrd\")] = True\n",
    "                \n",
    "            for bert_v2_sub in bert_v2[1:]:\n",
    "                if (bert_v2_super, bert_v2_sub, \"sprwrd\") not in check_existed_dict:\n",
    "                    output_bert_v1s.append(bert_v2_super)\n",
    "                    output_bert_v2s.append(bert_v2_sub)\n",
    "                    types.append(\"sprwrd\")\n",
    "                    check_existed_dict[(bert_v2_super, bert_v2_sub, \"sprwrd\")] = True\n",
    "                \n",
    "    return output_bert_v1s, output_bert_v2s, types\n",
    "    \n",
    "build_dep_parse_tree(\"I'm waiting ... It's 9am now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf95b754",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2777',\n",
       " 'text': \"To be completely fair, the only redeeming factor was the food, which was above average, but couldn't make up for all the other deficiencies of Teodora.\",\n",
       " 'aspects': [{'term': 'food',\n",
       "   'polarity': 'positive',\n",
       "   'from': '57',\n",
       "   'to': '61'}],\n",
       " 'category': [{'category': 'food', 'polarity': 'positive'},\n",
       "  {'category': 'anecdotes/miscellaneous', 'polarity': 'negative'}]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_sample = {'id': '2777', 'text': \"To be completely fair, the only redeeming factor was the food, which was above average, but couldn't make up for all the other deficiencies of Teodora.\", 'aspects': [{'term': 'food', 'polarity': 'positive', 'from': '57', 'to': '61'}], 'category': [{'category': 'food', 'polarity': 'positive'}, {'category': 'anecdotes/miscellaneous', 'polarity': 'negative'}]}\n",
    "bert_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cce367e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'food'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_1 = \"To be completely fair, the only redeeming factor was the food, which was above average, but couldn't make up for all the other deficiencies of Teodora.\"\n",
    "text_1[57:61]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78b8e14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_aspect(bert_tokens, bert_aspect):\n",
    "    term = bert_aspect[\"term\"]\n",
    "    \n",
    "\n",
    "def build_aspects(bert_tokens, bert_aspects):\n",
    "    for bert_aspect in bert_aspects:\n",
    "        build_aspect(bert_tokens, bert_aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba956154",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convert_index_coreNLP_to_bert' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3076626/118201569.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mbert_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcoreNLP_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_tree_corenlp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_and_depparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mcoreNLP_to_bert_index_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_index_coreNLP_to_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m {\n",
      "\u001b[0;31mNameError\u001b[0m: name 'convert_index_coreNLP_to_bert' is not defined"
     ]
    }
   ],
   "source": [
    "# input \n",
    "# BERT_ABSA:\n",
    "{\n",
    "    'id': '2777', \n",
    "    'text': \"To be completely fair, the only redeeming factor was the food, which was above average, but couldn't make up for all the other deficiencies of Teodora.\", \n",
    "    'aspects': [\n",
    "        {\n",
    "            'term': 'food', \n",
    "            'polarity': 'positive', \n",
    "            'from': '57', \n",
    "            'to': '61'\n",
    "        }\n",
    "    ], \n",
    "    'category': [\n",
    "        {\n",
    "            'category': 'food', \n",
    "            'polarity': 'positive'\n",
    "        }, {'category': 'anecdotes/miscellaneous', 'polarity': 'negative'}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# output:\n",
    "{\n",
    "    'text': \"\",\n",
    "    'bert_token_index': [],\n",
    "    'aspect_term_index': [], # in bert index\n",
    "    'edges': [ # in bert index\n",
    "        [],\n",
    "        []\n",
    "    ]\n",
    "}\n",
    "    \n",
    "text = inp['text']\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_token = bert_tokenizer.tokenize(text)\n",
    "(coreNLP_tokens, parse_tree_corenlp) = tokenize_and_depparse(inp['text'])\n",
    "coreNLP_to_bert_index_map = convert_index_coreNLP_to_bert(bert_token)\n",
    "\n",
    "{\n",
    "    'text': text,\n",
    "    'bert_token_index': bert_token,\n",
    "    'aspect_term_index': [], # in bert index\n",
    "    'edges': [ # in bert index\n",
    "        [],\n",
    "        []\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce688f3b",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17bda56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(pd.read_csv('../dataset/preprocessed_data/Laptops_Train.csv').T.to_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f616616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, data_dir, transformation='QA_M', num_classes=3, bert_tokenizer=None, max_length=0, seed=0):\n",
    "        random.seed(seed)\n",
    "        assert transformation in ['QA_M', 'QA_B', 'MLI_M', 'MLI_B'], 'Invalid transformation method'\n",
    "        assert num_classes in [2, 3], 'Invalid num_classes'\n",
    "        \n",
    "        self.transformation = transformation\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.polarity_dict = {'positive': 0, 'negative': 1, 'neutral': 2}\n",
    "        \n",
    "        # load data\n",
    "        self.data = list(pd.read_csv(data_dir).T.to_dict().values())\n",
    "        if num_classes == 2:\n",
    "            self.data = [d for d in self.data if d['label'] != 'neutral']\n",
    "    \n",
    "    def transform(self, sample):\n",
    "        seq1 = sample['text'].lower()\n",
    "        term = sample['term'].lower()\n",
    "        \n",
    "        if self.transformation == 'QA_M':\n",
    "            seq2 = f'what is the polarity of {term} ?'\n",
    "            label = self.polarity_dict[sample['label']]\n",
    "        elif self.transformation == 'MLI_M':\n",
    "            seq2 = term.lower()\n",
    "            label = self.polarity_dict[sample['label']]\n",
    "#         elif self.transformation == 'QA_B':\n",
    "#         elif self.transformation == 'MLI_B':\n",
    "        \n",
    "        return seq1, seq2, label\n",
    "        \n",
    "    def encode_text(self, seq1, seq2):\n",
    "        # encode\n",
    "        encoded_text = self.bert_tokenizer.encode_plus(\n",
    "            seq1,\n",
    "            seq2,\n",
    "            add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "            max_length=self.max_length,  # maximum length of a sentence\n",
    "            padding='max_length',  # Add [PAD]s\n",
    "            truncation=True, # Truncate up to maximum length\n",
    "            return_attention_mask=True,  # Generate the attention mask\n",
    "            return_tensors='pt',  # Ask the function to return PyTorch tensors\n",
    "        )\n",
    "        return encoded_text\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        '''\n",
    "        example = {\n",
    "            'id': 1000,\n",
    "            'text': 'The food is good, especially their more basic dishes, and the drinks are delicious.',\n",
    "            'term': 'food',\n",
    "            'label': 'positive',\n",
    "            }\n",
    "        '''\n",
    "            \n",
    "        # encoder\n",
    "        sample = self.data[item]\n",
    "        seq1, seq2, label = self.transform(sample)\n",
    "        encoded_text = self.encode_text(seq1, seq2)\n",
    "\n",
    "        single_input = {\n",
    "            'seq1': seq1,\n",
    "            'seq2': seq2,\n",
    "            'term': sample['term'],\n",
    "            'label': label, \n",
    "            'input_ids': encoded_text['input_ids'].flatten(),\n",
    "            'token_type_ids': encoded_text['token_type_ids'].flatten(),\n",
    "            'attention_mask': encoded_text['attention_mask'].flatten(),\n",
    "        }\n",
    "        return single_input\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(params)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        bert_tokenizer = BertTokenizer.from_pretrained(self.hparams.bert_name)\n",
    "        \n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            data_fit = Dataset(\n",
    "                data_dir=self.hparams.data_train_dir,\n",
    "                transformation=self.hparams.transformation,\n",
    "                num_classes=self.hparams.num_classes,\n",
    "                bert_tokenizer=bert_tokenizer,\n",
    "                max_length=self.hparams.max_length,\n",
    "                seed=self.hparams.seed)\n",
    "            \n",
    "            total_samples = data_fit.__len__()\n",
    "            train_samples = int(data_fit.__len__() * 0.8)\n",
    "            val_samples = total_samples - train_samples\n",
    "            self.data_train, self.data_val = random_split(\n",
    "                data_fit, [train_samples, val_samples], generator=torch.Generator().manual_seed(self.hparams.seed))\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.data_test = Dataset(\n",
    "                data_dir=self.hparams.data_test_dir,\n",
    "                transformation=self.hparams.transformation,\n",
    "                num_classes=self.hparams.num_classes,\n",
    "                bert_tokenizer=bert_tokenizer,\n",
    "                max_length=self.hparams.max_length,\n",
    "                seed=self.hparams.seed)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.data_train, \n",
    "            batch_size=self.hparams.batch_size, \n",
    "            num_workers=4, \n",
    "            shuffle=False, # Already shuffle in random_split() \n",
    "            drop_last=True, \n",
    "#             collate_fn=lambda x: x,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.data_val, \n",
    "            batch_size=self.hparams.batch_size, \n",
    "            num_workers=4, \n",
    "            shuffle=False,\n",
    "#             drop_last=True, \n",
    "#             collate_fn=lambda x: x,\n",
    "        )\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.data_test, \n",
    "            batch_size=self.hparams.batch_size, \n",
    "            num_workers=4, \n",
    "            shuffle=False,\n",
    "#             drop_last=True, \n",
    "#             collate_fn=lambda x: x,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34de55a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4644e66a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SentimentClassifier(pl.LightningModule):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(params)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.hparams.pretrained_bert_name)\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(\n",
    "            self.hparams.pretrained_bert_name, num_labels=3, output_hidden_states=True, output_attentions=True, return_dict=False)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        return optimizer  \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        loss, logits, hidden, _ = self.bert(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels,\n",
    "        )\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def margin_loss(self,  embedding_query, embedding_pos, embedding_neg):\n",
    "        scores_pos = (embeddings_query * embeddings_pos).sum(dim=-1)\n",
    "        scores_neg = (embeddings_query * embeddings_neg).sum(dim=-1) * self.scale\n",
    "        return scores_pos - scores_neg\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # ['seq1', 'seq2', 'term', 'label', 'input_ids', 'token_type_ids', 'attention_mask']\n",
    "        logits = self.forward(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            token_type_ids=batch['token_type_ids'],\n",
    "            labels=batch['label'],\n",
    "        )\n",
    "        \n",
    "        labels = batch['label']\n",
    "        ce_loss = self.cross_entropy_loss(logits, labels)        \n",
    "#         acc = utils.calc_accuracy(logits, labels).squeeze()\n",
    "#         logs = {\n",
    "#             'loss': ce_loss,\n",
    "#             'acc': acc,\n",
    "#         }\n",
    "#         self.log_dict(logs, prog_bar=True)\n",
    "        return ce_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logits = self.forward(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            token_type_ids=batch['token_type_ids'],\n",
    "            labels=batch['label'],\n",
    "        )\n",
    "        \n",
    "        labels = batch['label']\n",
    "        ce_loss = self.cross_entropy_loss(logits, labels)        \n",
    "        acc = utils.calc_accuracy(logits, labels).squeeze()\n",
    "        macro_f1 = utils.calc_f1(logits, labels, avg_type='macro').squeeze()\n",
    "        micro_f1 = utils.calc_f1(logits, labels, avg_type='micro').squeeze()\n",
    "\n",
    "        logs = {\n",
    "            'loss': ce_loss, \n",
    "            'acc': acc,\n",
    "            'macro_f1': macro_f1,\n",
    "            'micro_f1': micro_f1\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "        return logs\n",
    "    \n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_acc = torch.stack([x['acc'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_macro_f1 = torch.stack([x['macro_f1'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_micro_f1 = torch.stack([x['micro_f1'] for x in val_step_outputs]).mean().cpu()\n",
    "        logs = {\n",
    "            'val_loss': avg_loss, \n",
    "            'val_acc': avg_acc,\n",
    "            'val_macro_f1': avg_macro_f1,\n",
    "            'val_micro_f1': avg_micro_f1,\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "     \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        logits = self.forward(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            token_type_ids=batch['token_type_ids'],\n",
    "            labels=batch['label'],\n",
    "        )\n",
    "        \n",
    "        labels = batch['label']\n",
    "        ce_loss = self.cross_entropy_loss(logits, labels)        \n",
    "        acc = utils.calc_accuracy(logits, labels).squeeze()\n",
    "        macro_f1 = utils.calc_f1(logits, labels, avg_type='macro').squeeze()\n",
    "        micro_f1 = utils.calc_f1(logits, labels, avg_type='micro').squeeze()\n",
    "\n",
    "        logs = {\n",
    "            'loss': ce_loss, \n",
    "            'acc': acc,\n",
    "            'macro_f1': macro_f1,\n",
    "            'micro_f1': micro_f1\n",
    "        }\n",
    "        return logs\n",
    "    \n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in test_step_outputs]).mean().cpu()\n",
    "        avg_acc = torch.stack([x['acc'] for x in test_step_outputs]).mean().cpu()\n",
    "        avg_macro_f1 = torch.stack([x['macro_f1'] for x in test_step_outputs]).mean().cpu()\n",
    "        avg_micro_f1 = torch.stack([x['micro_f1'] for x in test_step_outputs]).mean().cpu()\n",
    "\n",
    "        logs = {\n",
    "            'test_loss': avg_loss, \n",
    "            'test_acc': avg_acc,\n",
    "            'test_macro_f1': avg_macro_f1,\n",
    "            'test_micro_f1': avg_micro_f1,\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "        return logs\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd6947a4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import commentjson\n",
    "from collections import OrderedDict\n",
    "\n",
    "def read_json(fname):\n",
    "    '''\n",
    "    Read in the json file specified by 'fname'\n",
    "    '''\n",
    "    with open(fname, 'rt') as handle:\n",
    "        return commentjson.load(handle, object_hook=OrderedDict)\n",
    "\n",
    "def build_model(config):\n",
    "    data_params, model_params = config['data_params'], config['model_params']\n",
    "    data = DataModule(data_params)\n",
    "    model = SentimentClassifier(model_params)\n",
    "    return data, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ae91c33",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_trainder(config):\n",
    "    trainer_params = config['trainer_params']\n",
    "    data_params = config['data_params']\n",
    "    \n",
    "    # callbacks\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        dirpath=trainer_params['checkpoint_dir'], \n",
    "        filename='{epoch}-{val_loss:.4f}-{val_acc:.4f}-{val_macro_f1:.4f}-{val_micro_f1:.4f}',\n",
    "        save_top_k=trainer_params['top_k'],\n",
    "        verbose=True,\n",
    "        monitor=trainer_params['metric'],\n",
    "        mode=trainer_params['mode'],\n",
    "    )\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        min_delta=0.00, \n",
    "        patience=trainer_params['patience'],\n",
    "        verbose=False,\n",
    "        mode=trainer_params['mode'],\n",
    "    )\n",
    "    callbacks = [checkpoint, early_stopping]\n",
    "    \n",
    "    # trainer_kwargs\n",
    "    trainer_kwargs = {\n",
    "        'max_epochs': trainer_params['max_epochs'],\n",
    "        'gpus': 1 if torch.cuda.is_available() else 0,\n",
    "    #     \"progress_bar_refresh_rate\":p_refresh,\n",
    "    #     'gradient_clip_val': hyperparameters['grad_clip'],\n",
    "        'weights_summary': 'full',\n",
    "        'deterministic': True,\n",
    "        'callbacks': callbacks,\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(**trainer_kwargs)\n",
    "    return trainer, trainer_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "803725a6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='Training.')\n",
    "\n",
    "# parser.add_argument('-config_file', help='config file path', default='../src/restaurant_config.json', type=str)\n",
    "# parser.add_argument('-f', '--fff', help='a dummy argument to fool ipython', default='1')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# args.config = read_json(args.config_file)\n",
    "# seed_everything(args.config['data_params']['seed'], workers=True)\n",
    "# data, clf = build_model(args.config)\n",
    "# trainer, trainer_kwargs = build_trainder(args.config)\n",
    "# trainer.fit(clf, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1f0de8",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d782c7b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 12345\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory ../model/restaurants exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Training.')\n",
    "\n",
    "parser.add_argument('-config_file', help='config file path', default='../src/restaurant_config.json', type=str)\n",
    "parser.add_argument('-f', '--fff', help='a dummy argument to fool ipython', default='1')\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.config = read_json(args.config_file)\n",
    "seed_everything(args.config['data_params']['seed'], workers=True)\n",
    "data, clf = build_model(args.config)\n",
    "trainer, trainer_kwargs = build_trainder(args.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17ca6e6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7770c45615fa4e92adb87d860b8c70bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'loss': tensor(0.4673, device='cuda:0'), 'acc': tensor(0.8203, dtype=torch.float64), 'macro_f1': tensor(0.7241, dtype=torch.float64), 'micro_f1': tensor(0.8203, dtype=torch.float64)}, {'loss': tensor(0.4060, device='cuda:0'), 'acc': tensor(0.8906, dtype=torch.float64), 'macro_f1': tensor(0.7525, dtype=torch.float64), 'micro_f1': tensor(0.8906, dtype=torch.float64)}, {'loss': tensor(0.4146, device='cuda:0'), 'acc': tensor(0.8359, dtype=torch.float64), 'macro_f1': tensor(0.6356, dtype=torch.float64), 'micro_f1': tensor(0.8359, dtype=torch.float64)}, {'loss': tensor(0.6661, device='cuda:0'), 'acc': tensor(0.7734, dtype=torch.float64), 'macro_f1': tensor(0.6064, dtype=torch.float64), 'micro_f1': tensor(0.7734, dtype=torch.float64)}, {'loss': tensor(0.7138, device='cuda:0'), 'acc': tensor(0.7578, dtype=torch.float64), 'macro_f1': tensor(0.5623, dtype=torch.float64), 'micro_f1': tensor(0.7578, dtype=torch.float64)}, {'loss': tensor(0.7445, device='cuda:0'), 'acc': tensor(0.7500, dtype=torch.float64), 'macro_f1': tensor(0.5685, dtype=torch.float64), 'micro_f1': tensor(0.7500, dtype=torch.float64)}, {'loss': tensor(0.7893, device='cuda:0'), 'acc': tensor(0.7266, dtype=torch.float64), 'macro_f1': tensor(0.5635, dtype=torch.float64), 'micro_f1': tensor(0.7266, dtype=torch.float64)}, {'loss': tensor(0.7322, device='cuda:0'), 'acc': tensor(0.7578, dtype=torch.float64), 'macro_f1': tensor(0.5861, dtype=torch.float64), 'micro_f1': tensor(0.7578, dtype=torch.float64)}, {'loss': tensor(0.8118, device='cuda:0'), 'acc': tensor(0.7708, dtype=torch.float64), 'macro_f1': tensor(0.5916, dtype=torch.float64), 'micro_f1': tensor(0.7708, dtype=torch.float64)}]\n",
      "tensor(0.6384)\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.7870370149612427,\n",
      " 'test_loss': 0.6384071707725525,\n",
      " 'test_macro_f1': 0.6211695671081543,\n",
      " 'test_micro_f1': 0.7870370149612427}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "paths = sorted(glob.glob('/home/hoang/github/BERT_ABSA/model/restaurants/*.ckpt'))\n",
    "model_test = SentimentClassifier.load_from_checkpoint(paths[0])\n",
    "result = trainer.test(model_test, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef78ace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
