{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54d7d217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1722273/2550329.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJ_PATH=/home/hoang/github/BERT_ABSA\n"
     ]
    }
   ],
   "source": [
    "import os, sys, re, datetime, random, gzip, json\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from itertools import accumulate\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "\n",
    "from time import time\n",
    "from math import ceil\n",
    "from multiprocessing import Pool\n",
    "from sentence_transformers import SentenceTransformer, models, losses, InputExample\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.trainer.trainer import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "\n",
    "PROJ_PATH = Path(os.path.join(re.sub(\"/BERT_ABSA.*$\", '', os.getcwd()), 'BERT_ABSA'))\n",
    "print(f'PROJ_PATH={PROJ_PATH}')\n",
    "sys.path.insert(1, str(PROJ_PATH))\n",
    "sys.path.insert(1, str(PROJ_PATH/'src'))\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583f6e87",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e90a1fcf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1730a922",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def parseXML(data_path):\n",
    "    tree = ET.ElementTree(file=data_path)\n",
    "    objs = list()\n",
    "    for sentence in tree.getroot():\n",
    "        obj = dict()\n",
    "        obj['id'] = sentence.attrib['id']\n",
    "        for item in sentence:\n",
    "            if item.tag == 'text':\n",
    "                obj['text'] = item.text\n",
    "            elif item.tag == 'aspectTerms':\n",
    "                obj['aspects'] = list()\n",
    "                for aspectTerm in item:\n",
    "                    if aspectTerm.attrib['polarity'] != 'conflict':\n",
    "                        obj['aspects'].append(aspectTerm.attrib)\n",
    "            elif item.tag == 'aspectCategories':\n",
    "                obj['category'] = list()\n",
    "                for category in item:\n",
    "                    obj['category'].append(category.attrib)\n",
    "        if 'aspects' in obj and len(obj['aspects']):\n",
    "            objs.append(obj)\n",
    "    return objs\n",
    "\n",
    "def convert_to_dataframe(objs):\n",
    "    output = []\n",
    "    for sentence in objs:\n",
    "        id = sentence['id']\n",
    "        text = sentence['text']\n",
    "        aspects = sentence['aspects']\n",
    "        for aspect in aspects:\n",
    "            term = aspect['term']\n",
    "            label = aspect['polarity']\n",
    "            output.append([id, text, term, label])\n",
    "    output = sorted(output, key=lambda x: x[0])\n",
    "    df = pd.DataFrame(output, columns=['id', 'text', 'term', 'label'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a695a5bb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_files = {\n",
    "    'restaurant': {\n",
    "        'train': 'Restaurants_Train.xml',\n",
    "        'test': 'Restaurants_Test.xml',\n",
    "        'trial': 'Restaurants_Trial.xml'\n",
    "    },\n",
    "    'laptop': {\n",
    "        'train': 'Laptops_Train.xml',\n",
    "        'test': 'Laptops_Test.xml',\n",
    "        'trial': 'Laptops_Trial.xml'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "756c8d37",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load: /home/hoang/github/BERT_ABSA/dataset/raw_data/Restaurants_Train.xml\n",
      "Save: /home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Restaurants_Train.pkl\n",
      "\n",
      "Load: /home/hoang/github/BERT_ABSA/dataset/raw_data/Restaurants_Test.xml\n",
      "Save: /home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Restaurants_Test.pkl\n",
      "\n",
      "Load: /home/hoang/github/BERT_ABSA/dataset/raw_data/Restaurants_Trial.xml\n",
      "Save: /home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Restaurants_Trial.pkl\n",
      "\n",
      "Load: /home/hoang/github/BERT_ABSA/dataset/raw_data/Laptops_Train.xml\n",
      "Save: /home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Laptops_Train.pkl\n",
      "\n",
      "Load: /home/hoang/github/BERT_ABSA/dataset/raw_data/Laptops_Test.xml\n",
      "Save: /home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Laptops_Test.pkl\n",
      "\n",
      "Load: /home/hoang/github/BERT_ABSA/dataset/raw_data/Laptops_Trial.xml\n",
      "Save: /home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Laptops_Trial.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dsname, fnames in dataset_files.items():\n",
    "    for g, fname in fnames.items():\n",
    "        input_path = str(PROJ_PATH/ 'dataset/raw_data' / fname)\n",
    "        output_path01 = str(PROJ_PATH/ 'dataset/preprocessed_data' / fname.replace('.xml', '.pkl'))\n",
    "        output_path02 = str(PROJ_PATH/ 'dataset/preprocessed_data' / fname.replace('.xml', '.csv'))\n",
    "        print(f'Load: {input_path}')\n",
    "        print(f'Save: {output_path01}\\n')\n",
    "        objs = parseXML(input_path)\n",
    "        df = convert_to_dataframe(objs)\n",
    "        pd.to_pickle(objs, output_path01)\n",
    "        df.to_csv(output_path02, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7382eecc",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>term</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>The food is good, especially their more basic ...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>The food is good, especially their more basic ...</td>\n",
       "      <td>dishes</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>The food is good, especially their more basic ...</td>\n",
       "      <td>drinks</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1002</td>\n",
       "      <td>The view is spectacular, and the food is great.</td>\n",
       "      <td>view</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1002</td>\n",
       "      <td>The view is spectacular, and the food is great.</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text    term     label\n",
       "0  1000  The food is good, especially their more basic ...    food  positive\n",
       "1  1000  The food is good, especially their more basic ...  dishes  positive\n",
       "2  1000  The food is good, especially their more basic ...  drinks  positive\n",
       "3  1002    The view is spectacular, and the food is great.    view  positive\n",
       "4  1002    The view is spectacular, and the food is great.    food  positive"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Restaurants_Train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96a806e",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ae6db19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(pd.read_csv('../dataset/preprocessed_data/Laptops_Train.csv').T.to_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00737fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, data_dir, transformation='QA_M', num_classes=3, bert_tokenizer=None, max_length=0, seed=0):\n",
    "        random.seed(seed)\n",
    "        assert transformation in ['QA_M', 'QA_B', 'MLI_M', 'MLI_B'], 'Invalid transformation method'\n",
    "        assert num_classes in [2, 3], 'Invalid num_classes'\n",
    "        \n",
    "        self.transformation = transformation\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.polarity_dict = {'positive': 0, 'negative': 1, 'neutral': 2}\n",
    "        \n",
    "        # load data\n",
    "        self.data = list(pd.read_csv(data_dir).T.to_dict().values())\n",
    "        if num_classes == 2:\n",
    "            self.data = [d for d in self.data if d['label'] != 'neutral']\n",
    "    \n",
    "    def transform(self, sample):\n",
    "        seq1 = sample['text'].lower()\n",
    "        term = sample['term'].lower()\n",
    "        \n",
    "        if self.transformation == 'QA_M':\n",
    "            seq2 = f'what is the polarity of {term} ?'\n",
    "            label = self.polarity_dict[sample['label']]\n",
    "        elif self.transformation == 'MLI_M':\n",
    "            seq2 = term.lower()\n",
    "            label = self.polarity_dict[sample['label']]\n",
    "#         elif self.transformation == 'QA_B':\n",
    "#         elif self.transformation == 'MLI_B':\n",
    "        \n",
    "        return seq1, seq2, label\n",
    "        \n",
    "    def encode_text(self, seq1, seq2):\n",
    "        # encode\n",
    "        encoded_text = self.bert_tokenizer.encode_plus(\n",
    "            seq1,\n",
    "            seq2,\n",
    "            add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "            max_length=self.max_length,  # maximum length of a sentence\n",
    "            padding='max_length',  # Add [PAD]s\n",
    "            truncation=True, # Truncate up to maximum length\n",
    "            return_attention_mask=True,  # Generate the attention mask\n",
    "            return_tensors='pt',  # Ask the function to return PyTorch tensors\n",
    "        )\n",
    "        return encoded_text\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        '''\n",
    "        example = {\n",
    "            'id': 1000,\n",
    "            'text': 'The food is good, especially their more basic dishes, and the drinks are delicious.',\n",
    "            'term': 'food',\n",
    "            'label': 'positive',\n",
    "            }\n",
    "        '''\n",
    "            \n",
    "        # encoder\n",
    "        sample = self.data[item]\n",
    "        seq1, seq2, label = self.transform(sample)\n",
    "        encoded_text = self.encode_text(seq1, seq2)\n",
    "\n",
    "        single_input = {\n",
    "            'seq1': seq1,\n",
    "            'seq2': seq2,\n",
    "            'term': sample['term'],\n",
    "            'label': label, \n",
    "            'input_ids': encoded_text['input_ids'].flatten(),\n",
    "            'token_type_ids': encoded_text['token_type_ids'].flatten(),\n",
    "            'attention_mask': encoded_text['attention_mask'].flatten(),\n",
    "        }\n",
    "        return single_input\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(params)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        bert_tokenizer = BertTokenizer.from_pretrained(self.hparams.bert_name)\n",
    "        \n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            data_fit = Dataset(\n",
    "                data_dir=self.hparams.data_train_dir,\n",
    "                transformation=self.hparams.transformation,\n",
    "                num_classes=self.hparams.num_classes,\n",
    "                bert_tokenizer=bert_tokenizer,\n",
    "                max_length=self.hparams.max_length,\n",
    "                seed=self.hparams.seed)\n",
    "            \n",
    "            total_samples = data_fit.__len__()\n",
    "            train_samples = int(data_fit.__len__() * 0.8)\n",
    "            val_samples = total_samples - train_samples\n",
    "            self.data_train, self.data_val = random_split(\n",
    "                data_fit, [train_samples, val_samples], generator=torch.Generator().manual_seed(self.hparams.seed))\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.data_test = Dataset(\n",
    "                data_dir=self.hparams.data_test_dir,\n",
    "                transformation=self.hparams.transformation,\n",
    "                num_classes=self.hparams.num_classes,\n",
    "                bert_tokenizer=bert_tokenizer,\n",
    "                max_length=self.hparams.max_length,\n",
    "                seed=self.hparams.seed)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.data_train, \n",
    "            batch_size=self.hparams.batch_size, \n",
    "            num_workers=4, \n",
    "            shuffle=False, # Already shuffle in random_split() \n",
    "            drop_last=True, \n",
    "#             collate_fn=lambda x: x,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.data_val, \n",
    "            batch_size=self.hparams.batch_size, \n",
    "            num_workers=4, \n",
    "            shuffle=False,\n",
    "#             drop_last=True, \n",
    "#             collate_fn=lambda x: x,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8707e72",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33a09608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(pl.LightningModule):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(params)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.hparams.pretrained_bert_name)\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(\n",
    "            self.hparams.pretrained_bert_name, num_labels=3, output_hidden_states=True, output_attentions=True, return_dict=False)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        return optimizer  \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        loss, logits, hidden, _ = self.bert(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels,\n",
    "        )\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def margin_loss(self,  embedding_query, embedding_pos, embedding_neg):\n",
    "        scores_pos = (embeddings_query * embeddings_pos).sum(dim=-1)\n",
    "        scores_neg = (embeddings_query * embeddings_neg).sum(dim=-1) * self.scale\n",
    "        return scores_pos - scores_neg\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # ['seq1', 'seq2', 'term', 'label', 'input_ids', 'token_type_ids', 'attention_mask']\n",
    "        logits = self.forward(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            token_type_ids=batch['token_type_ids'],\n",
    "            labels=batch['label'],\n",
    "        )\n",
    "        \n",
    "        labels = batch['label']\n",
    "        ce_loss = self.cross_entropy_loss(logits, labels)        \n",
    "#         acc = utils.calc_accuracy(logits, labels).squeeze()\n",
    "#         logs = {\n",
    "#             'loss': ce_loss,\n",
    "#             'acc': acc,\n",
    "#         }\n",
    "#         self.log_dict(logs, prog_bar=True)\n",
    "        return ce_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logits = self.forward(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            token_type_ids=batch['token_type_ids'],\n",
    "            labels=batch['label'],\n",
    "        )\n",
    "        \n",
    "        labels = batch['label']\n",
    "        ce_loss = self.cross_entropy_loss(logits, labels)        \n",
    "        acc = utils.calc_accuracy(logits, labels).squeeze()\n",
    "        macro_f1 = utils.calc_f1(logits, labels, avg_type='macro').squeeze()\n",
    "        micro_f1 = utils.calc_f1(logits, labels, avg_type='micro').squeeze()\n",
    "\n",
    "        logs = {\n",
    "            'loss': ce_loss, \n",
    "            'acc': acc,\n",
    "            'macro_f1': macro_f1,\n",
    "            'micro_f1': micro_f1\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "        return logs\n",
    "    \n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_acc = torch.stack([x['acc'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_macro_f1 = torch.stack([x['macro_f1'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_micro_f1 = torch.stack([x['micro_f1'] for x in val_step_outputs]).mean().cpu()\n",
    "        logs = {\n",
    "            'val_loss': avg_loss, \n",
    "            'val_acc': avg_acc,\n",
    "            'val_macro_f1': avg_macro_f1,\n",
    "            'val_micro_f1': avg_micro_f1,\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "     \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        logits = self.forward(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            token_type_ids=batch['token_type_ids'],\n",
    "            labels=batch['label'],\n",
    "        )\n",
    "        \n",
    "        labels = batch['label']\n",
    "        ce_loss = self.cross_entropy_loss(logits, labels)        \n",
    "        acc = utils.calc_accuracy(logits, labels).squeeze()\n",
    "        macro_f1 = utils.calc_f1(logits, labels, avg_type='macro').squeeze()\n",
    "        micro_f1 = utils.calc_f1(logits, labels, avg_type='micro').squeeze()\n",
    "\n",
    "        logs = {\n",
    "            'loss': ce_loss, \n",
    "            'acc': acc,\n",
    "            'macro_f1': macro_f1,\n",
    "            'micro_f1': micro_f1\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "        return logs\n",
    "    \n",
    "    def test_epoch_end(self, val_step_outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_acc = torch.stack([x['acc'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_macro_f1 = torch.stack([x['macro_f1'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_micro_f1 = torch.stack([x['micro_f1'] for x in val_step_outputs]).mean().cpu()\n",
    "        logs = {\n",
    "            'val_loss': avg_loss, \n",
    "            'val_acc': avg_acc,\n",
    "            'val_macro_f1': avg_macro_f1,\n",
    "            'val_micro_f1': avg_micro_f1,\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76166ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import commentjson\n",
    "from collections import OrderedDict\n",
    "\n",
    "def read_json(fname):\n",
    "    '''\n",
    "    Read in the json file specified by 'fname'\n",
    "    '''\n",
    "    with open(fname, 'rt') as handle:\n",
    "        return commentjson.load(handle, object_hook=OrderedDict)\n",
    "\n",
    "def build_model(config):\n",
    "    data_params, model_params = config['data_params'], config['model_params']\n",
    "    data = DataModule(data_params)\n",
    "    model = SentimentClassifier(model_params)\n",
    "    return data, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f1fdcb9",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def build_trainder(config):\n",
    "    trainer_params = config['trainer_params']\n",
    "    data_params = config['data_params']\n",
    "    \n",
    "    # callbacks\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        dirpath=trainer_params['checkpoint_path'], \n",
    "        filename='{epoch}-{val_loss:.4f}-{val_acc:.4f}-{val_macro_f1:.4f}-{val_micro_f1:.4f}',\n",
    "        save_top_k=trainer_params['top_k'],\n",
    "        verbose=True,\n",
    "        monitor=trainer_params['metric'],\n",
    "        mode=trainer_params['mode'],\n",
    "    )\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        min_delta=0.00, \n",
    "        patience=trainer_params['patience'],\n",
    "        verbose=False,\n",
    "        mode=trainer_params['mode'],\n",
    "    )\n",
    "    callbacks = [checkpoint, early_stopping]\n",
    "    \n",
    "    # trainer_kwargs\n",
    "    trainer_kwargs = {\n",
    "        'max_epochs': trainer_params['max_epochs'],\n",
    "        'gpus': 1 if torch.cuda.is_available() else 0,\n",
    "    #     \"progress_bar_refresh_rate\":p_refresh,\n",
    "    #     'gradient_clip_val': hyperparameters['grad_clip'],\n",
    "        'weights_summary': 'full',\n",
    "        'deterministic': True,\n",
    "        'callbacks': callbacks,\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(**trainer_kwargs)\n",
    "    return trainer, trainer_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad715c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 12345\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory ../model/restaurants exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "    | Name                                                  | Type                          | Params\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "0   | bert                                                  | BertForSequenceClassification | 109 M \n",
      "1   | bert.bert                                             | BertModel                     | 109 M \n",
      "2   | bert.bert.embeddings                                  | BertEmbeddings                | 23.8 M\n",
      "3   | bert.bert.embeddings.word_embeddings                  | Embedding                     | 23.4 M\n",
      "4   | bert.bert.embeddings.position_embeddings              | Embedding                     | 393 K \n",
      "5   | bert.bert.embeddings.token_type_embeddings            | Embedding                     | 1.5 K \n",
      "6   | bert.bert.embeddings.LayerNorm                        | LayerNorm                     | 1.5 K \n",
      "7   | bert.bert.embeddings.dropout                          | Dropout                       | 0     \n",
      "8   | bert.bert.encoder                                     | BertEncoder                   | 85.1 M\n",
      "9   | bert.bert.encoder.layer                               | ModuleList                    | 85.1 M\n",
      "10  | bert.bert.encoder.layer.0                             | BertLayer                     | 7.1 M \n",
      "11  | bert.bert.encoder.layer.0.attention                   | BertAttention                 | 2.4 M \n",
      "12  | bert.bert.encoder.layer.0.attention.self              | BertSelfAttention             | 1.8 M \n",
      "13  | bert.bert.encoder.layer.0.attention.self.query        | Linear                        | 590 K \n",
      "14  | bert.bert.encoder.layer.0.attention.self.key          | Linear                        | 590 K \n",
      "15  | bert.bert.encoder.layer.0.attention.self.value        | Linear                        | 590 K \n",
      "16  | bert.bert.encoder.layer.0.attention.self.dropout      | Dropout                       | 0     \n",
      "17  | bert.bert.encoder.layer.0.attention.output            | BertSelfOutput                | 592 K \n",
      "18  | bert.bert.encoder.layer.0.attention.output.dense      | Linear                        | 590 K \n",
      "19  | bert.bert.encoder.layer.0.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "20  | bert.bert.encoder.layer.0.attention.output.dropout    | Dropout                       | 0     \n",
      "21  | bert.bert.encoder.layer.0.intermediate                | BertIntermediate              | 2.4 M \n",
      "22  | bert.bert.encoder.layer.0.intermediate.dense          | Linear                        | 2.4 M \n",
      "23  | bert.bert.encoder.layer.0.output                      | BertOutput                    | 2.4 M \n",
      "24  | bert.bert.encoder.layer.0.output.dense                | Linear                        | 2.4 M \n",
      "25  | bert.bert.encoder.layer.0.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "26  | bert.bert.encoder.layer.0.output.dropout              | Dropout                       | 0     \n",
      "27  | bert.bert.encoder.layer.1                             | BertLayer                     | 7.1 M \n",
      "28  | bert.bert.encoder.layer.1.attention                   | BertAttention                 | 2.4 M \n",
      "29  | bert.bert.encoder.layer.1.attention.self              | BertSelfAttention             | 1.8 M \n",
      "30  | bert.bert.encoder.layer.1.attention.self.query        | Linear                        | 590 K \n",
      "31  | bert.bert.encoder.layer.1.attention.self.key          | Linear                        | 590 K \n",
      "32  | bert.bert.encoder.layer.1.attention.self.value        | Linear                        | 590 K \n",
      "33  | bert.bert.encoder.layer.1.attention.self.dropout      | Dropout                       | 0     \n",
      "34  | bert.bert.encoder.layer.1.attention.output            | BertSelfOutput                | 592 K \n",
      "35  | bert.bert.encoder.layer.1.attention.output.dense      | Linear                        | 590 K \n",
      "36  | bert.bert.encoder.layer.1.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "37  | bert.bert.encoder.layer.1.attention.output.dropout    | Dropout                       | 0     \n",
      "38  | bert.bert.encoder.layer.1.intermediate                | BertIntermediate              | 2.4 M \n",
      "39  | bert.bert.encoder.layer.1.intermediate.dense          | Linear                        | 2.4 M \n",
      "40  | bert.bert.encoder.layer.1.output                      | BertOutput                    | 2.4 M \n",
      "41  | bert.bert.encoder.layer.1.output.dense                | Linear                        | 2.4 M \n",
      "42  | bert.bert.encoder.layer.1.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "43  | bert.bert.encoder.layer.1.output.dropout              | Dropout                       | 0     \n",
      "44  | bert.bert.encoder.layer.2                             | BertLayer                     | 7.1 M \n",
      "45  | bert.bert.encoder.layer.2.attention                   | BertAttention                 | 2.4 M \n",
      "46  | bert.bert.encoder.layer.2.attention.self              | BertSelfAttention             | 1.8 M \n",
      "47  | bert.bert.encoder.layer.2.attention.self.query        | Linear                        | 590 K \n",
      "48  | bert.bert.encoder.layer.2.attention.self.key          | Linear                        | 590 K \n",
      "49  | bert.bert.encoder.layer.2.attention.self.value        | Linear                        | 590 K \n",
      "50  | bert.bert.encoder.layer.2.attention.self.dropout      | Dropout                       | 0     \n",
      "51  | bert.bert.encoder.layer.2.attention.output            | BertSelfOutput                | 592 K \n",
      "52  | bert.bert.encoder.layer.2.attention.output.dense      | Linear                        | 590 K \n",
      "53  | bert.bert.encoder.layer.2.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "54  | bert.bert.encoder.layer.2.attention.output.dropout    | Dropout                       | 0     \n",
      "55  | bert.bert.encoder.layer.2.intermediate                | BertIntermediate              | 2.4 M \n",
      "56  | bert.bert.encoder.layer.2.intermediate.dense          | Linear                        | 2.4 M \n",
      "57  | bert.bert.encoder.layer.2.output                      | BertOutput                    | 2.4 M \n",
      "58  | bert.bert.encoder.layer.2.output.dense                | Linear                        | 2.4 M \n",
      "59  | bert.bert.encoder.layer.2.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "60  | bert.bert.encoder.layer.2.output.dropout              | Dropout                       | 0     \n",
      "61  | bert.bert.encoder.layer.3                             | BertLayer                     | 7.1 M \n",
      "62  | bert.bert.encoder.layer.3.attention                   | BertAttention                 | 2.4 M \n",
      "63  | bert.bert.encoder.layer.3.attention.self              | BertSelfAttention             | 1.8 M \n",
      "64  | bert.bert.encoder.layer.3.attention.self.query        | Linear                        | 590 K \n",
      "65  | bert.bert.encoder.layer.3.attention.self.key          | Linear                        | 590 K \n",
      "66  | bert.bert.encoder.layer.3.attention.self.value        | Linear                        | 590 K \n",
      "67  | bert.bert.encoder.layer.3.attention.self.dropout      | Dropout                       | 0     \n",
      "68  | bert.bert.encoder.layer.3.attention.output            | BertSelfOutput                | 592 K \n",
      "69  | bert.bert.encoder.layer.3.attention.output.dense      | Linear                        | 590 K \n",
      "70  | bert.bert.encoder.layer.3.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "71  | bert.bert.encoder.layer.3.attention.output.dropout    | Dropout                       | 0     \n",
      "72  | bert.bert.encoder.layer.3.intermediate                | BertIntermediate              | 2.4 M \n",
      "73  | bert.bert.encoder.layer.3.intermediate.dense          | Linear                        | 2.4 M \n",
      "74  | bert.bert.encoder.layer.3.output                      | BertOutput                    | 2.4 M \n",
      "75  | bert.bert.encoder.layer.3.output.dense                | Linear                        | 2.4 M \n",
      "76  | bert.bert.encoder.layer.3.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "77  | bert.bert.encoder.layer.3.output.dropout              | Dropout                       | 0     \n",
      "78  | bert.bert.encoder.layer.4                             | BertLayer                     | 7.1 M \n",
      "79  | bert.bert.encoder.layer.4.attention                   | BertAttention                 | 2.4 M \n",
      "80  | bert.bert.encoder.layer.4.attention.self              | BertSelfAttention             | 1.8 M \n",
      "81  | bert.bert.encoder.layer.4.attention.self.query        | Linear                        | 590 K \n",
      "82  | bert.bert.encoder.layer.4.attention.self.key          | Linear                        | 590 K \n",
      "83  | bert.bert.encoder.layer.4.attention.self.value        | Linear                        | 590 K \n",
      "84  | bert.bert.encoder.layer.4.attention.self.dropout      | Dropout                       | 0     \n",
      "85  | bert.bert.encoder.layer.4.attention.output            | BertSelfOutput                | 592 K \n",
      "86  | bert.bert.encoder.layer.4.attention.output.dense      | Linear                        | 590 K \n",
      "87  | bert.bert.encoder.layer.4.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "88  | bert.bert.encoder.layer.4.attention.output.dropout    | Dropout                       | 0     \n",
      "89  | bert.bert.encoder.layer.4.intermediate                | BertIntermediate              | 2.4 M \n",
      "90  | bert.bert.encoder.layer.4.intermediate.dense          | Linear                        | 2.4 M \n",
      "91  | bert.bert.encoder.layer.4.output                      | BertOutput                    | 2.4 M \n",
      "92  | bert.bert.encoder.layer.4.output.dense                | Linear                        | 2.4 M \n",
      "93  | bert.bert.encoder.layer.4.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "94  | bert.bert.encoder.layer.4.output.dropout              | Dropout                       | 0     \n",
      "95  | bert.bert.encoder.layer.5                             | BertLayer                     | 7.1 M \n",
      "96  | bert.bert.encoder.layer.5.attention                   | BertAttention                 | 2.4 M \n",
      "97  | bert.bert.encoder.layer.5.attention.self              | BertSelfAttention             | 1.8 M \n",
      "98  | bert.bert.encoder.layer.5.attention.self.query        | Linear                        | 590 K \n",
      "99  | bert.bert.encoder.layer.5.attention.self.key          | Linear                        | 590 K \n",
      "100 | bert.bert.encoder.layer.5.attention.self.value        | Linear                        | 590 K \n",
      "101 | bert.bert.encoder.layer.5.attention.self.dropout      | Dropout                       | 0     \n",
      "102 | bert.bert.encoder.layer.5.attention.output            | BertSelfOutput                | 592 K \n",
      "103 | bert.bert.encoder.layer.5.attention.output.dense      | Linear                        | 590 K \n",
      "104 | bert.bert.encoder.layer.5.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "105 | bert.bert.encoder.layer.5.attention.output.dropout    | Dropout                       | 0     \n",
      "106 | bert.bert.encoder.layer.5.intermediate                | BertIntermediate              | 2.4 M \n",
      "107 | bert.bert.encoder.layer.5.intermediate.dense          | Linear                        | 2.4 M \n",
      "108 | bert.bert.encoder.layer.5.output                      | BertOutput                    | 2.4 M \n",
      "109 | bert.bert.encoder.layer.5.output.dense                | Linear                        | 2.4 M \n",
      "110 | bert.bert.encoder.layer.5.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "111 | bert.bert.encoder.layer.5.output.dropout              | Dropout                       | 0     \n",
      "112 | bert.bert.encoder.layer.6                             | BertLayer                     | 7.1 M \n",
      "113 | bert.bert.encoder.layer.6.attention                   | BertAttention                 | 2.4 M \n",
      "114 | bert.bert.encoder.layer.6.attention.self              | BertSelfAttention             | 1.8 M \n",
      "115 | bert.bert.encoder.layer.6.attention.self.query        | Linear                        | 590 K \n",
      "116 | bert.bert.encoder.layer.6.attention.self.key          | Linear                        | 590 K \n",
      "117 | bert.bert.encoder.layer.6.attention.self.value        | Linear                        | 590 K \n",
      "118 | bert.bert.encoder.layer.6.attention.self.dropout      | Dropout                       | 0     \n",
      "119 | bert.bert.encoder.layer.6.attention.output            | BertSelfOutput                | 592 K \n",
      "120 | bert.bert.encoder.layer.6.attention.output.dense      | Linear                        | 590 K \n",
      "121 | bert.bert.encoder.layer.6.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "122 | bert.bert.encoder.layer.6.attention.output.dropout    | Dropout                       | 0     \n",
      "123 | bert.bert.encoder.layer.6.intermediate                | BertIntermediate              | 2.4 M \n",
      "124 | bert.bert.encoder.layer.6.intermediate.dense          | Linear                        | 2.4 M \n",
      "125 | bert.bert.encoder.layer.6.output                      | BertOutput                    | 2.4 M \n",
      "126 | bert.bert.encoder.layer.6.output.dense                | Linear                        | 2.4 M \n",
      "127 | bert.bert.encoder.layer.6.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "128 | bert.bert.encoder.layer.6.output.dropout              | Dropout                       | 0     \n",
      "129 | bert.bert.encoder.layer.7                             | BertLayer                     | 7.1 M \n",
      "130 | bert.bert.encoder.layer.7.attention                   | BertAttention                 | 2.4 M \n",
      "131 | bert.bert.encoder.layer.7.attention.self              | BertSelfAttention             | 1.8 M \n",
      "132 | bert.bert.encoder.layer.7.attention.self.query        | Linear                        | 590 K \n",
      "133 | bert.bert.encoder.layer.7.attention.self.key          | Linear                        | 590 K \n",
      "134 | bert.bert.encoder.layer.7.attention.self.value        | Linear                        | 590 K \n",
      "135 | bert.bert.encoder.layer.7.attention.self.dropout      | Dropout                       | 0     \n",
      "136 | bert.bert.encoder.layer.7.attention.output            | BertSelfOutput                | 592 K \n",
      "137 | bert.bert.encoder.layer.7.attention.output.dense      | Linear                        | 590 K \n",
      "138 | bert.bert.encoder.layer.7.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "139 | bert.bert.encoder.layer.7.attention.output.dropout    | Dropout                       | 0     \n",
      "140 | bert.bert.encoder.layer.7.intermediate                | BertIntermediate              | 2.4 M \n",
      "141 | bert.bert.encoder.layer.7.intermediate.dense          | Linear                        | 2.4 M \n",
      "142 | bert.bert.encoder.layer.7.output                      | BertOutput                    | 2.4 M \n",
      "143 | bert.bert.encoder.layer.7.output.dense                | Linear                        | 2.4 M \n",
      "144 | bert.bert.encoder.layer.7.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "145 | bert.bert.encoder.layer.7.output.dropout              | Dropout                       | 0     \n",
      "146 | bert.bert.encoder.layer.8                             | BertLayer                     | 7.1 M \n",
      "147 | bert.bert.encoder.layer.8.attention                   | BertAttention                 | 2.4 M \n",
      "148 | bert.bert.encoder.layer.8.attention.self              | BertSelfAttention             | 1.8 M \n",
      "149 | bert.bert.encoder.layer.8.attention.self.query        | Linear                        | 590 K \n",
      "150 | bert.bert.encoder.layer.8.attention.self.key          | Linear                        | 590 K \n",
      "151 | bert.bert.encoder.layer.8.attention.self.value        | Linear                        | 590 K \n",
      "152 | bert.bert.encoder.layer.8.attention.self.dropout      | Dropout                       | 0     \n",
      "153 | bert.bert.encoder.layer.8.attention.output            | BertSelfOutput                | 592 K \n",
      "154 | bert.bert.encoder.layer.8.attention.output.dense      | Linear                        | 590 K \n",
      "155 | bert.bert.encoder.layer.8.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "156 | bert.bert.encoder.layer.8.attention.output.dropout    | Dropout                       | 0     \n",
      "157 | bert.bert.encoder.layer.8.intermediate                | BertIntermediate              | 2.4 M \n",
      "158 | bert.bert.encoder.layer.8.intermediate.dense          | Linear                        | 2.4 M \n",
      "159 | bert.bert.encoder.layer.8.output                      | BertOutput                    | 2.4 M \n",
      "160 | bert.bert.encoder.layer.8.output.dense                | Linear                        | 2.4 M \n",
      "161 | bert.bert.encoder.layer.8.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "162 | bert.bert.encoder.layer.8.output.dropout              | Dropout                       | 0     \n",
      "163 | bert.bert.encoder.layer.9                             | BertLayer                     | 7.1 M \n",
      "164 | bert.bert.encoder.layer.9.attention                   | BertAttention                 | 2.4 M \n",
      "165 | bert.bert.encoder.layer.9.attention.self              | BertSelfAttention             | 1.8 M \n",
      "166 | bert.bert.encoder.layer.9.attention.self.query        | Linear                        | 590 K \n",
      "167 | bert.bert.encoder.layer.9.attention.self.key          | Linear                        | 590 K \n",
      "168 | bert.bert.encoder.layer.9.attention.self.value        | Linear                        | 590 K \n",
      "169 | bert.bert.encoder.layer.9.attention.self.dropout      | Dropout                       | 0     \n",
      "170 | bert.bert.encoder.layer.9.attention.output            | BertSelfOutput                | 592 K \n",
      "171 | bert.bert.encoder.layer.9.attention.output.dense      | Linear                        | 590 K \n",
      "172 | bert.bert.encoder.layer.9.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "173 | bert.bert.encoder.layer.9.attention.output.dropout    | Dropout                       | 0     \n",
      "174 | bert.bert.encoder.layer.9.intermediate                | BertIntermediate              | 2.4 M \n",
      "175 | bert.bert.encoder.layer.9.intermediate.dense          | Linear                        | 2.4 M \n",
      "176 | bert.bert.encoder.layer.9.output                      | BertOutput                    | 2.4 M \n",
      "177 | bert.bert.encoder.layer.9.output.dense                | Linear                        | 2.4 M \n",
      "178 | bert.bert.encoder.layer.9.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "179 | bert.bert.encoder.layer.9.output.dropout              | Dropout                       | 0     \n",
      "180 | bert.bert.encoder.layer.10                            | BertLayer                     | 7.1 M \n",
      "181 | bert.bert.encoder.layer.10.attention                  | BertAttention                 | 2.4 M \n",
      "182 | bert.bert.encoder.layer.10.attention.self             | BertSelfAttention             | 1.8 M \n",
      "183 | bert.bert.encoder.layer.10.attention.self.query       | Linear                        | 590 K \n",
      "184 | bert.bert.encoder.layer.10.attention.self.key         | Linear                        | 590 K \n",
      "185 | bert.bert.encoder.layer.10.attention.self.value       | Linear                        | 590 K \n",
      "186 | bert.bert.encoder.layer.10.attention.self.dropout     | Dropout                       | 0     \n",
      "187 | bert.bert.encoder.layer.10.attention.output           | BertSelfOutput                | 592 K \n",
      "188 | bert.bert.encoder.layer.10.attention.output.dense     | Linear                        | 590 K \n",
      "189 | bert.bert.encoder.layer.10.attention.output.LayerNorm | LayerNorm                     | 1.5 K \n",
      "190 | bert.bert.encoder.layer.10.attention.output.dropout   | Dropout                       | 0     \n",
      "191 | bert.bert.encoder.layer.10.intermediate               | BertIntermediate              | 2.4 M \n",
      "192 | bert.bert.encoder.layer.10.intermediate.dense         | Linear                        | 2.4 M \n",
      "193 | bert.bert.encoder.layer.10.output                     | BertOutput                    | 2.4 M \n",
      "194 | bert.bert.encoder.layer.10.output.dense               | Linear                        | 2.4 M \n",
      "195 | bert.bert.encoder.layer.10.output.LayerNorm           | LayerNorm                     | 1.5 K \n",
      "196 | bert.bert.encoder.layer.10.output.dropout             | Dropout                       | 0     \n",
      "197 | bert.bert.encoder.layer.11                            | BertLayer                     | 7.1 M \n",
      "198 | bert.bert.encoder.layer.11.attention                  | BertAttention                 | 2.4 M \n",
      "199 | bert.bert.encoder.layer.11.attention.self             | BertSelfAttention             | 1.8 M \n",
      "200 | bert.bert.encoder.layer.11.attention.self.query       | Linear                        | 590 K \n",
      "201 | bert.bert.encoder.layer.11.attention.self.key         | Linear                        | 590 K \n",
      "202 | bert.bert.encoder.layer.11.attention.self.value       | Linear                        | 590 K \n",
      "203 | bert.bert.encoder.layer.11.attention.self.dropout     | Dropout                       | 0     \n",
      "204 | bert.bert.encoder.layer.11.attention.output           | BertSelfOutput                | 592 K \n",
      "205 | bert.bert.encoder.layer.11.attention.output.dense     | Linear                        | 590 K \n",
      "206 | bert.bert.encoder.layer.11.attention.output.LayerNorm | LayerNorm                     | 1.5 K \n",
      "207 | bert.bert.encoder.layer.11.attention.output.dropout   | Dropout                       | 0     \n",
      "208 | bert.bert.encoder.layer.11.intermediate               | BertIntermediate              | 2.4 M \n",
      "209 | bert.bert.encoder.layer.11.intermediate.dense         | Linear                        | 2.4 M \n",
      "210 | bert.bert.encoder.layer.11.output                     | BertOutput                    | 2.4 M \n",
      "211 | bert.bert.encoder.layer.11.output.dense               | Linear                        | 2.4 M \n",
      "212 | bert.bert.encoder.layer.11.output.LayerNorm           | LayerNorm                     | 1.5 K \n",
      "213 | bert.bert.encoder.layer.11.output.dropout             | Dropout                       | 0     \n",
      "214 | bert.bert.pooler                                      | BertPooler                    | 590 K \n",
      "215 | bert.bert.pooler.dense                                | Linear                        | 590 K \n",
      "216 | bert.bert.pooler.activation                           | Tanh                          | 0     \n",
      "217 | bert.dropout                                          | Dropout                       | 0     \n",
      "218 | bert.classifier                                       | Linear                        | 2.3 K \n",
      "219 | cross_entropy_loss                                    | CrossEntropyLoss              | 0     \n",
      "----------------------------------------------------------------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.938   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Global seed set to 12345\n",
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:326: UserWarning: The number of training samples (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e42c48797544bca5035427b6d592d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 0, global step 21: val_loss reached 0.72011 (best 0.72011), saving model to \"/home/hoang/github/BERT_ABSA/model/restaurants/epoch=0-val_loss=0.7201-val_acc=0.6715-val_macro_f1=0.4689-val_micro_f1=0.6715.ckpt\" as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 1, global step 43: val_loss reached 0.70259 (best 0.70259), saving model to \"/home/hoang/github/BERT_ABSA/model/restaurants/epoch=1-val_loss=0.7026-val_acc=0.7402-val_macro_f1=0.6066-val_micro_f1=0.7402.ckpt\" as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 2, global step 65: val_loss reached 0.66755 (best 0.66755), saving model to \"/home/hoang/github/BERT_ABSA/model/restaurants/epoch=2-val_loss=0.6676-val_acc=0.7673-val_macro_f1=0.6605-val_micro_f1=0.7673-v1.ckpt\" as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 3, global step 87: val_loss was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 4, global step 109: val_loss reached 0.63916 (best 0.63916), saving model to \"/home/hoang/github/BERT_ABSA/model/restaurants/epoch=4-val_loss=0.6392-val_acc=0.7826-val_macro_f1=0.6944-val_micro_f1=0.7826-v1.ckpt\" as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 5, global step 131: val_loss reached 0.69795 (best 0.63916), saving model to \"/home/hoang/github/BERT_ABSA/model/restaurants/epoch=5-val_loss=0.6979-val_acc=0.7542-val_macro_f1=0.6802-val_micro_f1=0.7542.ckpt\" as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 6, global step 153: val_loss was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 7, global step 175: val_loss was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 8, global step 197: val_loss reached 0.63226 (best 0.63226), saving model to \"/home/hoang/github/BERT_ABSA/model/restaurants/epoch=8-val_loss=0.6323-val_acc=0.8034-val_macro_f1=0.7262-val_micro_f1=0.8034-v1.ckpt\" as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 9, global step 219: val_loss was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 10, global step 241: val_loss was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 11, global step 263: val_loss was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 12, global step 285: val_loss was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 13, global step 307: val_loss was not in top 3\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Training.')\n",
    "\n",
    "parser.add_argument('-config_file', help='config file path', default='../src/restaurant_config.json', type=str)\n",
    "parser.add_argument('-f', '--fff', help='a dummy argument to fool ipython', default='1')\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.config = read_json(args.config_file)\n",
    "seed_everything(args.config['data_params']['seed'], workers=True)\n",
    "data, clf = build_model(args.config)\n",
    "trainer, trainer_kwargs = build_trainder(args.config)\n",
    "trainer.fit(clf, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3011ace",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3226c9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
