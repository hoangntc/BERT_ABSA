{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b8f68ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5005/3000755598.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJ_PATH=/home/hoang/github/BERT_ABSA\n"
     ]
    }
   ],
   "source": [
    "import os, sys, re, datetime, random, gzip, json\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from itertools import accumulate\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "\n",
    "from time import time\n",
    "from math import ceil\n",
    "from multiprocessing import Pool\n",
    "from sentence_transformers import SentenceTransformer, models, losses, InputExample\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.trainer.trainer import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "\n",
    "PROJ_PATH = Path(os.path.join(re.sub(\"/BERT_ABSA.*$\", '', os.getcwd()), 'BERT_ABSA'))\n",
    "print(f'PROJ_PATH={PROJ_PATH}')\n",
    "sys.path.insert(1, str(PROJ_PATH))\n",
    "sys.path.insert(1, str(PROJ_PATH/'src'))\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98389c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glob.glob('../model/restaurants/*.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de3f431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30902ca4",
   "metadata": {},
   "source": [
    "## Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eeda2c",
   "metadata": {},
   "source": [
    "### XML parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13eeaf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "277ac99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseXML(data_path):\n",
    "    tree = ET.ElementTree(file=data_path)\n",
    "    objs = list()\n",
    "    for sentence in tree.getroot():\n",
    "        obj = dict()\n",
    "        obj['id'] = sentence.attrib['id']\n",
    "        for item in sentence:\n",
    "            if item.tag == 'text':\n",
    "                obj['text'] = item.text\n",
    "            elif item.tag == 'aspectTerms':\n",
    "                obj['aspects'] = list()\n",
    "                for aspectTerm in item:\n",
    "                    if aspectTerm.attrib['polarity'] != 'conflict':\n",
    "                        obj['aspects'].append(aspectTerm.attrib)\n",
    "            elif item.tag == 'aspectCategories':\n",
    "                obj['category'] = list()\n",
    "                for category in item:\n",
    "                    obj['category'].append(category.attrib)\n",
    "        if 'aspects' in obj and len(obj['aspects']):\n",
    "            objs.append(obj)\n",
    "    return objs\n",
    "\n",
    "def convert_to_dataframe(objs):\n",
    "    output = []\n",
    "    for sentence in objs:\n",
    "        id = sentence['id']\n",
    "        text = sentence['text']\n",
    "        aspects = sentence['aspects']\n",
    "        for aspect in aspects:\n",
    "            term = aspect['term']\n",
    "            label = aspect['polarity']\n",
    "            output.append([id, text, term, label])\n",
    "    output = sorted(output, key=lambda x: x[0])\n",
    "    df = pd.DataFrame(output, columns=['id', 'text', 'term', 'label'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7277b0",
   "metadata": {},
   "source": [
    "### Dependency Tree parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e3383d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import json\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "# nlps = StanfordCoreNLP(str(PROJ_PATH / 'misc/stanford-corenlp-4.3.2'))\n",
    "to_strip_chars = \".,\\(\\)\"\n",
    "BERT_MODEL = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7dcd65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_bert_tokens(tokens, DEBUG=False):\n",
    "    \n",
    "    to_strip_chars = \".,\\(\\)\"\n",
    "    token_check_list_1 = [\"s\", \"re\", \"m\", \"ve\", \"ll\", \"d\"]\n",
    "    \n",
    "    tokens += [\"\"]\n",
    "    \n",
    "    current_token_group = []\n",
    "    output = []\n",
    "    last_token = None\n",
    "    for token_idx, token in enumerate(tokens):\n",
    "        \n",
    "        next_token = None\n",
    "        if token_idx + 1 < len(tokens):\n",
    "            next_token = tokens[token_idx + 1]\n",
    "\n",
    "        reset = True\n",
    "        if token == \"'\":\n",
    "            reset = False\n",
    "        elif token.startswith(\"##\"):\n",
    "            reset = False\n",
    "        elif last_token is not None:\n",
    "            if token in token_check_list_1:\n",
    "                reset = False\n",
    "\n",
    "        keep = True\n",
    "        if (token == '.') and (next_token is None):\n",
    "            keep = True\n",
    "        elif (token == '.') and (next_token is not None) and (next_token != '.'):\n",
    "            keep = True\n",
    "        else:\n",
    "            token = token.strip(to_strip_chars)\n",
    "            if token == \"\":\n",
    "                keep = False\n",
    "\n",
    "        if reset:\n",
    "            if len(current_token_group) > 0:\n",
    "                output += [current_token_group]\n",
    "            current_token_group = []\n",
    "        if keep:\n",
    "            if DEBUG:\n",
    "                current_token_group += [token]\n",
    "            else:\n",
    "                current_token_group += [token_idx]\n",
    "            last_token = token\n",
    "            \n",
    "    return output\n",
    "\n",
    "def process_core_nlp_tokens(tokens, DEBUG=False):\n",
    "    \n",
    "    to_strip_chars = \".,\\(\\)\"\n",
    "    \n",
    "    output = []\n",
    "    for token_idx, token in enumerate(tokens):\n",
    "        \n",
    "        point_stripped_token = token.strip(\".\")\n",
    "        if point_stripped_token == \"\":\n",
    "            if DEBUG:\n",
    "                output += [\".\"]\n",
    "            else: \n",
    "                output += [token_idx]\n",
    "        elif token.startswith(\"'\"):\n",
    "            if DEBUG:\n",
    "                output[-1] += token\n",
    "        else:\n",
    "            isTokenEndsWithPoint = token.endswith(\".\")\n",
    "            \n",
    "            token = token.strip(to_strip_chars)\n",
    "            if token != \"\":\n",
    "                if DEBUG:\n",
    "                    output += [token]\n",
    "                    if isTokenEndsWithPoint:\n",
    "                        output += [\"\"]\n",
    "                else: \n",
    "                    output += [token_idx]\n",
    "                    if isTokenEndsWithPoint:\n",
    "                        output += [-1]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d9b1d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_and_depparse(text):\n",
    "#     text+=' '\n",
    "#     text = re.sub(r'\\. ',' . ',text).strip()\n",
    "#     text = re.sub(r' {2,}',' ',text)\n",
    "#     nlp_properties = {\n",
    "#         'annotators': 'depparse',\n",
    "# #         'tokenize.options': 'splitHyphenated=false,normalizeParentheses=false',\n",
    "#         'tokenize.whitespace': True,  # all tokens have been tokenized before\n",
    "#         'ssplit.isOneSentence': False,\n",
    "#         'outputFormat': 'json',\n",
    "#     }\n",
    "    \n",
    "#     try:\n",
    "#         parsed = json.loads(nlps.annotate(text.strip(), nlp_properties))\n",
    "#     except:\n",
    "#         print('Error')\n",
    "        \n",
    "#     parsed = parsed['sentences']\n",
    "#     tokens = []\n",
    "#     tokens_dict = {}\n",
    "#     tuples = []\n",
    "#     tmplen = 0\n",
    "#     for item in parsed:\n",
    "#         for ite in item['tokens']:\n",
    "#             tokens.extend([ite['word']])\n",
    "#             tokens_dict[ite['index']] = ite['word']\n",
    "# #         tokens.extend([ite['word'] for ite in item['tokens']])\n",
    "#         tuples.extend([\n",
    "#             (\n",
    "#                 ite['dep'],\n",
    "#                 ite['governor']-1+tmplen,\n",
    "#                 ite['dependent']-1+tmplen\n",
    "#             ) for ite in item['basicDependencies'] if ite['dep']!='ROOT'\n",
    "#         ])\n",
    "#         tmplen=len(tokens)\n",
    "        \n",
    "#     return tokens, tuples\n",
    "\n",
    "def tokenize_and_depparse(text):\n",
    "    '''\n",
    "    # to_conll(10) will return the result in a format as follows:\n",
    "    # id word lemma ctag tag feats head(head's id) rel(syntactic relation)\n",
    "    # return values that is unknown will be shown as '_'\n",
    "    # tag and ctag are considered to be equal\n",
    "    '''\n",
    "    parser = CoreNLPDependencyParser(url='http://localhost:9000')\n",
    "    dep_parsed_sentence = parser.raw_parse(text)\n",
    "    deps = dep_parsed_sentence.__next__()\n",
    "    \n",
    "    lines = deps.to_conll(10).split('\\n')\n",
    "    tokens = []\n",
    "    tuples = []\n",
    "    for line in lines:\n",
    "        if line != '':\n",
    "            result = line.split('\\t')\n",
    "            # id word lemma ctag tag feats head(head's id) rel(syntactic relation)\n",
    "            tokens.append(result[1])\n",
    "            if result[7] != 'ROOT':\n",
    "                tuples.append((result[7], int(result[6])-1 , int(result[0])-1))   \n",
    "    return tokens, tuples\n",
    "\n",
    "def build_bert_token_whole(bert_token):\n",
    "    result = \"\"\n",
    "    for bert_sub_token in bert_token:\n",
    "        if bert_sub_token.startswith(\"##\"):\n",
    "            result += bert_sub_token[2:]\n",
    "        else:\n",
    "            result += bert_sub_token\n",
    "    return result\n",
    "\n",
    "def map_corenlp_to_bert_from_indexes(corenlp_processed_indexes, bert_processed_indexes):\n",
    "    output = {}\n",
    "    for (corenlp_processed_index, bert_processed_index) in zip(corenlp_processed_indexes, bert_processed_indexes):\n",
    "        output[corenlp_processed_index] = bert_processed_index\n",
    "    return output\n",
    "\n",
    "def map_corenlp_to_bert_from_indexes_2(corenlp_tokens, bert_tokens, corenlp_processed_indexes, bert_processed_indexes):\n",
    "\n",
    "    output = {}\n",
    "    \n",
    "    bert_run_idx_global = 0\n",
    "    for corenlp_idx in corenlp_processed_indexes:\n",
    "        for bert_run_idx, bert_idx_group in enumerate(bert_processed_indexes[bert_run_idx_global:]):\n",
    "        \n",
    "            corenlp_token = corenlp_tokens[corenlp_idx]\n",
    "\n",
    "            bert_token_group = map(lambda bert_idx: bert_tokens[bert_idx], bert_idx_group)\n",
    "            bert_token = build_bert_token_whole(bert_token_group)\n",
    "\n",
    "            lower_corenlp_token = corenlp_token.lower()\n",
    "            lower_bert_token = bert_token.lower()\n",
    "            if lower_corenlp_token.startswith(lower_bert_token) or lower_corenlp_token.endswith(lower_bert_token) or lower_bert_token.startswith(lower_corenlp_token) or lower_bert_token.endswith(lower_corenlp_token):\n",
    "                bert_run_idx_global = bert_run_idx + 1\n",
    "                output[corenlp_idx] = bert_idx_group\n",
    "                break;\n",
    "        \n",
    "    return output\n",
    "        \n",
    "def map_corenlp_to_bert(corenlp_tokens, bert_tokens, DEBUG=False):\n",
    "    corenlp_processed_indexes = process_core_nlp_tokens(corenlp_tokens, DEBUG)\n",
    "    bert_processed_indexes = process_bert_tokens(bert_tokens, DEBUG)\n",
    "#     return map_corenlp_to_bert_from_indexes(corenlp_processed_indexes, bert_processed_indexes)\n",
    "    return map_corenlp_to_bert_from_indexes_2(corenlp_tokens, bert_tokens, corenlp_processed_indexes, bert_processed_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "006bdf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = text.lower().strip()\n",
    "    return cleaned_text\n",
    "\n",
    "def build_dep_parse_tree(text, verbose=False):\n",
    "    '''\n",
    "    Parse dependency tree and map CoreNLP index to BERT index\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        output_bert_v1s: list of source node indexes\n",
    "        output_bert_v2s: list of target node indexes\n",
    "        types: list of dependency relation\n",
    "        \n",
    "    Usage:\n",
    "    ----------\n",
    "        build_dep_parse_tree(\"I'm waiting ... It's 9am now.\", True)\n",
    "    '''\n",
    "    check_existed_dict = {}\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    bert_tokens = bert_tokenizer.tokenize(text)\n",
    "    (corenlp_tokens, parse_tree_corenlp) = tokenize_and_depparse(text)\n",
    "    \n",
    "    corenlp_to_bert_map = map_corenlp_to_bert(corenlp_tokens, bert_tokens)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'BERT tokens: {bert_tokens}')\n",
    "        print(f'CoreNLP tokens: {corenlp_tokens}')\n",
    "        print(f'CoreNLP dependency tree: {parse_tree_corenlp}')\n",
    "        print(f'CoreNLP to BERT: {corenlp_to_bert_map}')\n",
    "        \n",
    "    output_bert_v1s = []\n",
    "    output_bert_v2s = []\n",
    "    types = []\n",
    "    for edge in parse_tree_corenlp:\n",
    "        (t, corenlp_v1, corenlp_v2) = edge\n",
    "        \n",
    "        if (corenlp_v1 not in corenlp_to_bert_map) or (corenlp_v2 not in corenlp_to_bert_map):\n",
    "            continue;\n",
    "        \n",
    "        bert_v1 = corenlp_to_bert_map[corenlp_v1]\n",
    "        bert_v2 = corenlp_to_bert_map[corenlp_v2]\n",
    "\n",
    "        if (len(bert_v1) > 0) and (len(bert_v2) > 0):\n",
    "            bert_v1_super = bert_v1[0]\n",
    "            bert_v2_super = bert_v2[0]\n",
    "            \n",
    "            output_bert_v1s.append(bert_v1_super)\n",
    "            output_bert_v2s.append(bert_v2_super)\n",
    "            types.append(t)\n",
    "            \n",
    "            for bert_v1_sub in bert_v1[1:]:\n",
    "                if (bert_v1_super, bert_v1_sub, \"sprwrd\") not in check_existed_dict:\n",
    "                    output_bert_v1s.append(bert_v1_super)\n",
    "                    output_bert_v2s.append(bert_v1_sub)\n",
    "                    types.append(\"sprwrd\")\n",
    "                    check_existed_dict[(bert_v1_super, bert_v1_sub, \"sprwrd\")] = True\n",
    "                \n",
    "            for bert_v2_sub in bert_v2[1:]:\n",
    "                if (bert_v2_super, bert_v2_sub, \"sprwrd\") not in check_existed_dict:\n",
    "                    output_bert_v1s.append(bert_v2_super)\n",
    "                    output_bert_v2s.append(bert_v2_sub)\n",
    "                    types.append(\"sprwrd\")\n",
    "                    check_existed_dict[(bert_v2_super, bert_v2_sub, \"sprwrd\")] = True\n",
    "                \n",
    "    return output_bert_v1s, output_bert_v2s, types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b56b4ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_sample(objs):\n",
    "    output = []\n",
    "    for sentence in tqdm(objs, total=len(objs)):\n",
    "        aspects = sentence['aspects']\n",
    "        for aspect in aspects:\n",
    "            source_dep, target_dep, edge_type = build_dep_parse_tree(sentence['text'])\n",
    "            output.append({\n",
    "                'id': sentence['id'],\n",
    "                'text': sentence['text'],\n",
    "                'term': aspect['term'],\n",
    "                'from': aspect['from'],\n",
    "                'to': aspect['to'],\n",
    "                'source_dep': source_dep,\n",
    "                'target_dep': target_dep,\n",
    "                'edge_type': edge_type,\n",
    "                'label': aspect['polarity'],\n",
    "            })\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68166324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a2c2cb7eea4b7fa19526172a4da312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/411 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_path01 = '/home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Laptops_Test.pkl'\n",
    "objs = pd.read_pickle(output_path01)\n",
    "for s in tqdm(objs, total=len(objs)):\n",
    "#     tokenize_and_depparse(s['text'])\n",
    "    build_dep_parse_tree(s['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137fb6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_files = {\n",
    "    'restaurant': {\n",
    "        'train': 'Restaurants_Train.xml',\n",
    "        'test': 'Restaurants_Test.xml',\n",
    "        'trial': 'Restaurants_Trial.xml'\n",
    "    },\n",
    "    'laptop': {\n",
    "        'train': 'Laptops_Train.xml',\n",
    "        'test': 'Laptops_Test.xml',\n",
    "        'trial': 'Laptops_Trial.xml'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8b612e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load: /home/hoang/github/BERT_ABSA/dataset/raw_data/Restaurants_Train.xml\n",
      "Save: /home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Restaurants_Train.pkl\n",
      "\n",
      "Load: /home/hoang/github/BERT_ABSA/dataset/raw_data/Restaurants_Test.xml\n",
      "Save: /home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Restaurants_Test.pkl\n",
      "\n",
      "Load: /home/hoang/github/BERT_ABSA/dataset/raw_data/Restaurants_Trial.xml\n",
      "Save: /home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Restaurants_Trial.pkl\n",
      "\n",
      "Load: /home/hoang/github/BERT_ABSA/dataset/raw_data/Laptops_Train.xml\n",
      "Save: /home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Laptops_Train.pkl\n",
      "\n",
      "Load: /home/hoang/github/BERT_ABSA/dataset/raw_data/Laptops_Test.xml\n",
      "Save: /home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Laptops_Test.pkl\n",
      "\n",
      "Load: /home/hoang/github/BERT_ABSA/dataset/raw_data/Laptops_Trial.xml\n",
      "Save: /home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Laptops_Trial.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dsname, fnames in dataset_files.items():\n",
    "    for g, fname in fnames.items():\n",
    "        input_path = str(PROJ_PATH/ 'dataset/raw_data' / fname)\n",
    "        output_path01 = str(PROJ_PATH/ 'dataset/preprocessed_data' / fname.replace('.xml', '.pkl'))\n",
    "        output_path02 = str(PROJ_PATH/ 'dataset/preprocessed_data' / fname.replace('.xml', '_data.pkl'))\n",
    "        print(f'Load: {input_path}')\n",
    "        print(f'Save parsed XML to: {output_path01}')\n",
    "        print(f'Save data to: {output_path02}\\n')\n",
    "        objs = parseXML(input_path)\n",
    "        output = parse_data_sample(objs)\n",
    "        pd.to_pickle(objs, output_path01)\n",
    "        pd.to_pickle(output, output_path02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bf8e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7147dc24",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6234c573",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = list(pd.read_csv('../dataset/preprocessed_data/Laptops_Train.csv').T.to_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d643a1d3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, data_dir, transformation='QA_M', num_classes=3, bert_tokenizer=None, max_length=0, seed=0):\n",
    "        random.seed(seed)\n",
    "        assert transformation in ['QA_M', 'MLI_M', 'KW_M'], 'Invalid transformation method'\n",
    "        assert num_classes in [2, 3], 'Invalid num_classes'\n",
    "        \n",
    "        self.transformation = transformation\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.polarity_dict = {'positive': 0, 'negative': 1, 'neutral': 2}\n",
    "        \n",
    "        # load data\n",
    "#         self.data = list(pd.read_csv(data_dir).T.to_dict().values())\n",
    "        self.data = pd.read_csv(data_dir).to_dict('records')\n",
    "        if num_classes == 2:\n",
    "            self.data = [d for d in self.data if d['label'] != 'neutral']\n",
    "    \n",
    "    def transform(self, sample):\n",
    "        seq1 = sample['text'].lower()\n",
    "        term = sample['term'].lower()\n",
    "        \n",
    "        if self.transformation == 'QA_M':\n",
    "            seq2 = f'what is the polarity of {term} ?'\n",
    "            label = self.polarity_dict[sample['label']]\n",
    "        elif self.transformation == 'MLI_M':\n",
    "            seq2 = term.lower()\n",
    "            label = self.polarity_dict[sample['label']]\n",
    "        elif self.transformation == 'KW_M':\n",
    "            seq2 = term\n",
    "            label = self.polarity_dict[sample['label']]\n",
    "        \n",
    "        return seq1, seq2, label\n",
    "        \n",
    "    def encode_text(self, seq1, seq2):\n",
    "        # encode\n",
    "        encoded_text = self.bert_tokenizer.encode_plus(\n",
    "            seq1,\n",
    "            seq2,\n",
    "            add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "            max_length=self.max_length,  # maximum length of a sentence\n",
    "            padding='max_length',  # Add [PAD]s\n",
    "            truncation=True, # Truncate up to maximum length\n",
    "            return_attention_mask=True,  # Generate the attention mask\n",
    "            return_tensors='pt',  # Ask the function to return PyTorch tensors\n",
    "        )\n",
    "        return encoded_text\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        '''\n",
    "        example = {\n",
    "            'id': 1000,\n",
    "            'text': 'The food is good, especially their more basic dishes, and the drinks are delicious.',\n",
    "            'term': 'food',\n",
    "            'label': 'positive',\n",
    "            }\n",
    "        '''\n",
    "            \n",
    "        # encoder\n",
    "        sample = self.data[item]\n",
    "        seq1, seq2, label = self.transform(sample)\n",
    "        encoded_text = self.encode_text(seq1, seq2)\n",
    "\n",
    "        single_input = {\n",
    "            'id': sample['id'],\n",
    "            'text': sample['text'],\n",
    "            'seq1': seq1,\n",
    "            'seq2': seq2,\n",
    "            'term': sample['term'],\n",
    "            'label': label, \n",
    "            'input_ids': encoded_text['input_ids'].flatten(),\n",
    "            'token_type_ids': encoded_text['token_type_ids'].flatten(),\n",
    "            'attention_mask': encoded_text['attention_mask'].flatten(),\n",
    "        }\n",
    "        return single_input\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(params)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        bert_tokenizer = BertTokenizer.from_pretrained(self.hparams.bert_name)\n",
    "        \n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            data_fit = Dataset(\n",
    "                data_dir=self.hparams.data_train_dir,\n",
    "                transformation=self.hparams.transformation,\n",
    "                num_classes=self.hparams.num_classes,\n",
    "                bert_tokenizer=bert_tokenizer,\n",
    "                max_length=self.hparams.max_length,\n",
    "                seed=self.hparams.seed)\n",
    "            \n",
    "            total_samples = data_fit.__len__()\n",
    "            train_samples = int(data_fit.__len__() * 0.8)\n",
    "            val_samples = total_samples - train_samples\n",
    "            self.data_train, self.data_val = random_split(\n",
    "                data_fit, [train_samples, val_samples], generator=torch.Generator().manual_seed(self.hparams.seed))\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.data_test = Dataset(\n",
    "                data_dir=self.hparams.data_test_dir,\n",
    "                transformation=self.hparams.transformation,\n",
    "                num_classes=self.hparams.num_classes,\n",
    "                bert_tokenizer=bert_tokenizer,\n",
    "                max_length=self.hparams.max_length,\n",
    "                seed=self.hparams.seed)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.data_train, \n",
    "            batch_size=self.hparams.batch_size, \n",
    "            num_workers=4, \n",
    "            shuffle=False, # Already shuffle in random_split() \n",
    "            drop_last=True, \n",
    "#             collate_fn=lambda x: x,\n",
    "        )\n",
    "    \n",
    "    def mytrain_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.data_train, \n",
    "            batch_size=self.hparams.batch_size, \n",
    "            num_workers=4, \n",
    "            shuffle=False, # Already shuffle in random_split() \n",
    "            drop_last=False, \n",
    "#             collate_fn=lambda x: x,\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.data_val, \n",
    "            batch_size=self.hparams.batch_size, \n",
    "            num_workers=4, \n",
    "            shuffle=False,\n",
    "#             drop_last=True, \n",
    "#             collate_fn=lambda x: x,\n",
    "        )\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.data_test, \n",
    "            batch_size=self.hparams.batch_size, \n",
    "            num_workers=4, \n",
    "            shuffle=False,\n",
    "#             drop_last=True, \n",
    "#             collate_fn=lambda x: x,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4758a845",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f124f3f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SentimentClassifier(pl.LightningModule):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(params)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.hparams.pretrained_bert_name)\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(\n",
    "            self.hparams.pretrained_bert_name, num_labels=3, output_hidden_states=True, output_attentions=True, return_dict=False)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        return optimizer  \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        loss, logits, hidden, _ = self.bert(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels,\n",
    "        )\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def margin_loss(self,  embedding_query, embedding_pos, embedding_neg):\n",
    "        scores_pos = (embeddings_query * embeddings_pos).sum(dim=-1)\n",
    "        scores_neg = (embeddings_query * embeddings_neg).sum(dim=-1) * self.scale\n",
    "        return scores_pos - scores_neg\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # ['seq1', 'seq2', 'term', 'label', 'input_ids', 'token_type_ids', 'attention_mask']\n",
    "        logits = self.forward(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            token_type_ids=batch['token_type_ids'],\n",
    "            labels=batch['label'],\n",
    "        )\n",
    "        \n",
    "        labels = batch['label']\n",
    "        ce_loss = self.cross_entropy_loss(logits, labels)        \n",
    "#         acc = utils.calc_accuracy(logits, labels).squeeze()\n",
    "#         logs = {\n",
    "#             'loss': ce_loss,\n",
    "#             'acc': acc,\n",
    "#         }\n",
    "#         self.log_dict(logs, prog_bar=True)\n",
    "        return ce_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logits = self.forward(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            token_type_ids=batch['token_type_ids'],\n",
    "            labels=batch['label'],\n",
    "        )\n",
    "        \n",
    "        labels = batch['label']\n",
    "        ce_loss = self.cross_entropy_loss(logits, labels)        \n",
    "        acc = utils.calc_accuracy(logits, labels).squeeze()\n",
    "        macro_f1 = utils.calc_f1(logits, labels, avg_type='macro').squeeze()\n",
    "        micro_f1 = utils.calc_f1(logits, labels, avg_type='micro').squeeze()\n",
    "\n",
    "        logs = {\n",
    "            'loss': ce_loss, \n",
    "            'acc': acc,\n",
    "            'macro_f1': macro_f1,\n",
    "            'micro_f1': micro_f1\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "        return logs\n",
    "    \n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_acc = torch.stack([x['acc'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_macro_f1 = torch.stack([x['macro_f1'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_micro_f1 = torch.stack([x['micro_f1'] for x in val_step_outputs]).mean().cpu()\n",
    "        logs = {\n",
    "            'val_loss': avg_loss, \n",
    "            'val_acc': avg_acc,\n",
    "            'val_macro_f1': avg_macro_f1,\n",
    "            'val_micro_f1': avg_micro_f1,\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "     \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        logits = self.forward(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            token_type_ids=batch['token_type_ids'],\n",
    "            labels=batch['label'],\n",
    "        )\n",
    "        \n",
    "        labels = batch['label']\n",
    "        ce_loss = self.cross_entropy_loss(logits, labels)        \n",
    "        acc = utils.calc_accuracy(logits, labels).squeeze()\n",
    "        macro_f1 = utils.calc_f1(logits, labels, avg_type='macro').squeeze()\n",
    "        micro_f1 = utils.calc_f1(logits, labels, avg_type='micro').squeeze()\n",
    "\n",
    "        logs = {\n",
    "            'loss': ce_loss, \n",
    "            'acc': acc,\n",
    "            'macro_f1': macro_f1,\n",
    "            'micro_f1': micro_f1\n",
    "        }\n",
    "        return logs\n",
    "    \n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in test_step_outputs]).mean().cpu()\n",
    "        avg_acc = torch.stack([x['acc'] for x in test_step_outputs]).mean().cpu()\n",
    "        avg_macro_f1 = torch.stack([x['macro_f1'] for x in test_step_outputs]).mean().cpu()\n",
    "        avg_micro_f1 = torch.stack([x['micro_f1'] for x in test_step_outputs]).mean().cpu()\n",
    "\n",
    "        logs = {\n",
    "            'test_loss': avg_loss, \n",
    "            'test_acc': avg_acc,\n",
    "            'test_macro_f1': avg_macro_f1,\n",
    "            'test_micro_f1': avg_micro_f1,\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "        return logs\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e067fc7b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import commentjson\n",
    "from collections import OrderedDict\n",
    "\n",
    "def read_json(fname):\n",
    "    '''\n",
    "    Read in the json file specified by 'fname'\n",
    "    '''\n",
    "    with open(fname, 'rt') as handle:\n",
    "        return commentjson.load(handle, object_hook=OrderedDict)\n",
    "\n",
    "def build_model(config):\n",
    "    data_params, model_params = config['data_params'], config['model_params']\n",
    "    data = DataModule(data_params)\n",
    "    model = SentimentClassifier(model_params)\n",
    "    return data, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee1c2ab6",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_trainer(config, phase=None):\n",
    "    trainer_params = config['trainer_params']\n",
    "    data_params = config['data_params']\n",
    "    \n",
    "    # callbacks\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        dirpath=trainer_params['checkpoint_dir'], \n",
    "        filename='{epoch}-{val_loss:.4f}-{val_acc:.4f}-{val_macro_f1:.4f}-{val_micro_f1:.4f}',\n",
    "        save_top_k=trainer_params['top_k'],\n",
    "        verbose=True,\n",
    "        monitor=trainer_params['metric'],\n",
    "        mode=trainer_params['mode'],\n",
    "    )\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        min_delta=0.00, \n",
    "        patience=trainer_params['patience'],\n",
    "        verbose=False,\n",
    "        mode=trainer_params['mode'],\n",
    "    )\n",
    "    metrics = {'loss': 'val_loss', 'acc': 'val_acc', 'macro_f1': 'val_macro_f1', 'micro_f1': 'val_micro_f1'}\n",
    "    tuner = TuneReportCallback(metrics, on='validation_end')\n",
    "    \n",
    "    if phase == 'tune':\n",
    "        callbacks = [tuner]\n",
    "    else:\n",
    "        callbacks = [checkpoint, early_stopping]\n",
    "    \n",
    "    # trainer_kwargs\n",
    "    trainer_kwargs = {\n",
    "        'max_epochs': trainer_params['max_epochs'],\n",
    "        'gpus': 1 if torch.cuda.is_available() else 0,\n",
    "    #     \"progress_bar_refresh_rate\": p_refresh,\n",
    "    #     'gradient_clip_val': hyperparameters['grad_clip'],\n",
    "        'weights_summary': 'full',\n",
    "        'deterministic': True,\n",
    "        'callbacks': callbacks,\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(**trainer_kwargs)\n",
    "    return trainer, trainer_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adedbb96",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def execute(config, phase='train'):\n",
    "    assert phase in ['train', 'test', 'tune'], 'Invalid phase!'\n",
    "    seed_everything(config['data_params']['seed'], workers=True)\n",
    "    data, clf = build_model(config)\n",
    "    trainer, trainer_kwargs = build_trainer(config, phase=phase)\n",
    "    \n",
    "    if phase == 'train':\n",
    "        trainer.fit(clf, data)\n",
    "    elif phase == 'test':\n",
    "        checkpoint_dir = Path(config['trainer_params']['checkpoint_dir'])\n",
    "        print(f'Load checkpoint from: {str(checkpoint_dir)}')\n",
    "        paths = sorted(checkpoint_dir.glob('*.ckpt'))\n",
    "        for p in paths:\n",
    "            print(p)\n",
    "            model_test = SentimentClassifier.load_from_checkpoint(p)\n",
    "            result = trainer.test(model_test, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "76b062c3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='Training.')\n",
    "\n",
    "# parser.add_argument('-config_file', help='config file path', default='../src/restaurant_config.json', type=str)\n",
    "# parser.add_argument('-f', '--fff', help='a dummy argument to fool ipython', default='1')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# args.config = read_json(args.config_file)\n",
    "# seed_everything(args.config['data_params']['seed'], workers=True)\n",
    "# data, clf = build_model(args.config)\n",
    "# trainer, trainer_kwargs = build_trainer(args.config)\n",
    "# trainer.fit(clf, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b276dcc8",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c936761",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py  config\t helper.py\t main.py   parser.py  utils.py\r\n",
      "__pycache__  dataset.py  lightning_logs  model.py  tuner.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/hoang/github/BERT_ABSA/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99cdfa59",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 12345\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory ../model/restaurants exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing...\n",
      "Load checkpoint from: ../model/restaurants\n",
      "../model/restaurants/epoch=0-val_loss=0.7201-val_acc=0.6715-val_macro_f1=0.4689-val_micro_f1=0.6715.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343a80c58e444791a1331f2a0aed5c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.7265625,\n",
      " 'test_loss': 0.6642863154411316,\n",
      " 'test_macro_f1': 0.49440622329711914,\n",
      " 'test_micro_f1': 0.7265625}\n",
      "--------------------------------------------------------------------------------\n",
      "../model/restaurants/epoch=2-val_loss=0.6676-val_acc=0.7673-val_macro_f1=0.6605-val_micro_f1=0.7673.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.prepare_data has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.prepare_data.\n",
      "  rank_zero_deprecation(\n",
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcafb3b90a76459cb103181ab6e5ec4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.7870370149612427,\n",
      " 'test_loss': 0.6384071707725525,\n",
      " 'test_macro_f1': 0.6211695671081543,\n",
      " 'test_micro_f1': 0.7870370149612427}\n",
      "--------------------------------------------------------------------------------\n",
      "../model/restaurants/epoch=4-val_loss=0.6392-val_acc=0.7826-val_macro_f1=0.6944-val_micro_f1=0.7826.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\n",
      "  rank_zero_deprecation(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.prepare_data has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.prepare_data.\n",
      "  rank_zero_deprecation(\n",
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda0bc0957a54264b259f916efd82a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.8035300374031067,\n",
      " 'test_loss': 0.5749001502990723,\n",
      " 'test_macro_f1': 0.6690912246704102,\n",
      " 'test_micro_f1': 0.8035300374031067}\n",
      "--------------------------------------------------------------------------------\n",
      "../model/restaurants/epoch=8-val_loss=0.6323-val_acc=0.8034-val_macro_f1=0.7262-val_micro_f1=0.8034.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\n",
      "  rank_zero_deprecation(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.prepare_data has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.prepare_data.\n",
      "  rank_zero_deprecation(\n",
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4694ce6b1aad48b2a2138da9c8998782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.8460648059844971,\n",
      " 'test_loss': 0.5475252270698547,\n",
      " 'test_macro_f1': 0.7520278096199036,\n",
      " 'test_micro_f1': 0.8460648059844971}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\n",
      "  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Training.')\n",
    "\n",
    "parser.add_argument('-config_file', help='config file path', default='../src/config/restaurant_config.json', type=str)\n",
    "parser.add_argument('-f', '--fff', help='a dummy argument to fool ipython', default='1')\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.config = read_json(args.config_file)\n",
    "execute(args.config, phase='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d07aefc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ae0bd00",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import MedianStoppingRule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28c37c77",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-21 15:13:32,179\tWARNING trial_runner.py:246 -- The maximum number of pending trials has been automatically set to the number of available cluster CPUs, which is high (280 CPUs/pending trials). If you're running an experiment with a large number of trials, this could lead to scheduling overhead. In this case, consider setting the `TUNE_MAX_PENDING_TRIALS_PG` environment variable to the desired maximum number of concurrent trials.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:13:32 (running for 00:00:00.13)<br>Memory usage on this node: 20.6/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 0/255 CPUs, 0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m PROJ_PATH=/home/hoang/github/BERT_ABSA\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m PROJ_PATH=/home/hoang/github/BERT_ABSA\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m PROJ_PATH=/home/hoang/github/BERT_ABSA\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m PROJ_PATH=/home/hoang/github/BERT_ABSA\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m PROJ_PATH=/home/hoang/github/BERT_ABSA\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m PROJ_PATH=/home/hoang/github/BERT_ABSA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m Global seed set to 12345\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m Global seed set to 12345\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:13:37 (running for 00:00:05.21)<br>Memory usage on this node: 23.1/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m GPU available: True, used: True\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m GPU available: True, used: True\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \r",
      "Validation sanity check: 0it [00:00, ?it/s]\r",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \r",
      "Validation sanity check: 0it [00:00, ?it/s]\r",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m     | Name                                                  | Type                          | Params\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m ----------------------------------------------------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 0   | bert                                                  | BertForSequenceClassification | 109 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 1   | bert.bert                                             | BertModel                     | 109 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 2   | bert.bert.embeddings                                  | BertEmbeddings                | 23.8 M\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 3   | bert.bert.embeddings.word_embeddings                  | Embedding                     | 23.4 M\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 4   | bert.bert.embeddings.position_embeddings              | Embedding                     | 393 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 5   | bert.bert.embeddings.token_type_embeddings            | Embedding                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 6   | bert.bert.embeddings.LayerNorm                        | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 7   | bert.bert.embeddings.dropout                          | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 8   | bert.bert.encoder                                     | BertEncoder                   | 85.1 M\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 9   | bert.bert.encoder.layer                               | ModuleList                    | 85.1 M\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 10  | bert.bert.encoder.layer.0                             | BertLayer                     | 7.1 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 11  | bert.bert.encoder.layer.0.attention                   | BertAttention                 | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 12  | bert.bert.encoder.layer.0.attention.self              | BertSelfAttention             | 1.8 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 13  | bert.bert.encoder.layer.0.attention.self.query        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 14  | bert.bert.encoder.layer.0.attention.self.key          | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 15  | bert.bert.encoder.layer.0.attention.self.value        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 16  | bert.bert.encoder.layer.0.attention.self.dropout      | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 17  | bert.bert.encoder.layer.0.attention.output            | BertSelfOutput                | 592 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 18  | bert.bert.encoder.layer.0.attention.output.dense      | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 19  | bert.bert.encoder.layer.0.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 20  | bert.bert.encoder.layer.0.attention.output.dropout    | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 21  | bert.bert.encoder.layer.0.intermediate                | BertIntermediate              | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 22  | bert.bert.encoder.layer.0.intermediate.dense          | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 23  | bert.bert.encoder.layer.0.output                      | BertOutput                    | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 24  | bert.bert.encoder.layer.0.output.dense                | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 25  | bert.bert.encoder.layer.0.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 26  | bert.bert.encoder.layer.0.output.dropout              | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 27  | bert.bert.encoder.layer.1                             | BertLayer                     | 7.1 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 28  | bert.bert.encoder.layer.1.attention                   | BertAttention                 | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 29  | bert.bert.encoder.layer.1.attention.self              | BertSelfAttention             | 1.8 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 30  | bert.bert.encoder.layer.1.attention.self.query        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 31  | bert.bert.encoder.layer.1.attention.self.key          | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 32  | bert.bert.encoder.layer.1.attention.self.value        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 33  | bert.bert.encoder.layer.1.attention.self.dropout      | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 34  | bert.bert.encoder.layer.1.attention.output            | BertSelfOutput                | 592 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 35  | bert.bert.encoder.layer.1.attention.output.dense      | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 36  | bert.bert.encoder.layer.1.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 37  | bert.bert.encoder.layer.1.attention.output.dropout    | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 38  | bert.bert.encoder.layer.1.intermediate                | BertIntermediate              | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 39  | bert.bert.encoder.layer.1.intermediate.dense          | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 40  | bert.bert.encoder.layer.1.output                      | BertOutput                    | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 41  | bert.bert.encoder.layer.1.output.dense                | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 42  | bert.bert.encoder.layer.1.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 43  | bert.bert.encoder.layer.1.output.dropout              | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 44  | bert.bert.encoder.layer.2                             | BertLayer                     | 7.1 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 45  | bert.bert.encoder.layer.2.attention                   | BertAttention                 | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 46  | bert.bert.encoder.layer.2.attention.self              | BertSelfAttention             | 1.8 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 47  | bert.bert.encoder.layer.2.attention.self.query        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 48  | bert.bert.encoder.layer.2.attention.self.key          | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 49  | bert.bert.encoder.layer.2.attention.self.value        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 50  | bert.bert.encoder.layer.2.attention.self.dropout      | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 51  | bert.bert.encoder.layer.2.attention.output            | BertSelfOutput                | 592 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 52  | bert.bert.encoder.layer.2.attention.output.dense      | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 53  | bert.bert.encoder.layer.2.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 54  | bert.bert.encoder.layer.2.attention.output.dropout    | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 55  | bert.bert.encoder.layer.2.intermediate                | BertIntermediate              | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 56  | bert.bert.encoder.layer.2.intermediate.dense          | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 57  | bert.bert.encoder.layer.2.output                      | BertOutput                    | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 58  | bert.bert.encoder.layer.2.output.dense                | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 59  | bert.bert.encoder.layer.2.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 60  | bert.bert.encoder.layer.2.output.dropout              | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 61  | bert.bert.encoder.layer.3                             | BertLayer                     | 7.1 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 62  | bert.bert.encoder.layer.3.attention                   | BertAttention                 | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 63  | bert.bert.encoder.layer.3.attention.self              | BertSelfAttention             | 1.8 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 64  | bert.bert.encoder.layer.3.attention.self.query        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 65  | bert.bert.encoder.layer.3.attention.self.key          | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 66  | bert.bert.encoder.layer.3.attention.self.value        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 67  | bert.bert.encoder.layer.3.attention.self.dropout      | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 68  | bert.bert.encoder.layer.3.attention.output            | BertSelfOutput                | 592 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 69  | bert.bert.encoder.layer.3.attention.output.dense      | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 70  | bert.bert.encoder.layer.3.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 71  | bert.bert.encoder.layer.3.attention.output.dropout    | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 72  | bert.bert.encoder.layer.3.intermediate                | BertIntermediate              | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 73  | bert.bert.encoder.layer.3.intermediate.dense          | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 74  | bert.bert.encoder.layer.3.output                      | BertOutput                    | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 75  | bert.bert.encoder.layer.3.output.dense                | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 76  | bert.bert.encoder.layer.3.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 77  | bert.bert.encoder.layer.3.output.dropout              | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 78  | bert.bert.encoder.layer.4                             | BertLayer                     | 7.1 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 79  | bert.bert.encoder.layer.4.attention                   | BertAttention                 | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 80  | bert.bert.encoder.layer.4.attention.self              | BertSelfAttention             | 1.8 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 81  | bert.bert.encoder.layer.4.attention.self.query        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 82  | bert.bert.encoder.layer.4.attention.self.key          | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 83  | bert.bert.encoder.layer.4.attention.self.value        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 84  | bert.bert.encoder.layer.4.attention.self.dropout      | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 85  | bert.bert.encoder.layer.4.attention.output            | BertSelfOutput                | 592 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 86  | bert.bert.encoder.layer.4.attention.output.dense      | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 87  | bert.bert.encoder.layer.4.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 88  | bert.bert.encoder.layer.4.attention.output.dropout    | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 89  | bert.bert.encoder.layer.4.intermediate                | BertIntermediate              | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 90  | bert.bert.encoder.layer.4.intermediate.dense          | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 91  | bert.bert.encoder.layer.4.output                      | BertOutput                    | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 92  | bert.bert.encoder.layer.4.output.dense                | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 93  | bert.bert.encoder.layer.4.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 94  | bert.bert.encoder.layer.4.output.dropout              | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 95  | bert.bert.encoder.layer.5                             | BertLayer                     | 7.1 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 96  | bert.bert.encoder.layer.5.attention                   | BertAttention                 | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m     | Name                                                  | Type                          | Params\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m ----------------------------------------------------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 0   | bert                                                  | BertForSequenceClassification | 109 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 1   | bert.bert                                             | BertModel                     | 109 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 2   | bert.bert.embeddings                                  | BertEmbeddings                | 23.8 M\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 3   | bert.bert.embeddings.word_embeddings                  | Embedding                     | 23.4 M\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 4   | bert.bert.embeddings.position_embeddings              | Embedding                     | 393 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 5   | bert.bert.embeddings.token_type_embeddings            | Embedding                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 6   | bert.bert.embeddings.LayerNorm                        | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 7   | bert.bert.embeddings.dropout                          | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 8   | bert.bert.encoder                                     | BertEncoder                   | 85.1 M\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 9   | bert.bert.encoder.layer                               | ModuleList                    | 85.1 M\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 10  | bert.bert.encoder.layer.0                             | BertLayer                     | 7.1 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 11  | bert.bert.encoder.layer.0.attention                   | BertAttention                 | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 12  | bert.bert.encoder.layer.0.attention.self              | BertSelfAttention             | 1.8 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 13  | bert.bert.encoder.layer.0.attention.self.query        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 14  | bert.bert.encoder.layer.0.attention.self.key          | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 15  | bert.bert.encoder.layer.0.attention.self.value        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 16  | bert.bert.encoder.layer.0.attention.self.dropout      | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 17  | bert.bert.encoder.layer.0.attention.output            | BertSelfOutput                | 592 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 18  | bert.bert.encoder.layer.0.attention.output.dense      | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 19  | bert.bert.encoder.layer.0.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 20  | bert.bert.encoder.layer.0.attention.output.dropout    | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 21  | bert.bert.encoder.layer.0.intermediate                | BertIntermediate              | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 22  | bert.bert.encoder.layer.0.intermediate.dense          | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 23  | bert.bert.encoder.layer.0.output                      | BertOutput                    | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 24  | bert.bert.encoder.layer.0.output.dense                | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 25  | bert.bert.encoder.layer.0.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 26  | bert.bert.encoder.layer.0.output.dropout              | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 27  | bert.bert.encoder.layer.1                             | BertLayer                     | 7.1 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 28  | bert.bert.encoder.layer.1.attention                   | BertAttention                 | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 29  | bert.bert.encoder.layer.1.attention.self              | BertSelfAttention             | 1.8 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 30  | bert.bert.encoder.layer.1.attention.self.query        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 31  | bert.bert.encoder.layer.1.attention.self.key          | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 32  | bert.bert.encoder.layer.1.attention.self.value        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 33  | bert.bert.encoder.layer.1.attention.self.dropout      | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 34  | bert.bert.encoder.layer.1.attention.output            | BertSelfOutput                | 592 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 35  | bert.bert.encoder.layer.1.attention.output.dense      | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 36  | bert.bert.encoder.layer.1.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 37  | bert.bert.encoder.layer.1.attention.output.dropout    | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 38  | bert.bert.encoder.layer.1.intermediate                | BertIntermediate              | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 39  | bert.bert.encoder.layer.1.intermediate.dense          | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 40  | bert.bert.encoder.layer.1.output                      | BertOutput                    | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 41  | bert.bert.encoder.layer.1.output.dense                | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 42  | bert.bert.encoder.layer.1.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 43  | bert.bert.encoder.layer.1.output.dropout              | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 44  | bert.bert.encoder.layer.2                             | BertLayer                     | 7.1 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 45  | bert.bert.encoder.layer.2.attention                   | BertAttention                 | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 46  | bert.bert.encoder.layer.2.attention.self              | BertSelfAttention             | 1.8 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 47  | bert.bert.encoder.layer.2.attention.self.query        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 48  | bert.bert.encoder.layer.2.attention.self.key          | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 49  | bert.bert.encoder.layer.2.attention.self.value        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 50  | bert.bert.encoder.layer.2.attention.self.dropout      | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 51  | bert.bert.encoder.layer.2.attention.output            | BertSelfOutput                | 592 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 52  | bert.bert.encoder.layer.2.attention.output.dense      | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 53  | bert.bert.encoder.layer.2.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 54  | bert.bert.encoder.layer.2.attention.output.dropout    | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 55  | bert.bert.encoder.layer.2.intermediate                | BertIntermediate              | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 56  | bert.bert.encoder.layer.2.intermediate.dense          | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 57  | bert.bert.encoder.layer.2.output                      | BertOutput                    | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 58  | bert.bert.encoder.layer.2.output.dense                | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 59  | bert.bert.encoder.layer.2.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 60  | bert.bert.encoder.layer.2.output.dropout              | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 61  | bert.bert.encoder.layer.3                             | BertLayer                     | 7.1 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 62  | bert.bert.encoder.layer.3.attention                   | BertAttention                 | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 63  | bert.bert.encoder.layer.3.attention.self              | BertSelfAttention             | 1.8 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 64  | bert.bert.encoder.layer.3.attention.self.query        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 65  | bert.bert.encoder.layer.3.attention.self.key          | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 66  | bert.bert.encoder.layer.3.attention.self.value        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 67  | bert.bert.encoder.layer.3.attention.self.dropout      | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 68  | bert.bert.encoder.layer.3.attention.output            | BertSelfOutput                | 592 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 69  | bert.bert.encoder.layer.3.attention.output.dense      | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 70  | bert.bert.encoder.layer.3.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 71  | bert.bert.encoder.layer.3.attention.output.dropout    | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 72  | bert.bert.encoder.layer.3.intermediate                | BertIntermediate              | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 73  | bert.bert.encoder.layer.3.intermediate.dense          | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 74  | bert.bert.encoder.layer.3.output                      | BertOutput                    | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 75  | bert.bert.encoder.layer.3.output.dense                | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 76  | bert.bert.encoder.layer.3.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 77  | bert.bert.encoder.layer.3.output.dropout              | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 78  | bert.bert.encoder.layer.4                             | BertLayer                     | 7.1 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 79  | bert.bert.encoder.layer.4.attention                   | BertAttention                 | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 80  | bert.bert.encoder.layer.4.attention.self              | BertSelfAttention             | 1.8 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 81  | bert.bert.encoder.layer.4.attention.self.query        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 82  | bert.bert.encoder.layer.4.attention.self.key          | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 83  | bert.bert.encoder.layer.4.attention.self.value        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 84  | bert.bert.encoder.layer.4.attention.self.dropout      | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 85  | bert.bert.encoder.layer.4.attention.output            | BertSelfOutput                | 592 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 86  | bert.bert.encoder.layer.4.attention.output.dense      | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 87  | bert.bert.encoder.layer.4.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 88  | bert.bert.encoder.layer.4.attention.output.dropout    | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 89  | bert.bert.encoder.layer.4.intermediate                | BertIntermediate              | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 90  | bert.bert.encoder.layer.4.intermediate.dense          | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 91  | bert.bert.encoder.layer.4.output                      | BertOutput                    | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 92  | bert.bert.encoder.layer.4.output.dense                | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 93  | bert.bert.encoder.layer.4.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 94  | bert.bert.encoder.layer.4.output.dropout              | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 95  | bert.bert.encoder.layer.5                             | BertLayer                     | 7.1 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 96  | bert.bert.encoder.layer.5.attention                   | BertAttention                 | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 97  | bert.bert.encoder.layer.5.attention.self              | BertSelfAttention             | 1.8 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 98  | bert.bert.encoder.layer.5.attention.self.query        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 99  | bert.bert.encoder.layer.5.attention.self.key          | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 100 | bert.bert.encoder.layer.5.attention.self.value        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 101 | bert.bert.encoder.layer.5.attention.self.dropout      | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 102 | bert.bert.encoder.layer.5.attention.output            | BertSelfOutput                | 592 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 103 | bert.bert.encoder.layer.5.attention.output.dense      | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 104 | bert.bert.encoder.layer.5.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 105 | bert.bert.encoder.layer.5.attention.output.dropout    | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 106 | bert.bert.encoder.layer.5.intermediate                | BertIntermediate              | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 107 | bert.bert.encoder.layer.5.intermediate.dense          | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 108 | bert.bert.encoder.layer.5.output                      | BertOutput                    | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 109 | bert.bert.encoder.layer.5.output.dense                | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 110 | bert.bert.encoder.layer.5.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 111 | bert.bert.encoder.layer.5.output.dropout              | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 112 | bert.bert.encoder.layer.6                             | BertLayer                     | 7.1 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 113 | bert.bert.encoder.layer.6.attention                   | BertAttention                 | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 114 | bert.bert.encoder.layer.6.attention.self              | BertSelfAttention             | 1.8 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 115 | bert.bert.encoder.layer.6.attention.self.query        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 116 | bert.bert.encoder.layer.6.attention.self.key          | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 117 | bert.bert.encoder.layer.6.attention.self.value        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 118 | bert.bert.encoder.layer.6.attention.self.dropout      | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 119 | bert.bert.encoder.layer.6.attention.output            | BertSelfOutput                | 592 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 120 | bert.bert.encoder.layer.6.attention.output.dense      | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 121 | bert.bert.encoder.layer.6.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 122 | bert.bert.encoder.layer.6.attention.output.dropout    | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 123 | bert.bert.encoder.layer.6.intermediate                | BertIntermediate              | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 124 | bert.bert.encoder.layer.6.intermediate.dense          | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 125 | bert.bert.encoder.layer.6.output                      | BertOutput                    | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 126 | bert.bert.encoder.layer.6.output.dense                | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 127 | bert.bert.encoder.layer.6.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 128 | bert.bert.encoder.layer.6.output.dropout              | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 129 | bert.bert.encoder.layer.7                             | BertLayer                     | 7.1 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 130 | bert.bert.encoder.layer.7.attention                   | BertAttention                 | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 131 | bert.bert.encoder.layer.7.attention.self              | BertSelfAttention             | 1.8 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 132 | bert.bert.encoder.layer.7.attention.self.query        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 133 | bert.bert.encoder.layer.7.attention.self.key          | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 134 | bert.bert.encoder.layer.7.attention.self.value        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 135 | bert.bert.encoder.layer.7.attention.self.dropout      | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 136 | bert.bert.encoder.layer.7.attention.output            | BertSelfOutput                | 592 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 137 | bert.bert.encoder.layer.7.attention.output.dense      | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 138 | bert.bert.encoder.layer.7.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 139 | bert.bert.encoder.layer.7.attention.output.dropout    | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 140 | bert.bert.encoder.layer.7.intermediate                | BertIntermediate              | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 141 | bert.bert.encoder.layer.7.intermediate.dense          | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 142 | bert.bert.encoder.layer.7.output                      | BertOutput                    | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 143 | bert.bert.encoder.layer.7.output.dense                | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 144 | bert.bert.encoder.layer.7.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 145 | bert.bert.encoder.layer.7.output.dropout              | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 146 | bert.bert.encoder.layer.8                             | BertLayer                     | 7.1 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 147 | bert.bert.encoder.layer.8.attention                   | BertAttention                 | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 148 | bert.bert.encoder.layer.8.attention.self              | BertSelfAttention             | 1.8 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 149 | bert.bert.encoder.layer.8.attention.self.query        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 150 | bert.bert.encoder.layer.8.attention.self.key          | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 151 | bert.bert.encoder.layer.8.attention.self.value        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 152 | bert.bert.encoder.layer.8.attention.self.dropout      | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 153 | bert.bert.encoder.layer.8.attention.output            | BertSelfOutput                | 592 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 154 | bert.bert.encoder.layer.8.attention.output.dense      | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 155 | bert.bert.encoder.layer.8.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 156 | bert.bert.encoder.layer.8.attention.output.dropout    | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 157 | bert.bert.encoder.layer.8.intermediate                | BertIntermediate              | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 158 | bert.bert.encoder.layer.8.intermediate.dense          | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 159 | bert.bert.encoder.layer.8.output                      | BertOutput                    | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 160 | bert.bert.encoder.layer.8.output.dense                | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 161 | bert.bert.encoder.layer.8.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 162 | bert.bert.encoder.layer.8.output.dropout              | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 163 | bert.bert.encoder.layer.9                             | BertLayer                     | 7.1 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 164 | bert.bert.encoder.layer.9.attention                   | BertAttention                 | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 165 | bert.bert.encoder.layer.9.attention.self              | BertSelfAttention             | 1.8 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 166 | bert.bert.encoder.layer.9.attention.self.query        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 167 | bert.bert.encoder.layer.9.attention.self.key          | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 168 | bert.bert.encoder.layer.9.attention.self.value        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 169 | bert.bert.encoder.layer.9.attention.self.dropout      | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 170 | bert.bert.encoder.layer.9.attention.output            | BertSelfOutput                | 592 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 171 | bert.bert.encoder.layer.9.attention.output.dense      | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 172 | bert.bert.encoder.layer.9.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 173 | bert.bert.encoder.layer.9.attention.output.dropout    | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 174 | bert.bert.encoder.layer.9.intermediate                | BertIntermediate              | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 175 | bert.bert.encoder.layer.9.intermediate.dense          | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 176 | bert.bert.encoder.layer.9.output                      | BertOutput                    | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 177 | bert.bert.encoder.layer.9.output.dense                | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 178 | bert.bert.encoder.layer.9.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 179 | bert.bert.encoder.layer.9.output.dropout              | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 180 | bert.bert.encoder.layer.10                            | BertLayer                     | 7.1 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 181 | bert.bert.encoder.layer.10.attention                  | BertAttention                 | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 182 | bert.bert.encoder.layer.10.attention.self             | BertSelfAttention             | 1.8 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 183 | bert.bert.encoder.layer.10.attention.self.query       | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 184 | bert.bert.encoder.layer.10.attention.self.key         | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 185 | bert.bert.encoder.layer.10.attention.self.value       | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 186 | bert.bert.encoder.layer.10.attention.self.dropout     | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 187 | bert.bert.encoder.layer.10.attention.output           | BertSelfOutput                | 592 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 188 | bert.bert.encoder.layer.10.attention.output.dense     | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 189 | bert.bert.encoder.layer.10.attention.output.LayerNorm | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 190 | bert.bert.encoder.layer.10.attention.output.dropout   | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 191 | bert.bert.encoder.layer.10.intermediate               | BertIntermediate              | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 192 | bert.bert.encoder.layer.10.intermediate.dense         | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 193 | bert.bert.encoder.layer.10.output                     | BertOutput                    | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 194 | bert.bert.encoder.layer.10.output.dense               | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 195 | bert.bert.encoder.layer.10.output.LayerNorm           | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 196 | bert.bert.encoder.layer.10.output.dropout             | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 97  | bert.bert.encoder.layer.5.attention.self              | BertSelfAttention             | 1.8 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 98  | bert.bert.encoder.layer.5.attention.self.query        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 99  | bert.bert.encoder.layer.5.attention.self.key          | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 100 | bert.bert.encoder.layer.5.attention.self.value        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 101 | bert.bert.encoder.layer.5.attention.self.dropout      | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 102 | bert.bert.encoder.layer.5.attention.output            | BertSelfOutput                | 592 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 103 | bert.bert.encoder.layer.5.attention.output.dense      | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 104 | bert.bert.encoder.layer.5.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 105 | bert.bert.encoder.layer.5.attention.output.dropout    | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 106 | bert.bert.encoder.layer.5.intermediate                | BertIntermediate              | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 107 | bert.bert.encoder.layer.5.intermediate.dense          | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 108 | bert.bert.encoder.layer.5.output                      | BertOutput                    | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 109 | bert.bert.encoder.layer.5.output.dense                | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 110 | bert.bert.encoder.layer.5.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 111 | bert.bert.encoder.layer.5.output.dropout              | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 112 | bert.bert.encoder.layer.6                             | BertLayer                     | 7.1 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 113 | bert.bert.encoder.layer.6.attention                   | BertAttention                 | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 114 | bert.bert.encoder.layer.6.attention.self              | BertSelfAttention             | 1.8 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 115 | bert.bert.encoder.layer.6.attention.self.query        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 116 | bert.bert.encoder.layer.6.attention.self.key          | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 117 | bert.bert.encoder.layer.6.attention.self.value        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 118 | bert.bert.encoder.layer.6.attention.self.dropout      | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 119 | bert.bert.encoder.layer.6.attention.output            | BertSelfOutput                | 592 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 120 | bert.bert.encoder.layer.6.attention.output.dense      | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 121 | bert.bert.encoder.layer.6.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 122 | bert.bert.encoder.layer.6.attention.output.dropout    | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 123 | bert.bert.encoder.layer.6.intermediate                | BertIntermediate              | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 124 | bert.bert.encoder.layer.6.intermediate.dense          | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 125 | bert.bert.encoder.layer.6.output                      | BertOutput                    | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 126 | bert.bert.encoder.layer.6.output.dense                | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 127 | bert.bert.encoder.layer.6.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 128 | bert.bert.encoder.layer.6.output.dropout              | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 129 | bert.bert.encoder.layer.7                             | BertLayer                     | 7.1 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 130 | bert.bert.encoder.layer.7.attention                   | BertAttention                 | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 131 | bert.bert.encoder.layer.7.attention.self              | BertSelfAttention             | 1.8 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 132 | bert.bert.encoder.layer.7.attention.self.query        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 133 | bert.bert.encoder.layer.7.attention.self.key          | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 134 | bert.bert.encoder.layer.7.attention.self.value        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 135 | bert.bert.encoder.layer.7.attention.self.dropout      | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 136 | bert.bert.encoder.layer.7.attention.output            | BertSelfOutput                | 592 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 137 | bert.bert.encoder.layer.7.attention.output.dense      | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 138 | bert.bert.encoder.layer.7.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 139 | bert.bert.encoder.layer.7.attention.output.dropout    | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 140 | bert.bert.encoder.layer.7.intermediate                | BertIntermediate              | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 141 | bert.bert.encoder.layer.7.intermediate.dense          | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 142 | bert.bert.encoder.layer.7.output                      | BertOutput                    | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 143 | bert.bert.encoder.layer.7.output.dense                | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 144 | bert.bert.encoder.layer.7.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 145 | bert.bert.encoder.layer.7.output.dropout              | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 146 | bert.bert.encoder.layer.8                             | BertLayer                     | 7.1 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 147 | bert.bert.encoder.layer.8.attention                   | BertAttention                 | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 148 | bert.bert.encoder.layer.8.attention.self              | BertSelfAttention             | 1.8 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 149 | bert.bert.encoder.layer.8.attention.self.query        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 150 | bert.bert.encoder.layer.8.attention.self.key          | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 151 | bert.bert.encoder.layer.8.attention.self.value        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 152 | bert.bert.encoder.layer.8.attention.self.dropout      | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 153 | bert.bert.encoder.layer.8.attention.output            | BertSelfOutput                | 592 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 154 | bert.bert.encoder.layer.8.attention.output.dense      | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 155 | bert.bert.encoder.layer.8.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 156 | bert.bert.encoder.layer.8.attention.output.dropout    | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 157 | bert.bert.encoder.layer.8.intermediate                | BertIntermediate              | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 158 | bert.bert.encoder.layer.8.intermediate.dense          | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 159 | bert.bert.encoder.layer.8.output                      | BertOutput                    | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 160 | bert.bert.encoder.layer.8.output.dense                | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 161 | bert.bert.encoder.layer.8.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 162 | bert.bert.encoder.layer.8.output.dropout              | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 163 | bert.bert.encoder.layer.9                             | BertLayer                     | 7.1 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 164 | bert.bert.encoder.layer.9.attention                   | BertAttention                 | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 165 | bert.bert.encoder.layer.9.attention.self              | BertSelfAttention             | 1.8 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 166 | bert.bert.encoder.layer.9.attention.self.query        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 167 | bert.bert.encoder.layer.9.attention.self.key          | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 168 | bert.bert.encoder.layer.9.attention.self.value        | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 169 | bert.bert.encoder.layer.9.attention.self.dropout      | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 170 | bert.bert.encoder.layer.9.attention.output            | BertSelfOutput                | 592 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 171 | bert.bert.encoder.layer.9.attention.output.dense      | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 172 | bert.bert.encoder.layer.9.attention.output.LayerNorm  | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 173 | bert.bert.encoder.layer.9.attention.output.dropout    | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 174 | bert.bert.encoder.layer.9.intermediate                | BertIntermediate              | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 175 | bert.bert.encoder.layer.9.intermediate.dense          | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 176 | bert.bert.encoder.layer.9.output                      | BertOutput                    | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 177 | bert.bert.encoder.layer.9.output.dense                | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 178 | bert.bert.encoder.layer.9.output.LayerNorm            | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 179 | bert.bert.encoder.layer.9.output.dropout              | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 180 | bert.bert.encoder.layer.10                            | BertLayer                     | 7.1 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 181 | bert.bert.encoder.layer.10.attention                  | BertAttention                 | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 182 | bert.bert.encoder.layer.10.attention.self             | BertSelfAttention             | 1.8 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 183 | bert.bert.encoder.layer.10.attention.self.query       | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 184 | bert.bert.encoder.layer.10.attention.self.key         | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 185 | bert.bert.encoder.layer.10.attention.self.value       | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 186 | bert.bert.encoder.layer.10.attention.self.dropout     | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 187 | bert.bert.encoder.layer.10.attention.output           | BertSelfOutput                | 592 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 188 | bert.bert.encoder.layer.10.attention.output.dense     | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 189 | bert.bert.encoder.layer.10.attention.output.LayerNorm | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 190 | bert.bert.encoder.layer.10.attention.output.dropout   | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 191 | bert.bert.encoder.layer.10.intermediate               | BertIntermediate              | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 192 | bert.bert.encoder.layer.10.intermediate.dense         | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 193 | bert.bert.encoder.layer.10.output                     | BertOutput                    | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 194 | bert.bert.encoder.layer.10.output.dense               | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 195 | bert.bert.encoder.layer.10.output.LayerNorm           | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 196 | bert.bert.encoder.layer.10.output.dropout             | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 197 | bert.bert.encoder.layer.11                            | BertLayer                     | 7.1 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 198 | bert.bert.encoder.layer.11.attention                  | BertAttention                 | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 199 | bert.bert.encoder.layer.11.attention.self             | BertSelfAttention             | 1.8 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 200 | bert.bert.encoder.layer.11.attention.self.query       | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 201 | bert.bert.encoder.layer.11.attention.self.key         | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 202 | bert.bert.encoder.layer.11.attention.self.value       | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 203 | bert.bert.encoder.layer.11.attention.self.dropout     | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 204 | bert.bert.encoder.layer.11.attention.output           | BertSelfOutput                | 592 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 205 | bert.bert.encoder.layer.11.attention.output.dense     | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 206 | bert.bert.encoder.layer.11.attention.output.LayerNorm | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 207 | bert.bert.encoder.layer.11.attention.output.dropout   | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 208 | bert.bert.encoder.layer.11.intermediate               | BertIntermediate              | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 209 | bert.bert.encoder.layer.11.intermediate.dense         | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 210 | bert.bert.encoder.layer.11.output                     | BertOutput                    | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 211 | bert.bert.encoder.layer.11.output.dense               | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 212 | bert.bert.encoder.layer.11.output.LayerNorm           | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 213 | bert.bert.encoder.layer.11.output.dropout             | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 214 | bert.bert.pooler                                      | BertPooler                    | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 215 | bert.bert.pooler.dense                                | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 216 | bert.bert.pooler.activation                           | Tanh                          | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 217 | bert.dropout                                          | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 218 | bert.classifier                                       | Linear                        | 2.3 K \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 219 | cross_entropy_loss                                    | CrossEntropyLoss              | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m ----------------------------------------------------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 109 M     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 109 M     Total params\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m 437.938   Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 197 | bert.bert.encoder.layer.11                            | BertLayer                     | 7.1 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 198 | bert.bert.encoder.layer.11.attention                  | BertAttention                 | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 199 | bert.bert.encoder.layer.11.attention.self             | BertSelfAttention             | 1.8 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 200 | bert.bert.encoder.layer.11.attention.self.query       | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 201 | bert.bert.encoder.layer.11.attention.self.key         | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 202 | bert.bert.encoder.layer.11.attention.self.value       | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 203 | bert.bert.encoder.layer.11.attention.self.dropout     | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 204 | bert.bert.encoder.layer.11.attention.output           | BertSelfOutput                | 592 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 205 | bert.bert.encoder.layer.11.attention.output.dense     | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 206 | bert.bert.encoder.layer.11.attention.output.LayerNorm | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 207 | bert.bert.encoder.layer.11.attention.output.dropout   | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 208 | bert.bert.encoder.layer.11.intermediate               | BertIntermediate              | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 209 | bert.bert.encoder.layer.11.intermediate.dense         | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 210 | bert.bert.encoder.layer.11.output                     | BertOutput                    | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 211 | bert.bert.encoder.layer.11.output.dense               | Linear                        | 2.4 M \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 212 | bert.bert.encoder.layer.11.output.LayerNorm           | LayerNorm                     | 1.5 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 213 | bert.bert.encoder.layer.11.output.dropout             | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 214 | bert.bert.pooler                                      | BertPooler                    | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 215 | bert.bert.pooler.dense                                | Linear                        | 590 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 216 | bert.bert.pooler.activation                           | Tanh                          | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 217 | bert.dropout                                          | Dropout                       | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 218 | bert.classifier                                       | Linear                        | 2.3 K \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 219 | cross_entropy_loss                                    | CrossEntropyLoss              | 0     \n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m ----------------------------------------------------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 109 M     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 109 M     Total params\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m 437.938   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \r",
      "Validation sanity check:  50%|█████     | 1/2 [00:01<00:01,  1.02s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:13:42 (running for 00:00:10.22)<br>Memory usage on this node: 28.4/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:  50%|█████     | 1/2 [00:01<00:01,  1.12s/it]\n",
      "Validation sanity check: 100%|██████████| 2/2 [00:01<00:00,  1.99it/s]\n",
      "Epoch 0:   0%|          | 0/28 [00:00<00:00, 8256.50it/s]             \n",
      "Validation sanity check: 100%|██████████| 2/2 [00:01<00:00,  1.83it/s]\n",
      "Epoch 0:   0%|          | 0/28 [00:00<00:00, 5924.16it/s]             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m Global seed set to 12345\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:326: UserWarning: The number of training samples (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m Global seed set to 12345\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:326: UserWarning: The number of training samples (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   4%|▎         | 1/28 [00:00<00:11,  2.36it/s, loss=1.21, v_num=0]\n",
      "Epoch 0:   4%|▎         | 1/28 [00:00<00:11,  2.26it/s, loss=1.21, v_num=0]\n",
      "Epoch 0:   7%|▋         | 2/28 [00:01<00:10,  2.40it/s, loss=2.31, v_num=0]\n",
      "Epoch 0:   7%|▋         | 2/28 [00:01<00:11,  2.33it/s, loss=1.15, v_num=0]\n",
      "Epoch 0:  11%|█         | 3/28 [00:01<00:10,  2.41it/s, loss=10.5, v_num=0]\n",
      "Epoch 0:  11%|█         | 3/28 [00:01<00:10,  2.37it/s, loss=2.4, v_num=0] \n",
      "Epoch 0:  14%|█▍        | 4/28 [00:02<00:09,  2.41it/s, loss=12.4, v_num=0]\n",
      "Epoch 0:  14%|█▍        | 4/28 [00:02<00:10,  2.39it/s, loss=2.72, v_num=0]\n",
      "Epoch 0:  18%|█▊        | 5/28 [00:02<00:09,  2.42it/s, loss=11, v_num=0]  \n",
      "Epoch 0:  18%|█▊        | 5/28 [00:02<00:09,  2.41it/s, loss=2.99, v_num=0]\n",
      "Epoch 0:  21%|██▏       | 6/28 [00:02<00:09,  2.43it/s, loss=10.2, v_num=0]\n",
      "Epoch 0:  21%|██▏       | 6/28 [00:02<00:09,  2.42it/s, loss=3.41, v_num=0]\n",
      "Epoch 0:  25%|██▌       | 7/28 [00:03<00:08,  2.44it/s, loss=9.81, v_num=0]\n",
      "Epoch 0:  25%|██▌       | 7/28 [00:03<00:08,  2.43it/s, loss=3.5, v_num=0] \n",
      "Epoch 0:  29%|██▊       | 8/28 [00:03<00:08,  2.44it/s, loss=9.39, v_num=0]\n",
      "Epoch 0:  29%|██▊       | 8/28 [00:03<00:08,  2.44it/s, loss=3.38, v_num=0]\n",
      "Epoch 0:  32%|███▏      | 9/28 [00:04<00:07,  2.44it/s, loss=9.17, v_num=0]\n",
      "Epoch 0:  32%|███▏      | 9/28 [00:04<00:07,  2.44it/s, loss=3.15, v_num=0]\n",
      "Epoch 0:  36%|███▌      | 10/28 [00:04<00:07,  2.44it/s, loss=8.67, v_num=0]\n",
      "Epoch 0:  36%|███▌      | 10/28 [00:04<00:07,  2.45it/s, loss=2.95, v_num=0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:13:47 (running for 00:00:15.23)<br>Memory usage on this node: 28.2/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  39%|███▉      | 11/28 [00:04<00:06,  2.45it/s, loss=8.73, v_num=0]\n",
      "Epoch 0:  39%|███▉      | 11/28 [00:04<00:06,  2.45it/s, loss=2.78, v_num=0]\n",
      "Epoch 0:  43%|████▎     | 12/28 [00:05<00:06,  2.44it/s, loss=8.24, v_num=0]\n",
      "Epoch 0:  43%|████▎     | 12/28 [00:05<00:06,  2.45it/s, loss=2.63, v_num=0]\n",
      "Epoch 0:  46%|████▋     | 13/28 [00:05<00:06,  2.45it/s, loss=7.74, v_num=0]\n",
      "Epoch 0:  46%|████▋     | 13/28 [00:05<00:06,  2.46it/s, loss=2.5, v_num=0] \n",
      "Epoch 0:  50%|█████     | 14/28 [00:06<00:05,  2.45it/s, loss=7.32, v_num=0]\n",
      "Epoch 0:  50%|█████     | 14/28 [00:06<00:05,  2.46it/s, loss=2.4, v_num=0]\n",
      "Epoch 0:  54%|█████▎    | 15/28 [00:06<00:05,  2.45it/s, loss=6.96, v_num=0]\n",
      "Epoch 0:  54%|█████▎    | 15/28 [00:06<00:05,  2.46it/s, loss=2.3, v_num=0]\n",
      "Epoch 0:  57%|█████▋    | 16/28 [00:06<00:04,  2.45it/s, loss=6.63, v_num=0]\n",
      "Epoch 0:  57%|█████▋    | 16/28 [00:06<00:04,  2.46it/s, loss=2.22, v_num=0]\n",
      "Epoch 0:  61%|██████    | 17/28 [00:07<00:04,  2.45it/s, loss=6.33, v_num=0]\n",
      "Epoch 0:  61%|██████    | 17/28 [00:07<00:04,  2.46it/s, loss=2.14, v_num=0]\n",
      "Epoch 0:  64%|██████▍   | 18/28 [00:07<00:04,  2.45it/s, loss=6.09, v_num=0]\n",
      "Epoch 0:  64%|██████▍   | 18/28 [00:07<00:04,  2.47it/s, loss=2.08, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 19/28 [00:08<00:03,  2.45it/s, loss=5.85, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 19/28 [00:08<00:03,  2.47it/s, loss=2.02, v_num=0]\n",
      "Epoch 0:  71%|███████▏  | 20/28 [00:08<00:03,  2.45it/s, loss=5.61, v_num=0]\n",
      "Epoch 0:  71%|███████▏  | 20/28 [00:08<00:03,  2.47it/s, loss=1.96, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 21/28 [00:08<00:02,  2.45it/s, loss=5.65, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 21/28 [00:08<00:02,  2.47it/s, loss=1.95, v_num=0]\n",
      "Epoch 0:  79%|███████▊  | 22/28 [00:09<00:02,  2.42it/s, loss=5.53, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A3)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  79%|███████▊  | 22/28 [00:09<00:02,  2.44it/s, loss=1.95, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A4)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:13:52 (running for 00:00:20.23)<br>Memory usage on this node: 28.2/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 0:  86%|████████▌ | 24/28 [00:10<00:01,  2.49it/s, loss=5.53, v_num=0]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.35it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 0:  93%|█████████▎| 26/28 [00:10<00:00,  2.62it/s, loss=5.53, v_num=0]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 0:  86%|████████▌ | 24/28 [00:10<00:01,  2.48it/s, loss=1.95, v_num=0]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  2.87it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.20it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 0:  93%|█████████▎| 26/28 [00:10<00:00,  2.61it/s, loss=1.95, v_num=0]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 0: 100%|██████████| 28/28 [00:10<00:00,  2.74it/s, loss=5.53, v_num=0]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:01<00:00,  4.79it/s]\u001b[A\n",
      "Result for execute_7fbfb_00001:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-13-53\n",
      "  done: false\n",
      "  experiment_id: 220fad4fc77f49c497bbc5974825553d\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 1\n",
      "  loss: 1.3895759582519531\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684473\n",
      "  time_since_restore: 18.19458031654358\n",
      "  time_this_iter_s: 18.19458031654358\n",
      "  time_total_s: 18.19458031654358\n",
      "  timestamp: 1637525633\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 7fbfb_00001\n",
      "  \n",
      "Epoch 0: 100%|██████████| 28/28 [00:10<00:00,  2.69it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 0: 100%|██████████| 28/28 [00:10<00:00,  2.72it/s, loss=1.95, v_num=0]\n",
      "Result for execute_7fbfb_00000:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-13-53\n",
      "  done: false\n",
      "  experiment_id: 313cd16ec062412fad43e9ffaa517873\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.953103244304657\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684464\n",
      "  time_since_restore: 18.4288272857666\n",
      "  time_this_iter_s: 18.4288272857666\n",
      "  time_total_s: 18.4288272857666\n",
      "  timestamp: 1637525633\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 7fbfb_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \r",
      "Epoch 0: 100%|██████████| 28/28 [00:10<00:00,  2.67it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \r",
      "                                                         \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/28 [00:00<00:00, 2721.81it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:   0%|          | 0/28 [00:00<00:00, 2421.65it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:   4%|▎         | 1/28 [00:00<00:11,  2.42it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 1:   4%|▎         | 1/28 [00:00<00:11,  2.30it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 1:   7%|▋         | 2/28 [00:01<00:10,  2.44it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:   7%|▋         | 2/28 [00:01<00:11,  2.36it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  11%|█         | 3/28 [00:01<00:10,  2.44it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:13:57 (running for 00:00:25.41)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         18.4288</td><td style=\"text-align: right;\">0.953103</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         18.1946</td><td style=\"text-align: right;\">1.38958 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  11%|█         | 3/28 [00:01<00:10,  2.39it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  14%|█▍        | 4/28 [00:02<00:09,  2.45it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  14%|█▍        | 4/28 [00:02<00:09,  2.40it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  18%|█▊        | 5/28 [00:02<00:09,  2.45it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  18%|█▊        | 5/28 [00:02<00:09,  2.41it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  21%|██▏       | 6/28 [00:02<00:08,  2.45it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  21%|██▏       | 6/28 [00:02<00:09,  2.42it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  25%|██▌       | 7/28 [00:03<00:08,  2.46it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  25%|██▌       | 7/28 [00:03<00:08,  2.43it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  29%|██▊       | 8/28 [00:03<00:08,  2.46it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  29%|██▊       | 8/28 [00:03<00:08,  2.44it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  32%|███▏      | 9/28 [00:04<00:07,  2.46it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  32%|███▏      | 9/28 [00:04<00:07,  2.44it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  36%|███▌      | 10/28 [00:04<00:07,  2.46it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  36%|███▌      | 10/28 [00:04<00:07,  2.44it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  39%|███▉      | 11/28 [00:04<00:06,  2.46it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  39%|███▉      | 11/28 [00:04<00:06,  2.45it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  43%|████▎     | 12/28 [00:05<00:06,  2.46it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  43%|████▎     | 12/28 [00:05<00:06,  2.45it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  46%|████▋     | 13/28 [00:05<00:06,  2.46it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  46%|████▋     | 13/28 [00:05<00:06,  2.45it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  50%|█████     | 14/28 [00:06<00:05,  2.47it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  50%|█████     | 14/28 [00:06<00:05,  2.46it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  54%|█████▎    | 15/28 [00:06<00:05,  2.47it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:14:02 (running for 00:00:30.42)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         18.4288</td><td style=\"text-align: right;\">0.953103</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         18.1946</td><td style=\"text-align: right;\">1.38958 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  54%|█████▎    | 15/28 [00:06<00:05,  2.46it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  57%|█████▋    | 16/28 [00:06<00:04,  2.47it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  57%|█████▋    | 16/28 [00:06<00:04,  2.46it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  61%|██████    | 17/28 [00:07<00:04,  2.47it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  61%|██████    | 17/28 [00:07<00:04,  2.46it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  64%|██████▍   | 18/28 [00:07<00:04,  2.47it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  64%|██████▍   | 18/28 [00:07<00:04,  2.46it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  68%|██████▊   | 19/28 [00:08<00:03,  2.47it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  68%|██████▊   | 19/28 [00:08<00:03,  2.46it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  71%|███████▏  | 20/28 [00:08<00:03,  2.47it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  71%|███████▏  | 20/28 [00:08<00:03,  2.47it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  75%|███████▌  | 21/28 [00:08<00:02,  2.47it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  75%|███████▌  | 21/28 [00:08<00:02,  2.47it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 1:  79%|███████▊  | 22/28 [00:09<00:02,  2.45it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A3)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  79%|███████▊  | 22/28 [00:09<00:02,  2.44it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A4)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 1:  86%|████████▌ | 24/28 [00:09<00:01,  2.52it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.25it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 1:  93%|█████████▎| 26/28 [00:10<00:00,  2.64it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 1:  86%|████████▌ | 24/28 [00:09<00:01,  2.50it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.09it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.24it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 1: 100%|██████████| 28/28 [00:10<00:00,  2.76it/s, loss=1.350, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.390, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 1:  93%|█████████▎| 26/28 [00:10<00:00,  2.63it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.16it/s]\u001b[A\n",
      "Result for execute_7fbfb_00001:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-14-06\n",
      "  done: false\n",
      "  experiment_id: 220fad4fc77f49c497bbc5974825553d\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 2\n",
      "  loss: 1.3658382892608643\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684473\n",
      "  time_since_restore: 31.29868197441101\n",
      "  time_this_iter_s: 13.104101657867432\n",
      "  time_total_s: 31.29868197441101\n",
      "  timestamp: 1637525646\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: 7fbfb_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 28/28 [00:10<00:00,  2.71it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 1: 100%|██████████| 28/28 [00:10<00:00,  2.75it/s, loss=0.940, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.953, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00000:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-14-06\n",
      "  done: false\n",
      "  experiment_id: 313cd16ec062412fad43e9ffaa517873\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 2\n",
      "  loss: 0.9443173408508301\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684464\n",
      "  time_since_restore: 31.65979552268982\n",
      "  time_this_iter_s: 13.230968236923218\n",
      "  time_total_s: 31.65979552268982\n",
      "  timestamp: 1637525646\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: 7fbfb_00000\n",
      "  \n",
      "Epoch 1: 100%|██████████| 28/28 [00:10<00:00,  2.69it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:14:07 (running for 00:00:35.64)<br>Memory usage on this node: 30.1/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         31.6598</td><td style=\"text-align: right;\">0.944317</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         31.2987</td><td style=\"text-align: right;\">1.36584 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/28 [00:00<00:00, 2223.92it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:   0%|          | 0/28 [00:00<00:00, 1949.03it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:   4%|▎         | 1/28 [00:00<00:10,  2.51it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 2:   7%|▋         | 2/28 [00:01<00:10,  2.49it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:   4%|▎         | 1/28 [00:00<00:11,  2.40it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 2:  11%|█         | 3/28 [00:01<00:10,  2.48it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:   7%|▋         | 2/28 [00:01<00:10,  2.43it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  14%|█▍        | 4/28 [00:02<00:09,  2.47it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  11%|█         | 3/28 [00:01<00:10,  2.44it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  18%|█▊        | 5/28 [00:02<00:09,  2.47it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  14%|█▍        | 4/28 [00:02<00:09,  2.44it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  21%|██▏       | 6/28 [00:02<00:08,  2.47it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  18%|█▊        | 5/28 [00:02<00:09,  2.45it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  25%|██▌       | 7/28 [00:03<00:08,  2.47it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  21%|██▏       | 6/28 [00:02<00:08,  2.46it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  29%|██▊       | 8/28 [00:03<00:08,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  25%|██▌       | 7/28 [00:03<00:08,  2.46it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:14:12 (running for 00:00:40.65)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         31.6598</td><td style=\"text-align: right;\">0.944317</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         31.2987</td><td style=\"text-align: right;\">1.36584 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  32%|███▏      | 9/28 [00:04<00:07,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  29%|██▊       | 8/28 [00:03<00:08,  2.46it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  36%|███▌      | 10/28 [00:04<00:07,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  32%|███▏      | 9/28 [00:04<00:07,  2.46it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  39%|███▉      | 11/28 [00:04<00:06,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  36%|███▌      | 10/28 [00:04<00:07,  2.46it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  43%|████▎     | 12/28 [00:05<00:06,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  39%|███▉      | 11/28 [00:04<00:06,  2.46it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  46%|████▋     | 13/28 [00:05<00:06,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  43%|████▎     | 12/28 [00:05<00:06,  2.46it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  50%|█████     | 14/28 [00:06<00:05,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  46%|████▋     | 13/28 [00:05<00:06,  2.46it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  54%|█████▎    | 15/28 [00:06<00:05,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  50%|█████     | 14/28 [00:06<00:05,  2.47it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  57%|█████▋    | 16/28 [00:06<00:04,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  54%|█████▎    | 15/28 [00:06<00:05,  2.47it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  61%|██████    | 17/28 [00:07<00:04,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  57%|█████▋    | 16/28 [00:06<00:04,  2.47it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  64%|██████▍   | 18/28 [00:07<00:04,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  61%|██████    | 17/28 [00:07<00:04,  2.47it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  68%|██████▊   | 19/28 [00:08<00:03,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  64%|██████▍   | 18/28 [00:07<00:04,  2.47it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  71%|███████▏  | 20/28 [00:08<00:03,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  68%|██████▊   | 19/28 [00:08<00:03,  2.47it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  75%|███████▌  | 21/28 [00:08<00:02,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  71%|███████▏  | 20/28 [00:08<00:03,  2.47it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:14:17 (running for 00:00:45.66)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         31.6598</td><td style=\"text-align: right;\">0.944317</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         31.2987</td><td style=\"text-align: right;\">1.36584 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  75%|███████▌  | 21/28 [00:08<00:02,  2.47it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 2:  79%|███████▊  | 22/28 [00:09<00:02,  2.43it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A3)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  79%|███████▊  | 22/28 [00:09<00:02,  2.44it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A4)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 2:  86%|████████▌ | 24/28 [00:09<00:01,  2.50it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.32it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 2:  93%|█████████▎| 26/28 [00:10<00:00,  2.62it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 2:  86%|████████▌ | 24/28 [00:09<00:01,  2.50it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.09it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 2: 100%|██████████| 28/28 [00:10<00:00,  2.74it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.370, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.21it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 2:  93%|█████████▎| 26/28 [00:10<00:00,  2.63it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00001:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-14-19\n",
      "  done: false\n",
      "  experiment_id: 220fad4fc77f49c497bbc5974825553d\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 3\n",
      "  loss: 1.3608464002609253\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684473\n",
      "  time_since_restore: 44.380446910858154\n",
      "  time_this_iter_s: 13.081764936447144\n",
      "  time_total_s: 44.380446910858154\n",
      "  timestamp: 1637525659\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 3\n",
      "  trial_id: 7fbfb_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 28/28 [00:10<00:00,  2.69it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.07it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 2: 100%|██████████| 28/28 [00:10<00:00,  2.75it/s, loss=0.933, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.944, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00000:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-14-19\n",
      "  done: false\n",
      "  experiment_id: 313cd16ec062412fad43e9ffaa517873\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 3\n",
      "  loss: 0.9540082812309265\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684464\n",
      "  time_since_restore: 44.79500722885132\n",
      "  time_this_iter_s: 13.135211706161499\n",
      "  time_total_s: 44.79500722885132\n",
      "  timestamp: 1637525659\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 3\n",
      "  trial_id: 7fbfb_00000\n",
      "  \n",
      "Epoch 2: 100%|██████████| 28/28 [00:10<00:00,  2.70it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/28 [00:00<00:00, 2511.56it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:   0%|          | 0/28 [00:00<00:00, 2100.30it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:   4%|▎         | 1/28 [00:00<00:11,  2.43it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:14:22 (running for 00:00:50.78)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         44.795 </td><td style=\"text-align: right;\">0.954008</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         44.3804</td><td style=\"text-align: right;\">1.36085 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:   7%|▋         | 2/28 [00:01<00:10,  2.45it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:   4%|▎         | 1/28 [00:00<00:11,  2.28it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 3:  11%|█         | 3/28 [00:01<00:10,  2.45it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:   7%|▋         | 2/28 [00:01<00:11,  2.34it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  14%|█▍        | 4/28 [00:02<00:09,  2.45it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  11%|█         | 3/28 [00:01<00:10,  2.36it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  18%|█▊        | 5/28 [00:02<00:09,  2.45it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  14%|█▍        | 4/28 [00:02<00:10,  2.37it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  21%|██▏       | 6/28 [00:02<00:08,  2.45it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  18%|█▊        | 5/28 [00:02<00:09,  2.39it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  25%|██▌       | 7/28 [00:03<00:08,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  21%|██▏       | 6/28 [00:02<00:09,  2.40it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  29%|██▊       | 8/28 [00:03<00:08,  2.45it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  25%|██▌       | 7/28 [00:03<00:08,  2.41it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  32%|███▏      | 9/28 [00:04<00:07,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  29%|██▊       | 8/28 [00:03<00:08,  2.41it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  36%|███▌      | 10/28 [00:04<00:07,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  32%|███▏      | 9/28 [00:04<00:07,  2.42it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  39%|███▉      | 11/28 [00:04<00:06,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  36%|███▌      | 10/28 [00:04<00:07,  2.42it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  43%|████▎     | 12/28 [00:05<00:06,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  39%|███▉      | 11/28 [00:04<00:07,  2.43it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  46%|████▋     | 13/28 [00:05<00:06,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  43%|████▎     | 12/28 [00:05<00:06,  2.43it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:14:27 (running for 00:00:55.79)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         44.795 </td><td style=\"text-align: right;\">0.954008</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         44.3804</td><td style=\"text-align: right;\">1.36085 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  50%|█████     | 14/28 [00:06<00:05,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  46%|████▋     | 13/28 [00:05<00:06,  2.43it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  54%|█████▎    | 15/28 [00:06<00:05,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  50%|█████     | 14/28 [00:06<00:05,  2.43it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  57%|█████▋    | 16/28 [00:06<00:04,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  54%|█████▎    | 15/28 [00:06<00:05,  2.44it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  61%|██████    | 17/28 [00:07<00:04,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  57%|█████▋    | 16/28 [00:06<00:04,  2.44it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  64%|██████▍   | 18/28 [00:07<00:04,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  61%|██████    | 17/28 [00:07<00:04,  2.44it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  68%|██████▊   | 19/28 [00:08<00:03,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  64%|██████▍   | 18/28 [00:07<00:04,  2.44it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  71%|███████▏  | 20/28 [00:08<00:03,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  68%|██████▊   | 19/28 [00:08<00:03,  2.44it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  75%|███████▌  | 21/28 [00:08<00:02,  2.46it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  71%|███████▏  | 20/28 [00:08<00:03,  2.44it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 3:  79%|███████▊  | 22/28 [00:09<00:02,  2.44it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A3)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  75%|███████▌  | 21/28 [00:09<00:02,  2.44it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 3:  86%|████████▌ | 24/28 [00:09<00:01,  2.51it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.30it/s]\u001b[A\n",
      "Epoch 3:  79%|███████▊  | 22/28 [00:09<00:02,  2.41it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A4)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 3:  93%|█████████▎| 26/28 [00:10<00:00,  2.63it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.14it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 3: 100%|██████████| 28/28 [00:10<00:00,  2.75it/s, loss=1.330, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.360, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 3:  86%|████████▌ | 24/28 [00:10<00:01,  2.48it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00001:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-14-32\n",
      "  done: false\n",
      "  experiment_id: 220fad4fc77f49c497bbc5974825553d\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 4\n",
      "  loss: 1.3012152910232544\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684473\n",
      "  time_since_restore: 57.5174674987793\n",
      "  time_this_iter_s: 13.137020587921143\n",
      "  time_total_s: 57.5174674987793\n",
      "  timestamp: 1637525672\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: 7fbfb_00001\n",
      "  \n",
      "Epoch 3: 100%|██████████| 28/28 [00:10<00:00,  2.70it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.29it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 3:  93%|█████████▎| 26/28 [00:10<00:00,  2.61it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.20it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 3: 100%|██████████| 28/28 [00:10<00:00,  2.73it/s, loss=0.944, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00000:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-14-33\n",
      "  done: false\n",
      "  experiment_id: 313cd16ec062412fad43e9ffaa517873\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 4\n",
      "  loss: 0.9542075991630554\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684464\n",
      "  time_since_restore: 58.19283699989319\n",
      "  time_this_iter_s: 13.39782977104187\n",
      "  time_total_s: 58.19283699989319\n",
      "  timestamp: 1637525673\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: 7fbfb_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_deprecation(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:14:33 (running for 00:01:01.18)<br>Memory usage on this node: 28.6/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         58.1928</td><td style=\"text-align: right;\">0.954208</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         57.5175</td><td style=\"text-align: right;\">1.30122 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \r",
      "Epoch 3: 100%|██████████| 28/28 [00:10<00:00,  2.68it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \r",
      "                                                         \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/28 [00:00<00:00, 2783.21it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:   0%|          | 0/28 [00:00<00:00, 1685.14it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:   4%|▎         | 1/28 [00:00<00:11,  2.43it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 4:   7%|▋         | 2/28 [00:01<00:10,  2.45it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  11%|█         | 3/28 [00:01<00:10,  2.45it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:   4%|▎         | 1/28 [00:00<00:11,  2.35it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 4:  14%|█▍        | 4/28 [00:02<00:09,  2.45it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:   7%|▋         | 2/28 [00:01<00:10,  2.38it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  18%|█▊        | 5/28 [00:02<00:09,  2.45it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  11%|█         | 3/28 [00:01<00:10,  2.40it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  21%|██▏       | 6/28 [00:02<00:08,  2.45it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  14%|█▍        | 4/28 [00:02<00:09,  2.42it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  25%|██▌       | 7/28 [00:03<00:08,  2.45it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  18%|█▊        | 5/28 [00:02<00:09,  2.43it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:14:38 (running for 00:01:06.19)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         58.1928</td><td style=\"text-align: right;\">0.954208</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         57.5175</td><td style=\"text-align: right;\">1.30122 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  29%|██▊       | 8/28 [00:03<00:08,  2.45it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  21%|██▏       | 6/28 [00:02<00:09,  2.43it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  32%|███▏      | 9/28 [00:04<00:07,  2.45it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  25%|██▌       | 7/28 [00:03<00:08,  2.44it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  29%|██▊       | 8/28 [00:03<00:08,  2.44it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  36%|███▌      | 10/28 [00:04<00:07,  2.45it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  39%|███▉      | 11/28 [00:04<00:06,  2.45it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  32%|███▏      | 9/28 [00:04<00:07,  2.44it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  36%|███▌      | 10/28 [00:04<00:07,  2.44it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  43%|████▎     | 12/28 [00:05<00:06,  2.45it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  46%|████▋     | 13/28 [00:05<00:06,  2.45it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  39%|███▉      | 11/28 [00:04<00:06,  2.44it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  50%|█████     | 14/28 [00:06<00:05,  2.45it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  43%|████▎     | 12/28 [00:05<00:06,  2.44it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  54%|█████▎    | 15/28 [00:06<00:05,  2.45it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  46%|████▋     | 13/28 [00:05<00:06,  2.44it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  57%|█████▋    | 16/28 [00:06<00:04,  2.45it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  50%|█████     | 14/28 [00:06<00:05,  2.45it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  61%|██████    | 17/28 [00:07<00:04,  2.45it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  54%|█████▎    | 15/28 [00:06<00:05,  2.45it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  64%|██████▍   | 18/28 [00:07<00:04,  2.45it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  57%|█████▋    | 16/28 [00:06<00:04,  2.45it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  61%|██████    | 17/28 [00:07<00:04,  2.45it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  68%|██████▊   | 19/28 [00:08<00:03,  2.45it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:14:43 (running for 00:01:11.20)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         58.1928</td><td style=\"text-align: right;\">0.954208</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         57.5175</td><td style=\"text-align: right;\">1.30122 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  71%|███████▏  | 20/28 [00:08<00:03,  2.45it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  64%|██████▍   | 18/28 [00:07<00:04,  2.45it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  68%|██████▊   | 19/28 [00:08<00:03,  2.45it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  75%|███████▌  | 21/28 [00:08<00:02,  2.45it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  71%|███████▏  | 20/28 [00:08<00:03,  2.45it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  79%|███████▊  | 22/28 [00:09<00:02,  2.43it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A3)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  75%|███████▌  | 21/28 [00:08<00:02,  2.45it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 4:  86%|████████▌ | 24/28 [00:09<00:01,  2.51it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.57it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 4:  93%|█████████▎| 26/28 [00:10<00:00,  2.64it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 4:  79%|███████▊  | 22/28 [00:09<00:02,  2.43it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A4)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.35it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 4: 100%|██████████| 28/28 [00:10<00:00,  2.76it/s, loss=1.270, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.300, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00001:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-14-45\n",
      "  done: false\n",
      "  experiment_id: 220fad4fc77f49c497bbc5974825553d\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 5\n",
      "  loss: 0.9605412483215332\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684473\n",
      "  time_since_restore: 70.4477367401123\n",
      "  time_this_iter_s: 12.930269241333008\n",
      "  time_total_s: 70.4477367401123\n",
      "  timestamp: 1637525685\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: 7fbfb_00001\n",
      "  \n",
      "Epoch 4: 100%|██████████| 28/28 [00:10<00:00,  2.71it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 4:  86%|████████▌ | 24/28 [00:10<00:01,  2.49it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.15it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 4:  93%|█████████▎| 26/28 [00:10<00:00,  2.61it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.00it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 4: 100%|██████████| 28/28 [00:10<00:00,  2.73it/s, loss=0.943, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.954, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00000:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-14-46\n",
      "  done: false\n",
      "  experiment_id: 313cd16ec062412fad43e9ffaa517873\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 5\n",
      "  loss: 0.9621781706809998\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684464\n",
      "  time_since_restore: 71.39581298828125\n",
      "  time_this_iter_s: 13.202975988388062\n",
      "  time_total_s: 71.39581298828125\n",
      "  timestamp: 1637525686\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: 7fbfb_00000\n",
      "  \n",
      "Epoch 4: 100%|██████████| 28/28 [00:10<00:00,  2.67it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \r",
      "Epoch 4:   0%|          | 0/28 [00:00<00:00, 8811.56it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\r",
      "Epoch 5:   0%|          | 0/28 [00:00<00:00, 2659.67it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:14:48 (running for 00:01:16.38)<br>Memory usage on this node: 29.4/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         71.3958</td><td style=\"text-align: right;\">0.962178</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         70.4477</td><td style=\"text-align: right;\">0.960541</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:   4%|▎         | 1/28 [00:00<00:11,  2.42it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 5:   0%|          | 0/28 [00:00<00:00, 1375.18it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:   7%|▋         | 2/28 [00:01<00:10,  2.44it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  11%|█         | 3/28 [00:01<00:10,  2.44it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  14%|█▍        | 4/28 [00:02<00:09,  2.44it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:   4%|▎         | 1/28 [00:00<00:11,  2.31it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 5:  18%|█▊        | 5/28 [00:02<00:09,  2.44it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:   7%|▋         | 2/28 [00:01<00:10,  2.37it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  21%|██▏       | 6/28 [00:02<00:08,  2.45it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  11%|█         | 3/28 [00:01<00:10,  2.39it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  25%|██▌       | 7/28 [00:03<00:08,  2.45it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  14%|█▍        | 4/28 [00:02<00:09,  2.41it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  29%|██▊       | 8/28 [00:03<00:08,  2.44it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  18%|█▊        | 5/28 [00:02<00:09,  2.42it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  32%|███▏      | 9/28 [00:04<00:07,  2.45it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  21%|██▏       | 6/28 [00:02<00:09,  2.43it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  36%|███▌      | 10/28 [00:04<00:07,  2.45it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  25%|██▌       | 7/28 [00:03<00:08,  2.43it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  39%|███▉      | 11/28 [00:04<00:06,  2.45it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  29%|██▊       | 8/28 [00:03<00:08,  2.43it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  43%|████▎     | 12/28 [00:05<00:06,  2.45it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  32%|███▏      | 9/28 [00:04<00:07,  2.43it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:14:53 (running for 00:01:21.39)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         71.3958</td><td style=\"text-align: right;\">0.962178</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         70.4477</td><td style=\"text-align: right;\">0.960541</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  46%|████▋     | 13/28 [00:05<00:06,  2.45it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  36%|███▌      | 10/28 [00:04<00:07,  2.44it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  50%|█████     | 14/28 [00:06<00:05,  2.45it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  39%|███▉      | 11/28 [00:04<00:06,  2.44it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  54%|█████▎    | 15/28 [00:06<00:05,  2.45it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  43%|████▎     | 12/28 [00:05<00:06,  2.44it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  57%|█████▋    | 16/28 [00:06<00:04,  2.45it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  46%|████▋     | 13/28 [00:05<00:06,  2.44it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  61%|██████    | 17/28 [00:07<00:04,  2.45it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  50%|█████     | 14/28 [00:06<00:05,  2.44it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  64%|██████▍   | 18/28 [00:07<00:04,  2.45it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  54%|█████▎    | 15/28 [00:06<00:05,  2.44it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  68%|██████▊   | 19/28 [00:08<00:03,  2.45it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  57%|█████▋    | 16/28 [00:06<00:04,  2.44it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  71%|███████▏  | 20/28 [00:08<00:03,  2.45it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  61%|██████    | 17/28 [00:07<00:04,  2.44it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  75%|███████▌  | 21/28 [00:08<00:02,  2.45it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  64%|██████▍   | 18/28 [00:07<00:04,  2.45it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  68%|██████▊   | 19/28 [00:08<00:03,  2.45it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 5:  79%|███████▊  | 22/28 [00:09<00:02,  2.43it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A3)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  71%|███████▏  | 20/28 [00:08<00:03,  2.45it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 5:  86%|████████▌ | 24/28 [00:09<00:01,  2.51it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.52it/s]\u001b[A\n",
      "Epoch 5:  75%|███████▌  | 21/28 [00:08<00:02,  2.45it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 5:  93%|█████████▎| 26/28 [00:10<00:00,  2.64it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.35it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 5: 100%|██████████| 28/28 [00:10<00:00,  2.76it/s, loss=0.953, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.961, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00001:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-14-58\n",
      "  done: false\n",
      "  experiment_id: 220fad4fc77f49c497bbc5974825553d\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 6\n",
      "  loss: 1.101098656654358\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684473\n",
      "  time_since_restore: 83.38338589668274\n",
      "  time_this_iter_s: 12.935649156570435\n",
      "  time_total_s: 83.38338589668274\n",
      "  timestamp: 1637525698\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 6\n",
      "  trial_id: 7fbfb_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:14:58 (running for 00:01:26.40)<br>Memory usage on this node: 28.1/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         71.3958</td><td style=\"text-align: right;\">0.962178</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         83.3834</td><td style=\"text-align: right;\">1.1011  </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \r",
      "Epoch 5: 100%|██████████| 28/28 [00:10<00:00,  2.71it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \r",
      "                                                         \u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \r",
      "Epoch 5:  79%|███████▊  | 22/28 [00:09<00:02,  2.42it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\r",
      "Epoch 5:  79%|███████▊  | 22/28 [00:09<00:02,  2.42it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \r",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \r",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 5:  86%|████████▌ | 24/28 [00:10<00:01,  2.47it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  2.98it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 5:  93%|█████████▎| 26/28 [00:10<00:00,  2.60it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:01<00:00,  4.92it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 5: 100%|██████████| 28/28 [00:10<00:00,  2.72it/s, loss=0.950, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.962, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00000:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-14-59\n",
      "  done: false\n",
      "  experiment_id: 313cd16ec062412fad43e9ffaa517873\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 6\n",
      "  loss: 0.9747904539108276\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684464\n",
      "  time_since_restore: 84.79104924201965\n",
      "  time_this_iter_s: 13.395236253738403\n",
      "  time_total_s: 84.79104924201965\n",
      "  timestamp: 1637525699\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 6\n",
      "  trial_id: 7fbfb_00000\n",
      "  \n",
      "Epoch 5: 100%|██████████| 28/28 [00:10<00:00,  2.66it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:   0%|          | 0/28 [00:00<00:00, 2734.23it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:   4%|▎         | 1/28 [00:00<00:11,  2.42it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 6:   7%|▋         | 2/28 [00:01<00:10,  2.43it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  11%|█         | 3/28 [00:01<00:10,  2.44it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:   0%|          | 0/28 [00:00<00:00, 2603.54it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  14%|█▍        | 4/28 [00:02<00:09,  2.43it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  18%|█▊        | 5/28 [00:02<00:09,  2.44it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:   4%|▎         | 1/28 [00:00<00:11,  2.25it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 6:  21%|██▏       | 6/28 [00:02<00:09,  2.44it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:   7%|▋         | 2/28 [00:01<00:11,  2.32it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:15:03 (running for 00:01:31.78)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         84.791 </td><td style=\"text-align: right;\">0.97479</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         83.3834</td><td style=\"text-align: right;\">1.1011 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  25%|██▌       | 7/28 [00:03<00:08,  2.44it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  11%|█         | 3/28 [00:01<00:10,  2.35it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  29%|██▊       | 8/28 [00:03<00:08,  2.44it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  14%|█▍        | 4/28 [00:02<00:10,  2.36it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  32%|███▏      | 9/28 [00:04<00:07,  2.44it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  18%|█▊        | 5/28 [00:02<00:09,  2.38it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  36%|███▌      | 10/28 [00:04<00:07,  2.44it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  21%|██▏       | 6/28 [00:02<00:09,  2.39it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  39%|███▉      | 11/28 [00:04<00:06,  2.44it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  25%|██▌       | 7/28 [00:03<00:08,  2.40it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  43%|████▎     | 12/28 [00:05<00:06,  2.44it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  29%|██▊       | 8/28 [00:03<00:08,  2.40it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  46%|████▋     | 13/28 [00:05<00:06,  2.44it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  32%|███▏      | 9/28 [00:04<00:07,  2.41it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  50%|█████     | 14/28 [00:06<00:05,  2.44it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  36%|███▌      | 10/28 [00:04<00:07,  2.42it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  54%|█████▎    | 15/28 [00:06<00:05,  2.44it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  39%|███▉      | 11/28 [00:04<00:07,  2.42it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  57%|█████▋    | 16/28 [00:06<00:04,  2.44it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  43%|████▎     | 12/28 [00:05<00:06,  2.42it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  61%|██████    | 17/28 [00:07<00:04,  2.44it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  46%|████▋     | 13/28 [00:05<00:06,  2.43it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  64%|██████▍   | 18/28 [00:07<00:04,  2.44it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  50%|█████     | 14/28 [00:06<00:05,  2.43it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:15:08 (running for 00:01:36.79)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         84.791 </td><td style=\"text-align: right;\">0.97479</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         83.3834</td><td style=\"text-align: right;\">1.1011 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  68%|██████▊   | 19/28 [00:08<00:03,  2.44it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  54%|█████▎    | 15/28 [00:06<00:05,  2.43it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  71%|███████▏  | 20/28 [00:08<00:03,  2.44it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  57%|█████▋    | 16/28 [00:06<00:04,  2.43it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  75%|███████▌  | 21/28 [00:09<00:02,  2.44it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  61%|██████    | 17/28 [00:07<00:04,  2.44it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  64%|██████▍   | 18/28 [00:07<00:04,  2.44it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  79%|███████▊  | 22/28 [00:09<00:02,  2.42it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A3)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  68%|██████▊   | 19/28 [00:08<00:03,  2.44it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 6:  86%|████████▌ | 24/28 [00:10<00:01,  2.49it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.45it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 6:  93%|█████████▎| 26/28 [00:10<00:00,  2.62it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  71%|███████▏  | 20/28 [00:08<00:03,  2.44it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.28it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 6: 100%|██████████| 28/28 [00:10<00:00,  2.74it/s, loss=1.080, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 6:  75%|███████▌  | 21/28 [00:09<00:02,  2.44it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00001:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-15-11\n",
      "  done: false\n",
      "  experiment_id: 220fad4fc77f49c497bbc5974825553d\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 7\n",
      "  loss: 1.272944450378418\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684473\n",
      "  time_since_restore: 96.38470673561096\n",
      "  time_this_iter_s: 13.001320838928223\n",
      "  time_total_s: 96.38470673561096\n",
      "  timestamp: 1637525711\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 7\n",
      "  trial_id: 7fbfb_00001\n",
      "  \n",
      "Epoch 6: 100%|██████████| 28/28 [00:10<00:00,  2.69it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  79%|███████▊  | 22/28 [00:09<00:02,  2.41it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A4)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 6:  86%|████████▌ | 24/28 [00:10<00:01,  2.47it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.05it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 6:  93%|█████████▎| 26/28 [00:10<00:00,  2.59it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:01<00:00,  4.98it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 6: 100%|██████████| 28/28 [00:10<00:00,  2.71it/s, loss=0.962, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.975, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00000:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-15-13\n",
      "  done: false\n",
      "  experiment_id: 313cd16ec062412fad43e9ffaa517873\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 7\n",
      "  loss: 0.9818439483642578\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684464\n",
      "  time_since_restore: 98.14199447631836\n",
      "  time_this_iter_s: 13.350945234298706\n",
      "  time_total_s: 98.14199447631836\n",
      "  timestamp: 1637525713\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 7\n",
      "  trial_id: 7fbfb_00000\n",
      "  \n",
      "Epoch 6: 100%|██████████| 28/28 [00:10<00:00,  2.66it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \r",
      "Epoch 6:   0%|          | 0/28 [00:00<00:00, 5336.26it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\r",
      "Epoch 7:   0%|          | 0/28 [00:00<00:00, 1787.85it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:15:14 (running for 00:01:42.13)<br>Memory usage on this node: 28.9/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         98.142 </td><td style=\"text-align: right;\">0.981844</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         96.3847</td><td style=\"text-align: right;\">1.27294 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:   4%|▎         | 1/28 [00:00<00:11,  2.45it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 7:   7%|▋         | 2/28 [00:01<00:10,  2.46it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  11%|█         | 3/28 [00:01<00:10,  2.46it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  14%|█▍        | 4/28 [00:02<00:09,  2.45it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:   0%|          | 0/28 [00:00<00:00, 2423.05it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  18%|█▊        | 5/28 [00:02<00:09,  2.45it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  21%|██▏       | 6/28 [00:02<00:08,  2.45it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:   4%|▎         | 1/28 [00:00<00:11,  2.30it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 7:  25%|██▌       | 7/28 [00:03<00:08,  2.45it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:   7%|▋         | 2/28 [00:01<00:11,  2.36it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  29%|██▊       | 8/28 [00:03<00:08,  2.45it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  11%|█         | 3/28 [00:01<00:10,  2.39it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  32%|███▏      | 9/28 [00:04<00:07,  2.45it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  14%|█▍        | 4/28 [00:02<00:09,  2.40it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  36%|███▌      | 10/28 [00:04<00:07,  2.45it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  18%|█▊        | 5/28 [00:02<00:09,  2.42it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  39%|███▉      | 11/28 [00:04<00:06,  2.45it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  21%|██▏       | 6/28 [00:02<00:09,  2.43it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  43%|████▎     | 12/28 [00:05<00:06,  2.45it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  25%|██▌       | 7/28 [00:03<00:08,  2.43it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:15:19 (running for 00:01:47.14)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         98.142 </td><td style=\"text-align: right;\">0.981844</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         96.3847</td><td style=\"text-align: right;\">1.27294 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:  29%|██▊       | 8/28 [00:03<00:08,  2.43it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  46%|████▋     | 13/28 [00:05<00:06,  2.45it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  50%|█████     | 14/28 [00:06<00:05,  2.45it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  32%|███▏      | 9/28 [00:04<00:07,  2.44it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  36%|███▌      | 10/28 [00:04<00:07,  2.44it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  54%|█████▎    | 15/28 [00:06<00:05,  2.45it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  57%|█████▋    | 16/28 [00:06<00:04,  2.45it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  39%|███▉      | 11/28 [00:04<00:06,  2.44it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  61%|██████    | 17/28 [00:07<00:04,  2.45it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  43%|████▎     | 12/28 [00:05<00:06,  2.45it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  64%|██████▍   | 18/28 [00:07<00:04,  2.45it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  46%|████▋     | 13/28 [00:05<00:06,  2.45it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  68%|██████▊   | 19/28 [00:08<00:03,  2.45it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  50%|█████     | 14/28 [00:06<00:05,  2.45it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  54%|█████▎    | 15/28 [00:06<00:05,  2.45it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  71%|███████▏  | 20/28 [00:08<00:03,  2.45it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  75%|███████▌  | 21/28 [00:08<00:02,  2.45it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  57%|█████▋    | 16/28 [00:06<00:04,  2.45it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  61%|██████    | 17/28 [00:07<00:04,  2.46it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  79%|███████▊  | 22/28 [00:09<00:02,  2.43it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A3)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  64%|██████▍   | 18/28 [00:07<00:04,  2.46it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 7:  86%|████████▌ | 24/28 [00:09<00:01,  2.51it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.50it/s]\u001b[A\n",
      "Epoch 7:  68%|██████▊   | 19/28 [00:08<00:03,  2.46it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 7:  93%|█████████▎| 26/28 [00:10<00:00,  2.63it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.26it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:15:24 (running for 00:01:52.14)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         98.142 </td><td style=\"text-align: right;\">0.981844</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         96.3847</td><td style=\"text-align: right;\">1.27294 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \r",
      "Validating:  83%|████████▎ | 5/6 [00:01<00:00,  5.75it/s]\u001b[A\r",
      "Epoch 7: 100%|██████████| 28/28 [00:10<00:00,  2.75it/s, loss=1.240, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.270, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \r",
      "Epoch 7:  71%|███████▏  | 20/28 [00:08<00:03,  2.46it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\r",
      "Epoch 7:  71%|███████▏  | 20/28 [00:08<00:03,  2.46it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00001:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-15-24\n",
      "  done: false\n",
      "  experiment_id: 220fad4fc77f49c497bbc5974825553d\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 8\n",
      "  loss: 1.2528069019317627\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684473\n",
      "  time_since_restore: 109.31054377555847\n",
      "  time_this_iter_s: 12.92583703994751\n",
      "  time_total_s: 109.31054377555847\n",
      "  timestamp: 1637525724\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 8\n",
      "  trial_id: 7fbfb_00001\n",
      "  \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \r",
      "Epoch 7: 100%|██████████| 28/28 [00:10<00:00,  2.70it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \r",
      "                                                         \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:  75%|███████▌  | 21/28 [00:08<00:02,  2.46it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 7:  79%|███████▊  | 22/28 [00:09<00:02,  2.43it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A4)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 7:  86%|████████▌ | 24/28 [00:10<00:01,  2.49it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.20it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 7:  93%|█████████▎| 26/28 [00:10<00:00,  2.62it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.09it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 7: 100%|██████████| 28/28 [00:10<00:00,  2.74it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00000:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-15-26\n",
      "  done: false\n",
      "  experiment_id: 313cd16ec062412fad43e9ffaa517873\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 8\n",
      "  loss: 0.9872599244117737\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684464\n",
      "  time_since_restore: 111.4210855960846\n",
      "  time_this_iter_s: 13.279091119766235\n",
      "  time_total_s: 111.4210855960846\n",
      "  timestamp: 1637525726\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 8\n",
      "  trial_id: 7fbfb_00000\n",
      "  \n",
      "Epoch 7: 100%|██████████| 28/28 [00:10<00:00,  2.68it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n",
      "Epoch 8:   0%|          | 0/28 [00:00<00:00, 2531.26it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:   4%|▎         | 1/28 [00:00<00:11,  2.45it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 8:   7%|▋         | 2/28 [00:01<00:10,  2.44it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  11%|█         | 3/28 [00:01<00:10,  2.44it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  14%|█▍        | 4/28 [00:02<00:09,  2.43it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:   0%|          | 0/28 [00:00<00:00, 2851.33it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  18%|█▊        | 5/28 [00:02<00:09,  2.43it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:15:29 (running for 00:01:57.41)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         111.421</td><td style=\"text-align: right;\">0.98726</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         109.311</td><td style=\"text-align: right;\">1.25281</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  21%|██▏       | 6/28 [00:02<00:09,  2.44it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:   4%|▎         | 1/28 [00:00<00:11,  2.29it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 8:  25%|██▌       | 7/28 [00:03<00:08,  2.44it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:   7%|▋         | 2/28 [00:01<00:11,  2.33it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  29%|██▊       | 8/28 [00:03<00:08,  2.44it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  11%|█         | 3/28 [00:01<00:10,  2.36it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  32%|███▏      | 9/28 [00:04<00:07,  2.44it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  14%|█▍        | 4/28 [00:02<00:10,  2.38it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  36%|███▌      | 10/28 [00:04<00:07,  2.44it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  18%|█▊        | 5/28 [00:02<00:09,  2.40it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  39%|███▉      | 11/28 [00:04<00:06,  2.44it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  21%|██▏       | 6/28 [00:02<00:09,  2.41it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  43%|████▎     | 12/28 [00:05<00:06,  2.44it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  25%|██▌       | 7/28 [00:03<00:08,  2.42it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  46%|████▋     | 13/28 [00:05<00:06,  2.44it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  29%|██▊       | 8/28 [00:03<00:08,  2.42it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  50%|█████     | 14/28 [00:06<00:05,  2.44it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  32%|███▏      | 9/28 [00:04<00:07,  2.43it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  54%|█████▎    | 15/28 [00:06<00:05,  2.44it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  36%|███▌      | 10/28 [00:04<00:07,  2.43it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  57%|█████▋    | 16/28 [00:06<00:04,  2.44it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  39%|███▉      | 11/28 [00:04<00:06,  2.43it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  61%|██████    | 17/28 [00:07<00:04,  2.44it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  43%|████▎     | 12/28 [00:05<00:06,  2.43it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  64%|██████▍   | 18/28 [00:07<00:04,  2.44it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:15:34 (running for 00:02:02.42)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         111.421</td><td style=\"text-align: right;\">0.98726</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         109.311</td><td style=\"text-align: right;\">1.25281</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  46%|████▋     | 13/28 [00:05<00:06,  2.44it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  68%|██████▊   | 19/28 [00:08<00:03,  2.44it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  50%|█████     | 14/28 [00:06<00:05,  2.44it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  71%|███████▏  | 20/28 [00:08<00:03,  2.44it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  54%|█████▎    | 15/28 [00:06<00:05,  2.44it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  75%|███████▌  | 21/28 [00:09<00:02,  2.44it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  57%|█████▋    | 16/28 [00:06<00:04,  2.44it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  79%|███████▊  | 22/28 [00:09<00:02,  2.42it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A3)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  61%|██████    | 17/28 [00:07<00:04,  2.44it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 8:  86%|████████▌ | 24/28 [00:10<00:01,  2.50it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  64%|██████▍   | 18/28 [00:07<00:04,  2.45it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.43it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 8:  93%|█████████▎| 26/28 [00:10<00:00,  2.63it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.27it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 8: 100%|██████████| 28/28 [00:10<00:00,  2.74it/s, loss=1.220, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.250, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  68%|██████▊   | 19/28 [00:08<00:03,  2.45it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00001:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-15-37\n",
      "  done: false\n",
      "  experiment_id: 220fad4fc77f49c497bbc5974825553d\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 9\n",
      "  loss: 1.5841922760009766\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684473\n",
      "  time_since_restore: 122.25242161750793\n",
      "  time_this_iter_s: 12.941877841949463\n",
      "  time_total_s: 122.25242161750793\n",
      "  timestamp: 1637525737\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 9\n",
      "  trial_id: 7fbfb_00001\n",
      "  \n",
      "Epoch 8: 100%|██████████| 28/28 [00:10<00:00,  2.70it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n",
      "Epoch 8:  71%|███████▏  | 20/28 [00:08<00:03,  2.45it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  75%|███████▌  | 21/28 [00:08<00:02,  2.45it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 8:  79%|███████▊  | 22/28 [00:09<00:02,  2.42it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A4)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 8:  86%|████████▌ | 24/28 [00:10<00:01,  2.47it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.02it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 8:  93%|█████████▎| 26/28 [00:10<00:00,  2.60it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:01<00:00,  4.95it/s]\u001b[A\n",
      "Epoch 9:   0%|          | 0/28 [00:00<00:00, 2764.87it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 8: 100%|██████████| 28/28 [00:10<00:00,  2.72it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.987, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00000:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-15-39\n",
      "  done: false\n",
      "  experiment_id: 313cd16ec062412fad43e9ffaa517873\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 9\n",
      "  loss: 0.9881195425987244\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684464\n",
      "  time_since_restore: 124.76351761817932\n",
      "  time_this_iter_s: 13.342432022094727\n",
      "  time_total_s: 124.76351761817932\n",
      "  timestamp: 1637525739\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 9\n",
      "  trial_id: 7fbfb_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:15:39 (running for 00:02:07.75)<br>Memory usage on this node: 28.2/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         124.764</td><td style=\"text-align: right;\">0.98812</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         122.252</td><td style=\"text-align: right;\">1.58419</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \r",
      "Epoch 8: 100%|██████████| 28/28 [00:10<00:00,  2.66it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \r",
      "                                                         \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:   4%|▎         | 1/28 [00:00<00:11,  2.35it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 9:   7%|▋         | 2/28 [00:01<00:10,  2.38it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  11%|█         | 3/28 [00:01<00:10,  2.39it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  14%|█▍        | 4/28 [00:02<00:09,  2.40it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  18%|█▊        | 5/28 [00:02<00:09,  2.41it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:   0%|          | 0/28 [00:00<00:00, 1522.43it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  21%|██▏       | 6/28 [00:02<00:09,  2.42it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  25%|██▌       | 7/28 [00:03<00:08,  2.42it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:   4%|▎         | 1/28 [00:00<00:11,  2.25it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 9:  29%|██▊       | 8/28 [00:03<00:08,  2.42it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:   7%|▋         | 2/28 [00:01<00:11,  2.32it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  32%|███▏      | 9/28 [00:04<00:07,  2.42it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  11%|█         | 3/28 [00:01<00:10,  2.35it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  36%|███▌      | 10/28 [00:04<00:07,  2.43it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  14%|█▍        | 4/28 [00:02<00:10,  2.36it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  39%|███▉      | 11/28 [00:04<00:07,  2.42it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  18%|█▊        | 5/28 [00:02<00:09,  2.38it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:15:44 (running for 00:02:12.76)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         124.764</td><td style=\"text-align: right;\">0.98812</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         122.252</td><td style=\"text-align: right;\">1.58419</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  43%|████▎     | 12/28 [00:05<00:06,  2.42it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  21%|██▏       | 6/28 [00:02<00:09,  2.39it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  46%|████▋     | 13/28 [00:05<00:06,  2.43it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  25%|██▌       | 7/28 [00:03<00:08,  2.40it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  50%|█████     | 14/28 [00:06<00:05,  2.43it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  29%|██▊       | 8/28 [00:03<00:08,  2.40it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  54%|█████▎    | 15/28 [00:06<00:05,  2.43it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  32%|███▏      | 9/28 [00:04<00:07,  2.41it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  57%|█████▋    | 16/28 [00:06<00:04,  2.43it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  36%|███▌      | 10/28 [00:04<00:07,  2.41it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  61%|██████    | 17/28 [00:07<00:04,  2.43it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  39%|███▉      | 11/28 [00:04<00:07,  2.42it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  64%|██████▍   | 18/28 [00:07<00:04,  2.43it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  43%|████▎     | 12/28 [00:05<00:06,  2.42it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  68%|██████▊   | 19/28 [00:08<00:03,  2.43it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  46%|████▋     | 13/28 [00:05<00:06,  2.42it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  71%|███████▏  | 20/28 [00:08<00:03,  2.43it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  50%|█████     | 14/28 [00:06<00:05,  2.42it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  75%|███████▌  | 21/28 [00:09<00:02,  2.43it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  54%|█████▎    | 15/28 [00:06<00:05,  2.43it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  79%|███████▊  | 22/28 [00:09<00:02,  2.41it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A3)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  57%|█████▋    | 16/28 [00:06<00:04,  2.43it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 9:  86%|████████▌ | 24/28 [00:10<00:01,  2.48it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  61%|██████    | 17/28 [00:07<00:04,  2.43it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.38it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:15:49 (running for 00:02:17.77)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         124.764</td><td style=\"text-align: right;\">0.98812</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         122.252</td><td style=\"text-align: right;\">1.58419</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 9:  93%|█████████▎| 26/28 [00:10<00:00,  2.61it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.23it/s]\u001b[A\n",
      "Epoch 9:  64%|██████▍   | 18/28 [00:07<00:04,  2.43it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 9: 100%|██████████| 28/28 [00:10<00:00,  2.73it/s, loss=1.540, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.580, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00001:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-15-50\n",
      "  done: false\n",
      "  experiment_id: 220fad4fc77f49c497bbc5974825553d\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 10\n",
      "  loss: 2.8608627319335938\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684473\n",
      "  time_since_restore: 135.2492926120758\n",
      "  time_this_iter_s: 12.996870994567871\n",
      "  time_total_s: 135.2492926120758\n",
      "  timestamp: 1637525750\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: 7fbfb_00001\n",
      "  \n",
      "Epoch 9: 100%|██████████| 28/28 [00:10<00:00,  2.68it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  68%|██████▊   | 19/28 [00:08<00:03,  2.43it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  71%|███████▏  | 20/28 [00:08<00:03,  2.44it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  75%|███████▌  | 21/28 [00:09<00:02,  2.44it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 9:  79%|███████▊  | 22/28 [00:09<00:02,  2.41it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A4)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 9:  86%|████████▌ | 24/28 [00:10<00:01,  2.47it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.19it/s]\u001b[A\n",
      "Epoch 10:   0%|          | 0/28 [00:00<00:00, 2669.83it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 9:  93%|█████████▎| 26/28 [00:10<00:00,  2.60it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.09it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 9: 100%|██████████| 28/28 [00:10<00:00,  2.72it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00000:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-15-53\n",
      "  done: false\n",
      "  experiment_id: 313cd16ec062412fad43e9ffaa517873\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.9778022766113281\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684464\n",
      "  time_since_restore: 138.02128624916077\n",
      "  time_this_iter_s: 13.257768630981445\n",
      "  time_total_s: 138.02128624916077\n",
      "  timestamp: 1637525753\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: 7fbfb_00000\n",
      "  \n",
      "Epoch 9: 100%|██████████| 28/28 [00:10<00:00,  2.66it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:   4%|▎         | 1/28 [00:00<00:11,  2.42it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 10:   7%|▋         | 2/28 [00:01<00:10,  2.43it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  11%|█         | 3/28 [00:01<00:10,  2.43it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  14%|█▍        | 4/28 [00:02<00:09,  2.42it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  18%|█▊        | 5/28 [00:02<00:09,  2.42it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:15:55 (running for 00:02:23.01)<br>Memory usage on this node: 29.4/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         138.021</td><td style=\"text-align: right;\">0.977802</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         135.249</td><td style=\"text-align: right;\">2.86086 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  21%|██▏       | 6/28 [00:02<00:09,  2.43it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:   0%|          | 0/28 [00:00<00:00, 2376.38it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  25%|██▌       | 7/28 [00:03<00:08,  2.43it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  29%|██▊       | 8/28 [00:03<00:08,  2.43it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:   4%|▎         | 1/28 [00:00<00:11,  2.32it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 10:  32%|███▏      | 9/28 [00:04<00:07,  2.43it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:   7%|▋         | 2/28 [00:01<00:10,  2.37it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  36%|███▌      | 10/28 [00:04<00:07,  2.43it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  11%|█         | 3/28 [00:01<00:10,  2.38it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  39%|███▉      | 11/28 [00:04<00:06,  2.43it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  14%|█▍        | 4/28 [00:02<00:10,  2.39it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  43%|████▎     | 12/28 [00:05<00:06,  2.43it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  18%|█▊        | 5/28 [00:02<00:09,  2.40it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  46%|████▋     | 13/28 [00:05<00:06,  2.43it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  21%|██▏       | 6/28 [00:02<00:09,  2.42it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  50%|█████     | 14/28 [00:06<00:05,  2.43it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  25%|██▌       | 7/28 [00:03<00:08,  2.42it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  54%|█████▎    | 15/28 [00:06<00:05,  2.43it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  29%|██▊       | 8/28 [00:03<00:08,  2.43it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  57%|█████▋    | 16/28 [00:06<00:04,  2.43it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  32%|███▏      | 9/28 [00:04<00:07,  2.42it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  61%|██████    | 17/28 [00:07<00:04,  2.43it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:16:00 (running for 00:02:28.02)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         138.021</td><td style=\"text-align: right;\">0.977802</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         135.249</td><td style=\"text-align: right;\">2.86086 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  36%|███▌      | 10/28 [00:04<00:07,  2.43it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  64%|██████▍   | 18/28 [00:07<00:04,  2.43it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  39%|███▉      | 11/28 [00:04<00:06,  2.43it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  68%|██████▊   | 19/28 [00:08<00:03,  2.43it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  43%|████▎     | 12/28 [00:05<00:06,  2.43it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  71%|███████▏  | 20/28 [00:08<00:03,  2.43it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  46%|████▋     | 13/28 [00:05<00:06,  2.43it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  75%|███████▌  | 21/28 [00:09<00:02,  2.43it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  50%|█████     | 14/28 [00:06<00:05,  2.43it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  54%|█████▎    | 15/28 [00:06<00:05,  2.43it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  79%|███████▊  | 22/28 [00:09<00:02,  2.41it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A3)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  57%|█████▋    | 16/28 [00:06<00:04,  2.44it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 10:  86%|████████▌ | 24/28 [00:10<00:01,  2.49it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.52it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 10:  93%|█████████▎| 26/28 [00:10<00:00,  2.61it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  61%|██████    | 17/28 [00:07<00:04,  2.43it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.33it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 10: 100%|██████████| 28/28 [00:10<00:00,  2.73it/s, loss=2.790, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=2.860, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00001:\n",
      "  acc: 0.17428861558437347\n",
      "  date: 2021-11-21_15-16-03\n",
      "  done: false\n",
      "  experiment_id: 220fad4fc77f49c497bbc5974825553d\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 11\n",
      "  loss: 1.0958223342895508\n",
      "  macro_f1: 0.09812276810407639\n",
      "  micro_f1: 0.17428861558437347\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684473\n",
      "  time_since_restore: 148.23371171951294\n",
      "  time_this_iter_s: 12.984419107437134\n",
      "  time_total_s: 148.23371171951294\n",
      "  timestamp: 1637525763\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 11\n",
      "  trial_id: 7fbfb_00001\n",
      "  \n",
      "Epoch 10: 100%|██████████| 28/28 [00:10<00:00,  2.68it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "                                                         \u001b[A\n",
      "Epoch 10:  64%|██████▍   | 18/28 [00:07<00:04,  2.43it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  68%|██████▊   | 19/28 [00:08<00:03,  2.44it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  71%|███████▏  | 20/28 [00:08<00:03,  2.44it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  75%|███████▌  | 21/28 [00:09<00:02,  2.44it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 10:  79%|███████▊  | 22/28 [00:09<00:02,  2.41it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A4)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:16:05 (running for 00:02:33.25)<br>Memory usage on this node: 29.4/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         138.021</td><td style=\"text-align: right;\">0.977802</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\"> 0.252275 </td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         148.234</td><td style=\"text-align: right;\">1.09582 </td><td style=\"text-align: right;\">0.174289</td><td style=\"text-align: right;\"> 0.0981228</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:   0%|          | 0/28 [00:00<00:00, 2139.95it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 10:  86%|████████▌ | 24/28 [00:10<00:01,  2.46it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.00it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 10:  93%|█████████▎| 26/28 [00:10<00:00,  2.58it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:01<00:00,  4.89it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 10: 100%|██████████| 28/28 [00:10<00:00,  2.70it/s, loss=0.966, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.978, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 11:   4%|▎         | 1/28 [00:00<00:10,  2.47it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]  \n",
      "Result for execute_7fbfb_00000:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-16-06\n",
      "  done: false\n",
      "  experiment_id: 313cd16ec062412fad43e9ffaa517873\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 11\n",
      "  loss: 0.9881485104560852\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684464\n",
      "  time_since_restore: 151.34115552902222\n",
      "  time_this_iter_s: 13.31986927986145\n",
      "  time_total_s: 151.34115552902222\n",
      "  timestamp: 1637525766\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 11\n",
      "  trial_id: 7fbfb_00000\n",
      "  \n",
      "Epoch 10: 100%|██████████| 28/28 [00:10<00:00,  2.65it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:   7%|▋         | 2/28 [00:01<00:10,  2.47it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 11:  11%|█         | 3/28 [00:01<00:10,  2.45it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 11:  14%|█▍        | 4/28 [00:02<00:09,  2.45it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 11:  18%|█▊        | 5/28 [00:02<00:09,  2.45it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 11:  21%|██▏       | 6/28 [00:02<00:08,  2.45it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 11:  25%|██▌       | 7/28 [00:03<00:08,  2.45it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 11:   0%|          | 0/28 [00:00<00:00, 2513.06it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 11:  29%|██▊       | 8/28 [00:03<00:08,  2.45it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 11:  32%|███▏      | 9/28 [00:04<00:07,  2.44it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 11:   4%|▎         | 1/28 [00:00<00:11,  2.30it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 11:  36%|███▌      | 10/28 [00:04<00:07,  2.45it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 11:   7%|▋         | 2/28 [00:01<00:11,  2.36it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:16:10 (running for 00:02:38.33)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         151.341</td><td style=\"text-align: right;\">0.988149</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\"> 0.252275 </td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         148.234</td><td style=\"text-align: right;\">1.09582 </td><td style=\"text-align: right;\">0.174289</td><td style=\"text-align: right;\"> 0.0981228</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:  39%|███▉      | 11/28 [00:04<00:06,  2.44it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 11:  11%|█         | 3/28 [00:01<00:10,  2.38it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 11:  43%|████▎     | 12/28 [00:05<00:06,  2.44it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 11:  14%|█▍        | 4/28 [00:02<00:10,  2.39it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 11:  46%|████▋     | 13/28 [00:05<00:06,  2.44it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 11:  18%|█▊        | 5/28 [00:02<00:09,  2.41it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 11:  50%|█████     | 14/28 [00:06<00:05,  2.44it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 11:  21%|██▏       | 6/28 [00:02<00:09,  2.41it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 11:  54%|█████▎    | 15/28 [00:06<00:05,  2.44it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 11:  25%|██▌       | 7/28 [00:03<00:08,  2.42it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 11:  57%|█████▋    | 16/28 [00:06<00:04,  2.44it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 11:  29%|██▊       | 8/28 [00:03<00:08,  2.42it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 11:  61%|██████    | 17/28 [00:07<00:04,  2.44it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 11:  32%|███▏      | 9/28 [00:04<00:07,  2.42it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 11:  64%|██████▍   | 18/28 [00:07<00:04,  2.44it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 11:  36%|███▌      | 10/28 [00:04<00:07,  2.42it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 11:  68%|██████▊   | 19/28 [00:08<00:03,  2.44it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 11:  39%|███▉      | 11/28 [00:04<00:07,  2.43it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 11:  71%|███████▏  | 20/28 [00:08<00:03,  2.44it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 11:  43%|████▎     | 12/28 [00:05<00:06,  2.43it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 11:  75%|███████▌  | 21/28 [00:09<00:02,  2.44it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 11:  46%|████▋     | 13/28 [00:05<00:06,  2.43it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 11:  79%|███████▊  | 22/28 [00:09<00:02,  2.42it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A3)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  50%|█████     | 14/28 [00:06<00:05,  2.43it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 11:  54%|█████▎    | 15/28 [00:06<00:05,  2.44it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:16:15 (running for 00:02:43.33)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         151.341</td><td style=\"text-align: right;\">0.988149</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\"> 0.252275 </td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         148.234</td><td style=\"text-align: right;\">1.09582 </td><td style=\"text-align: right;\">0.174289</td><td style=\"text-align: right;\"> 0.0981228</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 11:  86%|████████▌ | 24/28 [00:10<00:01,  2.49it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.35it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 11:  93%|█████████▎| 26/28 [00:10<00:00,  2.62it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 11:  57%|█████▋    | 16/28 [00:06<00:04,  2.44it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.19it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 11: 100%|██████████| 28/28 [00:10<00:00,  2.74it/s, loss=1.090, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.100, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 11:  61%|██████    | 17/28 [00:07<00:04,  2.44it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for execute_7fbfb_00001:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-16-16\n",
      "  done: false\n",
      "  experiment_id: 220fad4fc77f49c497bbc5974825553d\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 12\n",
      "  loss: 1.049596905708313\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684473\n",
      "  time_since_restore: 161.24533534049988\n",
      "  time_this_iter_s: 13.011623620986938\n",
      "  time_total_s: 161.24533534049988\n",
      "  timestamp: 1637525776\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 12\n",
      "  trial_id: 7fbfb_00001\n",
      "  \n",
      "Epoch 11: 100%|██████████| 28/28 [00:10<00:00,  2.69it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "                                                         \u001b[A\n",
      "Epoch 11:  64%|██████▍   | 18/28 [00:07<00:04,  2.44it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 11:  68%|██████▊   | 19/28 [00:08<00:03,  2.44it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 11:  71%|███████▏  | 20/28 [00:08<00:03,  2.44it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 11:  75%|███████▌  | 21/28 [00:08<00:02,  2.44it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 11:  79%|███████▊  | 22/28 [00:09<00:02,  2.42it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A4)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12:   0%|          | 0/28 [00:00<00:00, 2878.73it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 11:  86%|████████▌ | 24/28 [00:10<00:01,  2.47it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.01it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 11:  93%|█████████▎| 26/28 [00:10<00:00,  2.59it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:   4%|▎         | 1/28 [00:00<00:10,  2.46it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:01<00:00,  4.95it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 11: 100%|██████████| 28/28 [00:10<00:00,  2.71it/s, loss=0.977, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.988, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00000:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-16-19\n",
      "  done: false\n",
      "  experiment_id: 313cd16ec062412fad43e9ffaa517873\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 12\n",
      "  loss: 0.9846223592758179\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684464\n",
      "  time_since_restore: 164.61564254760742\n",
      "  time_this_iter_s: 13.274487018585205\n",
      "  time_total_s: 164.61564254760742\n",
      "  timestamp: 1637525779\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 12\n",
      "  trial_id: 7fbfb_00000\n",
      "  \n",
      "Epoch 12:   7%|▋         | 2/28 [00:01<00:10,  2.45it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 11: 100%|██████████| 28/28 [00:10<00:00,  2.66it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:  11%|█         | 3/28 [00:01<00:10,  2.45it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  14%|█▍        | 4/28 [00:02<00:09,  2.44it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:16:20 (running for 00:02:48.60)<br>Memory usage on this node: 29.0/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         164.616</td><td style=\"text-align: right;\">0.984622</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         161.245</td><td style=\"text-align: right;\">1.0496  </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:  18%|█▊        | 5/28 [00:02<00:09,  2.45it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  21%|██▏       | 6/28 [00:02<00:08,  2.45it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  25%|██▌       | 7/28 [00:03<00:08,  2.44it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:   0%|          | 0/28 [00:00<00:00, 2360.33it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  29%|██▊       | 8/28 [00:03<00:08,  2.44it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  32%|███▏      | 9/28 [00:04<00:07,  2.44it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  36%|███▌      | 10/28 [00:04<00:07,  2.44it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:   4%|▎         | 1/28 [00:00<00:11,  2.25it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 12:   7%|▋         | 2/28 [00:01<00:11,  2.32it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  39%|███▉      | 11/28 [00:04<00:06,  2.44it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  43%|████▎     | 12/28 [00:05<00:06,  2.44it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  11%|█         | 3/28 [00:01<00:10,  2.35it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  14%|█▍        | 4/28 [00:02<00:10,  2.37it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  46%|████▋     | 13/28 [00:05<00:06,  2.44it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  50%|█████     | 14/28 [00:06<00:05,  2.44it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  18%|█▊        | 5/28 [00:02<00:09,  2.39it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  21%|██▏       | 6/28 [00:02<00:09,  2.40it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  54%|█████▎    | 15/28 [00:06<00:05,  2.44it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  57%|█████▋    | 16/28 [00:06<00:04,  2.44it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  25%|██▌       | 7/28 [00:03<00:08,  2.41it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  57%|█████▋    | 16/28 [00:06<00:04,  2.44it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:16:25 (running for 00:02:53.61)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         164.616</td><td style=\"text-align: right;\">0.984622</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         161.245</td><td style=\"text-align: right;\">1.0496  </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:  29%|██▊       | 8/28 [00:03<00:08,  2.42it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  61%|██████    | 17/28 [00:07<00:04,  2.44it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  32%|███▏      | 9/28 [00:04<00:07,  2.42it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  64%|██████▍   | 18/28 [00:07<00:04,  2.44it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  36%|███▌      | 10/28 [00:04<00:07,  2.43it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  68%|██████▊   | 19/28 [00:08<00:03,  2.44it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  39%|███▉      | 11/28 [00:04<00:06,  2.43it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  71%|███████▏  | 20/28 [00:08<00:03,  2.44it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  43%|████▎     | 12/28 [00:05<00:06,  2.43it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  75%|███████▌  | 21/28 [00:09<00:02,  2.44it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  46%|████▋     | 13/28 [00:05<00:06,  2.43it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  79%|███████▊  | 22/28 [00:09<00:02,  2.42it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A3)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12:  50%|█████     | 14/28 [00:06<00:05,  2.44it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 12:  86%|████████▌ | 24/28 [00:10<00:01,  2.50it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.39it/s]\u001b[A\n",
      "Epoch 12:  54%|█████▎    | 15/28 [00:06<00:05,  2.44it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 12:  93%|█████████▎| 26/28 [00:10<00:00,  2.62it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.23it/s]\u001b[A\n",
      "Epoch 12:  57%|█████▋    | 16/28 [00:06<00:04,  2.44it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 12: 100%|██████████| 28/28 [00:10<00:00,  2.74it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00001:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-16-29\n",
      "  done: false\n",
      "  experiment_id: 220fad4fc77f49c497bbc5974825553d\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 13\n",
      "  loss: 1.0953091382980347\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684473\n",
      "  time_since_restore: 174.15555953979492\n",
      "  time_this_iter_s: 12.910224199295044\n",
      "  time_total_s: 174.15555953979492\n",
      "  timestamp: 1637525789\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 13\n",
      "  trial_id: 7fbfb_00001\n",
      "  \n",
      "Epoch 12: 100%|██████████| 28/28 [00:10<00:00,  2.69it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:  61%|██████    | 17/28 [00:07<00:04,  2.44it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  64%|██████▍   | 18/28 [00:07<00:04,  2.44it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  68%|██████▊   | 19/28 [00:08<00:03,  2.44it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  71%|███████▏  | 20/28 [00:08<00:03,  2.45it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  75%|███████▌  | 21/28 [00:08<00:02,  2.45it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:16:31 (running for 00:02:59.17)<br>Memory usage on this node: 29.1/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         164.616</td><td style=\"text-align: right;\">0.984622</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         174.156</td><td style=\"text-align: right;\">1.09531 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:   0%|          | 0/28 [00:00<00:00, 2683.50it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 12:  79%|███████▊  | 22/28 [00:09<00:02,  2.42it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A4)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 12:  86%|████████▌ | 24/28 [00:10<00:01,  2.47it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:   4%|▎         | 1/28 [00:00<00:11,  2.33it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  2.98it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 12:  93%|█████████▎| 26/28 [00:10<00:00,  2.60it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:01<00:00,  4.94it/s]\u001b[A\n",
      "Epoch 13:   7%|▋         | 2/28 [00:01<00:10,  2.37it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 12: 100%|██████████| 28/28 [00:10<00:00,  2.72it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00000:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-16-33\n",
      "  done: false\n",
      "  experiment_id: 313cd16ec062412fad43e9ffaa517873\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 13\n",
      "  loss: 0.9821922183036804\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684464\n",
      "  time_since_restore: 177.84688687324524\n",
      "  time_this_iter_s: 13.231244325637817\n",
      "  time_total_s: 177.84688687324524\n",
      "  timestamp: 1637525793\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 13\n",
      "  trial_id: 7fbfb_00000\n",
      "  \n",
      "Epoch 12: 100%|██████████| 28/28 [00:10<00:00,  2.67it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:  11%|█         | 3/28 [00:01<00:10,  2.39it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  14%|█▍        | 4/28 [00:02<00:10,  2.40it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  18%|█▊        | 5/28 [00:02<00:09,  2.40it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  21%|██▏       | 6/28 [00:02<00:09,  2.41it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  25%|██▌       | 7/28 [00:03<00:08,  2.41it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  29%|██▊       | 8/28 [00:03<00:08,  2.42it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:   0%|          | 0/28 [00:00<00:00, 2399.49it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  32%|███▏      | 9/28 [00:04<00:07,  2.42it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  36%|███▌      | 10/28 [00:04<00:07,  2.42it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:   4%|▎         | 1/28 [00:00<00:11,  2.29it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 13:  39%|███▉      | 11/28 [00:04<00:07,  2.42it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:   7%|▋         | 2/28 [00:01<00:11,  2.35it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  43%|████▎     | 12/28 [00:05<00:06,  2.42it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:16:37 (running for 00:03:04.84)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         177.847</td><td style=\"text-align: right;\">0.982192</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         174.156</td><td style=\"text-align: right;\">1.09531 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:  11%|█         | 3/28 [00:01<00:10,  2.38it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  46%|████▋     | 13/28 [00:05<00:06,  2.42it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  14%|█▍        | 4/28 [00:02<00:10,  2.39it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  50%|█████     | 14/28 [00:06<00:05,  2.43it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  18%|█▊        | 5/28 [00:02<00:09,  2.40it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  54%|█████▎    | 15/28 [00:06<00:05,  2.42it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  21%|██▏       | 6/28 [00:02<00:09,  2.41it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  57%|█████▋    | 16/28 [00:07<00:04,  2.43it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  25%|██▌       | 7/28 [00:03<00:08,  2.42it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  61%|██████    | 17/28 [00:07<00:04,  2.43it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  29%|██▊       | 8/28 [00:03<00:08,  2.42it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  64%|██████▍   | 18/28 [00:07<00:04,  2.43it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  32%|███▏      | 9/28 [00:04<00:07,  2.42it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  68%|██████▊   | 19/28 [00:08<00:03,  2.43it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  36%|███▌      | 10/28 [00:04<00:07,  2.42it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  71%|███████▏  | 20/28 [00:08<00:03,  2.43it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  39%|███▉      | 11/28 [00:04<00:07,  2.43it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  75%|███████▌  | 21/28 [00:09<00:02,  2.43it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  43%|████▎     | 12/28 [00:05<00:06,  2.43it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  79%|███████▊  | 22/28 [00:09<00:02,  2.41it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A3)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13:  46%|████▋     | 13/28 [00:05<00:06,  2.43it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 13:  86%|████████▌ | 24/28 [00:10<00:01,  2.49it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  50%|█████     | 14/28 [00:06<00:05,  2.43it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.46it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 13:  93%|█████████▎| 26/28 [00:10<00:00,  2.61it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  54%|█████▎    | 15/28 [00:06<00:05,  2.43it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:16:42 (running for 00:03:09.84)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         177.847</td><td style=\"text-align: right;\">0.982192</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         174.156</td><td style=\"text-align: right;\">1.09531 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.29it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 13: 100%|██████████| 28/28 [00:10<00:00,  2.73it/s, loss=1.070, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.100, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00001:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-16-42\n",
      "  done: false\n",
      "  experiment_id: 220fad4fc77f49c497bbc5974825553d\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 14\n",
      "  loss: 1.2438485622406006\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684473\n",
      "  time_since_restore: 187.1149616241455\n",
      "  time_this_iter_s: 12.959402084350586\n",
      "  time_total_s: 187.1149616241455\n",
      "  timestamp: 1637525802\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 14\n",
      "  trial_id: 7fbfb_00001\n",
      "  \n",
      "Epoch 13: 100%|██████████| 28/28 [00:10<00:00,  2.68it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n",
      "Epoch 13:  57%|█████▋    | 16/28 [00:06<00:04,  2.44it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:  61%|██████    | 17/28 [00:07<00:04,  2.44it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  64%|██████▍   | 18/28 [00:07<00:04,  2.44it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  68%|██████▊   | 19/28 [00:08<00:03,  2.44it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  71%|███████▏  | 20/28 [00:08<00:03,  2.44it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:   0%|          | 0/28 [00:00<00:00, 2600.31it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  75%|███████▌  | 21/28 [00:09<00:02,  2.44it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 13:  79%|███████▊  | 22/28 [00:09<00:02,  2.41it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A4)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:   4%|▎         | 1/28 [00:00<00:11,  2.30it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 13:  86%|████████▌ | 24/28 [00:10<00:01,  2.47it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:   7%|▋         | 2/28 [00:01<00:11,  2.35it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.08it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 13:  93%|█████████▎| 26/28 [00:10<00:00,  2.59it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:01<00:00,  5.01it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 13: 100%|██████████| 28/28 [00:10<00:00,  2.71it/s, loss=0.972, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  11%|█         | 3/28 [00:01<00:10,  2.37it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00000:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-16-46\n",
      "  done: false\n",
      "  experiment_id: 313cd16ec062412fad43e9ffaa517873\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 14\n",
      "  loss: 0.9809864163398743\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684464\n",
      "  time_since_restore: 191.10352396965027\n",
      "  time_this_iter_s: 13.25663709640503\n",
      "  time_total_s: 191.10352396965027\n",
      "  timestamp: 1637525806\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 14\n",
      "  trial_id: 7fbfb_00000\n",
      "  \n",
      "Epoch 13: 100%|██████████| 28/28 [00:10<00:00,  2.66it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:  14%|█▍        | 4/28 [00:02<00:10,  2.38it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  18%|█▊        | 5/28 [00:02<00:09,  2.40it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:16:47 (running for 00:03:15.09)<br>Memory usage on this node: 29.0/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         191.104</td><td style=\"text-align: right;\">0.980986</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         187.115</td><td style=\"text-align: right;\">1.24385 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:  21%|██▏       | 6/28 [00:02<00:09,  2.40it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  25%|██▌       | 7/28 [00:03<00:08,  2.41it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  29%|██▊       | 8/28 [00:03<00:08,  2.41it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  32%|███▏      | 9/28 [00:04<00:07,  2.41it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:   0%|          | 0/28 [00:00<00:00, 2071.26it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  36%|███▌      | 10/28 [00:04<00:07,  2.41it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  39%|███▉      | 11/28 [00:04<00:07,  2.41it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:   4%|▎         | 1/28 [00:00<00:11,  2.32it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 14:  43%|████▎     | 12/28 [00:05<00:06,  2.41it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:   7%|▋         | 2/28 [00:01<00:10,  2.37it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  46%|████▋     | 13/28 [00:05<00:06,  2.41it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  11%|█         | 3/28 [00:01<00:10,  2.39it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  50%|█████     | 14/28 [00:06<00:05,  2.42it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  14%|█▍        | 4/28 [00:02<00:09,  2.41it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  54%|█████▎    | 15/28 [00:06<00:05,  2.42it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  18%|█▊        | 5/28 [00:02<00:09,  2.41it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  57%|█████▋    | 16/28 [00:07<00:04,  2.42it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  21%|██▏       | 6/28 [00:02<00:09,  2.42it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  61%|██████    | 17/28 [00:07<00:04,  2.42it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  25%|██▌       | 7/28 [00:03<00:08,  2.43it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:16:52 (running for 00:03:20.10)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         191.104</td><td style=\"text-align: right;\">0.980986</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         187.115</td><td style=\"text-align: right;\">1.24385 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:  64%|██████▍   | 18/28 [00:07<00:04,  2.42it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  29%|██▊       | 8/28 [00:03<00:08,  2.43it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  68%|██████▊   | 19/28 [00:08<00:03,  2.42it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  32%|███▏      | 9/28 [00:04<00:07,  2.43it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  71%|███████▏  | 20/28 [00:08<00:03,  2.42it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  36%|███▌      | 10/28 [00:04<00:07,  2.44it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  75%|███████▌  | 21/28 [00:09<00:02,  2.42it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  39%|███▉      | 11/28 [00:04<00:06,  2.44it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  43%|████▎     | 12/28 [00:05<00:06,  2.44it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  79%|███████▊  | 22/28 [00:09<00:02,  2.39it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A3)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:  46%|████▋     | 13/28 [00:05<00:06,  2.44it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 14:  86%|████████▌ | 24/28 [00:10<00:01,  2.47it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.40it/s]\u001b[A\n",
      "Epoch 14:  50%|█████     | 14/28 [00:06<00:05,  2.44it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 14:  93%|█████████▎| 26/28 [00:10<00:00,  2.59it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.23it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 14: 100%|██████████| 28/28 [00:10<00:00,  2.71it/s, loss=1.210, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.240, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  54%|█████▎    | 15/28 [00:06<00:05,  2.44it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00001:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-16-55\n",
      "  done: false\n",
      "  experiment_id: 220fad4fc77f49c497bbc5974825553d\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 15\n",
      "  loss: 1.0486350059509277\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684473\n",
      "  time_since_restore: 200.1896493434906\n",
      "  time_this_iter_s: 13.074687719345093\n",
      "  time_total_s: 200.1896493434906\n",
      "  timestamp: 1637525815\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 15\n",
      "  trial_id: 7fbfb_00001\n",
      "  \n",
      "Epoch 14: 100%|██████████| 28/28 [00:10<00:00,  2.66it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:  57%|█████▋    | 16/28 [00:06<00:04,  2.44it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  61%|██████    | 17/28 [00:07<00:04,  2.44it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  64%|██████▍   | 18/28 [00:07<00:04,  2.44it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  68%|██████▊   | 19/28 [00:08<00:03,  2.45it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  71%|███████▏  | 20/28 [00:08<00:03,  2.45it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:16:57 (running for 00:03:25.21)<br>Memory usage on this node: 28.9/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         191.104</td><td style=\"text-align: right;\">0.980986</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         200.19 </td><td style=\"text-align: right;\">1.04864 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:   0%|          | 0/28 [00:00<00:00, 2668.13it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  75%|███████▌  | 21/28 [00:08<00:02,  2.45it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 14:  79%|███████▊  | 22/28 [00:09<00:02,  2.42it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A4)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:   4%|▎         | 1/28 [00:00<00:10,  2.47it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 15:   7%|▋         | 2/28 [00:01<00:10,  2.46it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 14:  86%|████████▌ | 24/28 [00:10<00:01,  2.48it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.12it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 14:  93%|█████████▎| 26/28 [00:10<00:00,  2.60it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  11%|█         | 3/28 [00:01<00:10,  2.45it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.04it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 14: 100%|██████████| 28/28 [00:10<00:00,  2.72it/s, loss=0.970, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.981, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00000:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-16-59\n",
      "  done: false\n",
      "  experiment_id: 313cd16ec062412fad43e9ffaa517873\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 15\n",
      "  loss: 0.9791774749755859\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684464\n",
      "  time_since_restore: 204.3592791557312\n",
      "  time_this_iter_s: 13.255755186080933\n",
      "  time_total_s: 204.3592791557312\n",
      "  timestamp: 1637525819\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 15\n",
      "  trial_id: 7fbfb_00000\n",
      "  \n",
      "Epoch 14: 100%|██████████| 28/28 [00:10<00:00,  2.67it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:  14%|█▍        | 4/28 [00:02<00:09,  2.45it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  18%|█▊        | 5/28 [00:02<00:09,  2.45it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  21%|██▏       | 6/28 [00:02<00:08,  2.45it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  25%|██▌       | 7/28 [00:03<00:08,  2.45it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  29%|██▊       | 8/28 [00:03<00:08,  2.45it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  32%|███▏      | 9/28 [00:04<00:07,  2.45it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:   0%|          | 0/28 [00:00<00:00, 2007.80it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  36%|███▌      | 10/28 [00:04<00:07,  2.45it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  39%|███▉      | 11/28 [00:04<00:06,  2.45it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:17:02 (running for 00:03:30.34)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         204.359</td><td style=\"text-align: right;\">0.979177</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         200.19 </td><td style=\"text-align: right;\">1.04864 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:   4%|▎         | 1/28 [00:00<00:11,  2.29it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 15:  43%|████▎     | 12/28 [00:05<00:06,  2.44it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  46%|████▋     | 13/28 [00:05<00:06,  2.44it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:   7%|▋         | 2/28 [00:01<00:11,  2.35it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  11%|█         | 3/28 [00:01<00:10,  2.38it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  50%|█████     | 14/28 [00:06<00:05,  2.45it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  54%|█████▎    | 15/28 [00:06<00:05,  2.44it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  14%|█▍        | 4/28 [00:02<00:10,  2.39it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  18%|█▊        | 5/28 [00:02<00:09,  2.40it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  57%|█████▋    | 16/28 [00:06<00:04,  2.44it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  21%|██▏       | 6/28 [00:02<00:09,  2.41it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  61%|██████    | 17/28 [00:07<00:04,  2.44it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  25%|██▌       | 7/28 [00:03<00:08,  2.42it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  64%|██████▍   | 18/28 [00:07<00:04,  2.44it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  29%|██▊       | 8/28 [00:03<00:08,  2.42it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  68%|██████▊   | 19/28 [00:08<00:03,  2.44it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  32%|███▏      | 9/28 [00:04<00:07,  2.42it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  71%|███████▏  | 20/28 [00:08<00:03,  2.44it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  36%|███▌      | 10/28 [00:04<00:07,  2.43it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  75%|███████▌  | 21/28 [00:09<00:02,  2.44it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  39%|███▉      | 11/28 [00:04<00:07,  2.43it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  79%|███████▊  | 22/28 [00:09<00:02,  2.42it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A3)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:  43%|████▎     | 12/28 [00:05<00:06,  2.43it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:17:07 (running for 00:03:35.36)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         204.359</td><td style=\"text-align: right;\">0.979177</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         200.19 </td><td style=\"text-align: right;\">1.04864 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 15:  86%|████████▌ | 24/28 [00:10<00:01,  2.49it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.30it/s]\u001b[A\n",
      "Epoch 15:  46%|████▋     | 13/28 [00:05<00:06,  2.44it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 15:  93%|█████████▎| 26/28 [00:10<00:00,  2.61it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.13it/s]\u001b[A\n",
      "Epoch 15:  50%|█████     | 14/28 [00:06<00:05,  2.44it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 15: 100%|██████████| 28/28 [00:10<00:00,  2.73it/s, loss=1.030, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.050, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00001:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-17-08\n",
      "  done: false\n",
      "  experiment_id: 220fad4fc77f49c497bbc5974825553d\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 16\n",
      "  loss: 1.8140772581100464\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684473\n",
      "  time_since_restore: 213.17510271072388\n",
      "  time_this_iter_s: 12.985453367233276\n",
      "  time_total_s: 213.17510271072388\n",
      "  timestamp: 1637525828\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 16\n",
      "  trial_id: 7fbfb_00001\n",
      "  \n",
      "Epoch 15: 100%|██████████| 28/28 [00:10<00:00,  2.68it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n",
      "Epoch 15:  54%|█████▎    | 15/28 [00:06<00:05,  2.44it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:  57%|█████▋    | 16/28 [00:06<00:04,  2.44it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  61%|██████    | 17/28 [00:07<00:04,  2.44it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  64%|██████▍   | 18/28 [00:07<00:04,  2.44it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  68%|██████▊   | 19/28 [00:08<00:03,  2.44it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:   0%|          | 0/28 [00:00<00:00, 1863.31it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  71%|███████▏  | 20/28 [00:08<00:03,  2.44it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 15:  75%|███████▌  | 21/28 [00:08<00:02,  2.45it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:   4%|▎         | 1/28 [00:00<00:11,  2.41it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 15:  79%|███████▊  | 22/28 [00:09<00:02,  2.42it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A4)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16:   7%|▋         | 2/28 [00:01<00:10,  2.43it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 15:  86%|████████▌ | 24/28 [00:10<00:01,  2.47it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  11%|█         | 3/28 [00:01<00:10,  2.43it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  2.94it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 15:  93%|█████████▎| 26/28 [00:10<00:00,  2.59it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:01<00:00,  4.83it/s]\u001b[A\n",
      "Epoch 16:  14%|█▍        | 4/28 [00:02<00:09,  2.43it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 15: 100%|██████████| 28/28 [00:10<00:00,  2.71it/s, loss=0.968, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.979, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00000:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-17-12\n",
      "  done: false\n",
      "  experiment_id: 313cd16ec062412fad43e9ffaa517873\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 16\n",
      "  loss: 0.9934203624725342\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684464\n",
      "  time_since_restore: 217.63608765602112\n",
      "  time_this_iter_s: 13.276808500289917\n",
      "  time_total_s: 217.63608765602112\n",
      "  timestamp: 1637525832\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 16\n",
      "  trial_id: 7fbfb_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:17:12 (running for 00:03:40.62)<br>Memory usage on this node: 28.2/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         217.636</td><td style=\"text-align: right;\">0.99342</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         213.175</td><td style=\"text-align: right;\">1.81408</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \r",
      "Epoch 15: 100%|██████████| 28/28 [00:10<00:00,  2.66it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \r",
      "                                                         \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:  18%|█▊        | 5/28 [00:02<00:09,  2.43it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  21%|██▏       | 6/28 [00:02<00:09,  2.43it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  25%|██▌       | 7/28 [00:03<00:08,  2.43it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  29%|██▊       | 8/28 [00:03<00:08,  2.43it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  32%|███▏      | 9/28 [00:04<00:07,  2.43it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  36%|███▌      | 10/28 [00:04<00:07,  2.43it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:   0%|          | 0/28 [00:00<00:00, 2229.83it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  39%|███▉      | 11/28 [00:04<00:06,  2.44it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  43%|████▎     | 12/28 [00:05<00:06,  2.44it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:   4%|▎         | 1/28 [00:00<00:11,  2.29it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 16:  46%|████▋     | 13/28 [00:05<00:06,  2.44it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:   7%|▋         | 2/28 [00:01<00:11,  2.35it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  50%|█████     | 14/28 [00:06<00:05,  2.44it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  11%|█         | 3/28 [00:01<00:10,  2.37it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  54%|█████▎    | 15/28 [00:06<00:05,  2.44it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  14%|█▍        | 4/28 [00:02<00:10,  2.38it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  57%|█████▋    | 16/28 [00:06<00:04,  2.44it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  18%|█▊        | 5/28 [00:02<00:09,  2.39it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:17:17 (running for 00:03:45.63)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         217.636</td><td style=\"text-align: right;\">0.99342</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         213.175</td><td style=\"text-align: right;\">1.81408</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:  61%|██████    | 17/28 [00:07<00:04,  2.44it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  21%|██▏       | 6/28 [00:02<00:09,  2.40it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  64%|██████▍   | 18/28 [00:07<00:04,  2.44it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  25%|██▌       | 7/28 [00:03<00:08,  2.41it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  68%|██████▊   | 19/28 [00:08<00:03,  2.44it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  29%|██▊       | 8/28 [00:03<00:08,  2.41it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  71%|███████▏  | 20/28 [00:08<00:03,  2.44it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  32%|███▏      | 9/28 [00:04<00:07,  2.41it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  75%|███████▌  | 21/28 [00:09<00:02,  2.44it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  36%|███▌      | 10/28 [00:04<00:07,  2.42it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  79%|███████▊  | 22/28 [00:09<00:02,  2.42it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A3)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16:  39%|███▉      | 11/28 [00:04<00:07,  2.42it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 16:  86%|████████▌ | 24/28 [00:10<00:01,  2.49it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  43%|████▎     | 12/28 [00:05<00:06,  2.42it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.39it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 16:  93%|█████████▎| 26/28 [00:10<00:00,  2.62it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.22it/s]\u001b[A\n",
      "Epoch 16:  46%|████▋     | 13/28 [00:05<00:06,  2.42it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 16: 100%|██████████| 28/28 [00:10<00:00,  2.74it/s, loss=1.770, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=1.810, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00001:\n",
      "  acc: 0.17428861558437347\n",
      "  date: 2021-11-21_15-17-21\n",
      "  done: false\n",
      "  experiment_id: 220fad4fc77f49c497bbc5974825553d\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 17\n",
      "  loss: 1.7447353601455688\n",
      "  macro_f1: 0.09812276810407639\n",
      "  micro_f1: 0.17428861558437347\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684473\n",
      "  time_since_restore: 226.11617994308472\n",
      "  time_this_iter_s: 12.94107723236084\n",
      "  time_total_s: 226.11617994308472\n",
      "  timestamp: 1637525841\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 17\n",
      "  trial_id: 7fbfb_00001\n",
      "  \n",
      "Epoch 16: 100%|██████████| 28/28 [00:10<00:00,  2.69it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "                                                         \u001b[A\n",
      "Epoch 16:  50%|█████     | 14/28 [00:06<00:05,  2.42it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:  54%|█████▎    | 15/28 [00:06<00:05,  2.42it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  57%|█████▋    | 16/28 [00:07<00:04,  2.43it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  61%|██████    | 17/28 [00:07<00:04,  2.43it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 16:  64%|██████▍   | 18/28 [00:07<00:04,  2.43it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:17:23 (running for 00:03:51.13)<br>Memory usage on this node: 29.1/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         217.636</td><td style=\"text-align: right;\">0.99342</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\"> 0.252275 </td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         226.116</td><td style=\"text-align: right;\">1.74474</td><td style=\"text-align: right;\">0.174289</td><td style=\"text-align: right;\"> 0.0981228</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:  68%|██████▊   | 19/28 [00:08<00:03,  2.43it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 17:   0%|          | 0/28 [00:00<00:00, 1425.18it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 16:  71%|███████▏  | 20/28 [00:08<00:03,  2.43it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 17:   4%|▎         | 1/28 [00:00<00:11,  2.39it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]  \n",
      "Epoch 16:  75%|███████▌  | 21/28 [00:09<00:02,  2.43it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 17:   7%|▋         | 2/28 [00:01<00:10,  2.41it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 16:  79%|███████▊  | 22/28 [00:09<00:02,  2.41it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A4)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17:  11%|█         | 3/28 [00:01<00:10,  2.39it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 16:  86%|████████▌ | 24/28 [00:10<00:01,  2.47it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.25it/s]\u001b[A\n",
      "Epoch 17:  14%|█▍        | 4/28 [00:02<00:10,  2.39it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 16:  93%|█████████▎| 26/28 [00:10<00:00,  2.60it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.09it/s]\u001b[A\n",
      "Epoch 17:  18%|█▊        | 5/28 [00:02<00:09,  2.40it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 16: 100%|██████████| 28/28 [00:10<00:00,  2.72it/s, loss=0.983, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.993, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00000:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-17-26\n",
      "  done: false\n",
      "  experiment_id: 313cd16ec062412fad43e9ffaa517873\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 17\n",
      "  loss: 0.9821619987487793\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684464\n",
      "  time_since_restore: 230.91986751556396\n",
      "  time_this_iter_s: 13.283779859542847\n",
      "  time_total_s: 230.91986751556396\n",
      "  timestamp: 1637525846\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 17\n",
      "  trial_id: 7fbfb_00000\n",
      "  \n",
      "Epoch 16: 100%|██████████| 28/28 [00:10<00:00,  2.67it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:  21%|██▏       | 6/28 [00:02<00:09,  2.40it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 17:  25%|██▌       | 7/28 [00:03<00:08,  2.41it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 17:  29%|██▊       | 8/28 [00:03<00:08,  2.41it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 17:  32%|███▏      | 9/28 [00:04<00:07,  2.41it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 17:  36%|███▌      | 10/28 [00:04<00:07,  2.42it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 17:  39%|███▉      | 11/28 [00:04<00:07,  2.42it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 17:   0%|          | 0/28 [00:00<00:00, 2629.66it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 17:  43%|████▎     | 12/28 [00:05<00:06,  2.42it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:17:29 (running for 00:03:56.91)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         230.92 </td><td style=\"text-align: right;\">0.982162</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\"> 0.252275 </td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         226.116</td><td style=\"text-align: right;\">1.74474 </td><td style=\"text-align: right;\">0.174289</td><td style=\"text-align: right;\"> 0.0981228</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:  46%|████▋     | 13/28 [00:05<00:06,  2.42it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 17:   4%|▎         | 1/28 [00:00<00:11,  2.32it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 17:  50%|█████     | 14/28 [00:06<00:05,  2.42it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 17:   7%|▋         | 2/28 [00:01<00:10,  2.36it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 17:  54%|█████▎    | 15/28 [00:06<00:05,  2.42it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 17:  11%|█         | 3/28 [00:01<00:10,  2.38it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 17:  57%|█████▋    | 16/28 [00:07<00:04,  2.42it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 17:  14%|█▍        | 4/28 [00:02<00:10,  2.39it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 17:  61%|██████    | 17/28 [00:07<00:04,  2.42it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 17:  18%|█▊        | 5/28 [00:02<00:09,  2.40it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 17:  64%|██████▍   | 18/28 [00:07<00:04,  2.43it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 17:  21%|██▏       | 6/28 [00:02<00:09,  2.41it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 17:  68%|██████▊   | 19/28 [00:08<00:03,  2.43it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 17:  25%|██▌       | 7/28 [00:03<00:08,  2.41it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 17:  71%|███████▏  | 20/28 [00:08<00:03,  2.43it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 17:  29%|██▊       | 8/28 [00:03<00:08,  2.41it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 17:  75%|███████▌  | 21/28 [00:09<00:02,  2.43it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 17:  32%|███▏      | 9/28 [00:04<00:07,  2.42it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 17:  79%|███████▊  | 22/28 [00:09<00:02,  2.41it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A3)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17:  36%|███▌      | 10/28 [00:04<00:07,  2.42it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 17:  86%|████████▌ | 24/28 [00:10<00:01,  2.48it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 17:  39%|███▉      | 11/28 [00:04<00:07,  2.42it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.38it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 17:  93%|█████████▎| 26/28 [00:10<00:00,  2.61it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 17:  43%|████▎     | 12/28 [00:05<00:06,  2.42it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.23it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:17:34 (running for 00:04:01.92)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         230.92 </td><td style=\"text-align: right;\">0.982162</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\"> 0.252275 </td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         226.116</td><td style=\"text-align: right;\">1.74474 </td><td style=\"text-align: right;\">0.174289</td><td style=\"text-align: right;\"> 0.0981228</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 17: 100%|██████████| 28/28 [00:10<00:00,  2.73it/s, loss=1.730, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.740, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Result for execute_7fbfb_00001:\n",
      "  acc: 0.17428861558437347\n",
      "  date: 2021-11-21_15-17-34\n",
      "  done: false\n",
      "  experiment_id: 220fad4fc77f49c497bbc5974825553d\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 18\n",
      "  loss: 1.757312536239624\n",
      "  macro_f1: 0.09812276810407639\n",
      "  micro_f1: 0.17428861558437347\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684473\n",
      "  time_since_restore: 239.0854847431183\n",
      "  time_this_iter_s: 12.96930480003357\n",
      "  time_total_s: 239.0854847431183\n",
      "  timestamp: 1637525854\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 18\n",
      "  trial_id: 7fbfb_00001\n",
      "  \n",
      "Epoch 17: 100%|██████████| 28/28 [00:10<00:00,  2.68it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "                                                         \u001b[A\n",
      "Epoch 17:  46%|████▋     | 13/28 [00:05<00:06,  2.42it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:  50%|█████     | 14/28 [00:06<00:05,  2.43it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 17:  54%|█████▎    | 15/28 [00:06<00:05,  2.43it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 17:  57%|█████▋    | 16/28 [00:07<00:04,  2.43it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 17:  61%|██████    | 17/28 [00:07<00:04,  2.43it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 17:  64%|██████▍   | 18/28 [00:07<00:04,  2.43it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 18:   0%|          | 0/28 [00:00<00:00, 2702.52it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 17:  68%|██████▊   | 19/28 [00:08<00:03,  2.43it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 17:  71%|███████▏  | 20/28 [00:08<00:03,  2.43it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 18:   4%|▎         | 1/28 [00:00<00:10,  2.46it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]  \n",
      "Epoch 17:  75%|███████▌  | 21/28 [00:09<00:02,  2.44it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 18:   7%|▋         | 2/28 [00:01<00:10,  2.46it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 17:  79%|███████▊  | 22/28 [00:09<00:02,  2.41it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A4)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18:  11%|█         | 3/28 [00:01<00:10,  2.46it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 18:  14%|█▍        | 4/28 [00:02<00:09,  2.46it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 17:  86%|████████▌ | 24/28 [00:10<00:01,  2.48it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.26it/s]\u001b[A\n",
      "Epoch 18:  18%|█▊        | 5/28 [00:02<00:09,  2.45it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 17:  93%|█████████▎| 26/28 [00:10<00:00,  2.61it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.16it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 17: 100%|██████████| 28/28 [00:10<00:00,  2.73it/s, loss=0.971, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.982, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:17:39 (running for 00:04:07.10)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         230.92 </td><td style=\"text-align: right;\">0.982162</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\"> 0.252275 </td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         239.085</td><td style=\"text-align: right;\">1.75731 </td><td style=\"text-align: right;\">0.174289</td><td style=\"text-align: right;\"> 0.0981228</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:  21%|██▏       | 6/28 [00:02<00:08,  2.45it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Result for execute_7fbfb_00000:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-17-39\n",
      "  done: false\n",
      "  experiment_id: 313cd16ec062412fad43e9ffaa517873\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 18\n",
      "  loss: 0.9843190908432007\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684464\n",
      "  time_since_restore: 244.17909479141235\n",
      "  time_this_iter_s: 13.259227275848389\n",
      "  time_total_s: 244.17909479141235\n",
      "  timestamp: 1637525859\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 18\n",
      "  trial_id: 7fbfb_00000\n",
      "  \n",
      "Epoch 17: 100%|██████████| 28/28 [00:10<00:00,  2.67it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:  25%|██▌       | 7/28 [00:03<00:08,  2.45it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 18:  29%|██▊       | 8/28 [00:03<00:08,  2.45it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 18:  32%|███▏      | 9/28 [00:04<00:07,  2.45it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 18:  36%|███▌      | 10/28 [00:04<00:07,  2.45it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 18:  39%|███▉      | 11/28 [00:04<00:06,  2.45it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 18:  43%|████▎     | 12/28 [00:05<00:06,  2.45it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 18:   0%|          | 0/28 [00:00<00:00, 2589.08it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 18:  46%|████▋     | 13/28 [00:05<00:06,  2.45it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 18:  50%|█████     | 14/28 [00:06<00:05,  2.45it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 18:   4%|▎         | 1/28 [00:00<00:11,  2.39it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 18:   7%|▋         | 2/28 [00:01<00:10,  2.41it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 18:  54%|█████▎    | 15/28 [00:06<00:05,  2.45it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 18:  57%|█████▋    | 16/28 [00:06<00:04,  2.45it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 18:  11%|█         | 3/28 [00:01<00:10,  2.42it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 18:  61%|██████    | 17/28 [00:07<00:04,  2.45it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 18:  14%|█▍        | 4/28 [00:02<00:09,  2.42it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 18:  64%|██████▍   | 18/28 [00:07<00:04,  2.45it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 18:  18%|█▊        | 5/28 [00:02<00:09,  2.43it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:17:44 (running for 00:04:12.16)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         244.179</td><td style=\"text-align: right;\">0.984319</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\"> 0.252275 </td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         239.085</td><td style=\"text-align: right;\">1.75731 </td><td style=\"text-align: right;\">0.174289</td><td style=\"text-align: right;\"> 0.0981228</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:  68%|██████▊   | 19/28 [00:08<00:03,  2.45it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 18:  21%|██▏       | 6/28 [00:02<00:09,  2.44it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 18:  71%|███████▏  | 20/28 [00:08<00:03,  2.45it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 18:  25%|██▌       | 7/28 [00:03<00:08,  2.44it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 18:  75%|███████▌  | 21/28 [00:08<00:02,  2.45it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 18:  29%|██▊       | 8/28 [00:03<00:08,  2.44it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 18:  79%|███████▊  | 22/28 [00:09<00:02,  2.43it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 18:  32%|███▏      | 9/28 [00:04<00:07,  2.44it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A3)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18:  36%|███▌      | 10/28 [00:04<00:07,  2.45it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 18:  86%|████████▌ | 24/28 [00:10<00:01,  2.49it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.28it/s]\u001b[A\n",
      "Epoch 18:  39%|███▉      | 11/28 [00:04<00:06,  2.45it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 18:  93%|█████████▎| 26/28 [00:10<00:00,  2.62it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.14it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 18: 100%|██████████| 28/28 [00:10<00:00,  2.74it/s, loss=1.770, v_num=0, acc=0.170, macro_f1=0.0962, micro_f1=0.170, val_loss=1.760, val_acc=0.174, val_macro_f1=0.0981, val_micro_f1=0.174]\n",
      "Epoch 18:  43%|████▎     | 12/28 [00:05<00:06,  2.45it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00001:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-17-47\n",
      "  done: false\n",
      "  experiment_id: 220fad4fc77f49c497bbc5974825553d\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 19\n",
      "  loss: 0.9655664563179016\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684473\n",
      "  time_since_restore: 252.03130769729614\n",
      "  time_this_iter_s: 12.945822954177856\n",
      "  time_total_s: 252.03130769729614\n",
      "  timestamp: 1637525867\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 19\n",
      "  trial_id: 7fbfb_00001\n",
      "  \n",
      "Epoch 18: 100%|██████████| 28/28 [00:10<00:00,  2.68it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "                                                         \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:  46%|████▋     | 13/28 [00:05<00:06,  2.45it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 18:  50%|█████     | 14/28 [00:06<00:05,  2.45it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 18:  54%|█████▎    | 15/28 [00:06<00:05,  2.45it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 18:  57%|█████▋    | 16/28 [00:06<00:04,  2.45it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 18:  61%|██████    | 17/28 [00:07<00:04,  2.45it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:   0%|          | 0/28 [00:00<00:00, 1288.97it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 18:  64%|██████▍   | 18/28 [00:07<00:04,  2.45it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 18:  68%|██████▊   | 19/28 [00:08<00:03,  2.45it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:17:50 (running for 00:04:18.05)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         244.179</td><td style=\"text-align: right;\">0.984319</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         252.031</td><td style=\"text-align: right;\">0.965566</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19:   4%|▎         | 1/28 [00:00<00:11,  2.34it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 18:  71%|███████▏  | 20/28 [00:08<00:03,  2.45it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:   7%|▋         | 2/28 [00:01<00:10,  2.38it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 18:  75%|███████▌  | 21/28 [00:08<00:02,  2.46it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  11%|█         | 3/28 [00:01<00:10,  2.39it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 18:  79%|███████▊  | 22/28 [00:09<00:02,  2.43it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A4)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19:  14%|█▍        | 4/28 [00:02<00:09,  2.40it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 18:  86%|████████▌ | 24/28 [00:10<00:01,  2.49it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  18%|█▊        | 5/28 [00:02<00:09,  2.41it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.14it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 18:  93%|█████████▎| 26/28 [00:10<00:00,  2.62it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.07it/s]\u001b[A\n",
      "Epoch 19:  21%|██▏       | 6/28 [00:02<00:09,  2.42it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 18: 100%|██████████| 28/28 [00:10<00:00,  2.74it/s, loss=0.973, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.984, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00000:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-17-52\n",
      "  done: false\n",
      "  experiment_id: 313cd16ec062412fad43e9ffaa517873\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 19\n",
      "  loss: 0.9848313331604004\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684464\n",
      "  time_since_restore: 257.3464345932007\n",
      "  time_this_iter_s: 13.16733980178833\n",
      "  time_total_s: 257.3464345932007\n",
      "  timestamp: 1637525872\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 19\n",
      "  trial_id: 7fbfb_00000\n",
      "  \n",
      "Epoch 18: 100%|██████████| 28/28 [00:10<00:00,  2.68it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "                                                         \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19:  25%|██▌       | 7/28 [00:03<00:08,  2.42it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  29%|██▊       | 8/28 [00:03<00:08,  2.42it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  32%|███▏      | 9/28 [00:04<00:07,  2.42it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  36%|███▌      | 10/28 [00:04<00:07,  2.43it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  39%|███▉      | 11/28 [00:04<00:07,  2.43it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  43%|████▎     | 12/28 [00:05<00:06,  2.43it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:   0%|          | 0/28 [00:00<00:00, 2528.21it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  46%|████▋     | 13/28 [00:05<00:06,  2.43it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:17:55 (running for 00:04:23.33)<br>Memory usage on this node: 28.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 2.0/255 CPUs, 2.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         257.346</td><td style=\"text-align: right;\">0.984831</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>RUNNING </td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         252.031</td><td style=\"text-align: right;\">0.965566</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19:  50%|█████     | 14/28 [00:06<00:05,  2.43it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:   4%|▎         | 1/28 [00:00<00:11,  2.26it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]  \n",
      "Epoch 19:  54%|█████▎    | 15/28 [00:06<00:05,  2.43it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:   7%|▋         | 2/28 [00:01<00:11,  2.32it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  57%|█████▋    | 16/28 [00:06<00:04,  2.43it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  11%|█         | 3/28 [00:01<00:10,  2.34it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  61%|██████    | 17/28 [00:07<00:04,  2.43it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  14%|█▍        | 4/28 [00:02<00:10,  2.36it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  64%|██████▍   | 18/28 [00:07<00:04,  2.43it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  18%|█▊        | 5/28 [00:02<00:09,  2.38it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  68%|██████▊   | 19/28 [00:08<00:03,  2.43it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  21%|██▏       | 6/28 [00:02<00:09,  2.39it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  71%|███████▏  | 20/28 [00:08<00:03,  2.43it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  25%|██▌       | 7/28 [00:03<00:08,  2.40it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  75%|███████▌  | 21/28 [00:09<00:02,  2.44it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  29%|██▊       | 8/28 [00:03<00:08,  2.40it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  79%|███████▊  | 22/28 [00:09<00:02,  2.42it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A3)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19:  32%|███▏      | 9/28 [00:04<00:07,  2.40it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  36%|███▌      | 10/28 [00:04<00:07,  2.41it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 19:  86%|████████▌ | 24/28 [00:10<00:01,  2.49it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.40it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 19:  93%|█████████▎| 26/28 [00:10<00:00,  2.62it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.20it/s]\u001b[A\n",
      "Epoch 19:  39%|███▉      | 11/28 [00:04<00:07,  2.41it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684473)\u001b[0m \n",
      "Epoch 19: 100%|██████████| 28/28 [00:10<00:00,  2.73it/s, loss=0.957, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.966, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00001:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-18-00\n",
      "  done: true\n",
      "  experiment_id: 220fad4fc77f49c497bbc5974825553d\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 20\n",
      "  loss: 1.066162347793579\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684473\n",
      "  time_since_restore: 265.0563807487488\n",
      "  time_this_iter_s: 13.025073051452637\n",
      "  time_total_s: 265.0563807487488\n",
      "  timestamp: 1637525880\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 20\n",
      "  trial_id: 7fbfb_00001\n",
      "  \n",
      "Epoch 19:  43%|████▎     | 12/28 [00:05<00:06,  2.41it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684473)\u001b[0m   rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19:  46%|████▋     | 13/28 [00:05<00:06,  2.41it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  50%|█████     | 14/28 [00:06<00:05,  2.42it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:18:01 (running for 00:04:29.07)<br>Memory usage on this node: 24.4/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 1.0/255 CPUs, 1.0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (1 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>RUNNING   </td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         257.346</td><td style=\"text-align: right;\">0.984831</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>TERMINATED</td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         265.056</td><td style=\"text-align: right;\">1.06616 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19:  54%|█████▎    | 15/28 [00:06<00:05,  2.42it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  57%|█████▋    | 16/28 [00:07<00:04,  2.42it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  61%|██████    | 17/28 [00:07<00:04,  2.42it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  64%|██████▍   | 18/28 [00:07<00:04,  2.42it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  68%|██████▊   | 19/28 [00:08<00:03,  2.42it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  71%|███████▏  | 20/28 [00:08<00:03,  2.43it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  75%|███████▌  | 21/28 [00:09<00:02,  2.43it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Epoch 19:  79%|███████▊  | 22/28 [00:09<00:02,  2.40it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A4)\u001b[0m \n",
      "Validating:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 19:  86%|████████▌ | 24/28 [00:10<00:01,  2.46it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  33%|███▎      | 2/6 [00:00<00:01,  3.13it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 19:  93%|█████████▎| 26/28 [00:10<00:00,  2.58it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Validating:  67%|██████▋   | 4/6 [00:00<00:00,  5.05it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=2684464)\u001b[0m \n",
      "Epoch 19: 100%|██████████| 28/28 [00:10<00:00,  2.70it/s, loss=0.974, v_num=0, acc=0.621, macro_f1=0.255, micro_f1=0.621, val_loss=0.985, val_acc=0.610, val_macro_f1=0.252, val_micro_f1=0.610]\n",
      "Result for execute_7fbfb_00000:\n",
      "  acc: 0.6104865074157715\n",
      "  date: 2021-11-21_15-18-05\n",
      "  done: true\n",
      "  experiment_id: 313cd16ec062412fad43e9ffaa517873\n",
      "  hostname: Zeus\n",
      "  iterations_since_restore: 20\n",
      "  loss: 0.9848718047142029\n",
      "  macro_f1: 0.2522749900817871\n",
      "  micro_f1: 0.6104865074157715\n",
      "  node_ip: 141.117.3.96\n",
      "  pid: 2684464\n",
      "  time_since_restore: 270.63092517852783\n",
      "  time_this_iter_s: 13.284490585327148\n",
      "  time_total_s: 270.63092517852783\n",
      "  timestamp: 1637525885\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 20\n",
      "  trial_id: 7fbfb_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-21 15:18:05 (running for 00:04:33.63)<br>Memory usage on this node: 24.3/251.7 GiB<br>Using MedianStoppingRule: num_stopped=0.<br>Resources requested: 0/255 CPUs, 0/2 GPUs, 0.0/160.61 GiB heap, 0.0/72.83 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hoang/github/BERT_ABSA/experiment/laptop/execute_2021-11-21_15-13-32<br>Number of trials: 2/2 (2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  model_params/hidden_size</th><th style=\"text-align: right;\">  model_params/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  macro_f1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>execute_7fbfb_00000</td><td>TERMINATED</td><td>141.117.3.96:2684464</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.00376421</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         270.631</td><td style=\"text-align: right;\">0.984872</td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "<tr><td>execute_7fbfb_00001</td><td>TERMINATED</td><td>141.117.3.96:2684473</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">       0.024002  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         265.056</td><td style=\"text-align: right;\">1.06616 </td><td style=\"text-align: right;\">0.610487</td><td style=\"text-align: right;\">  0.252275</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m /home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001b[2m\u001b[36m(pid=2684464)\u001b[0m   rank_zero_deprecation(\n",
      "2021-11-21 15:18:05,920\tINFO tune.py:630 -- Total run time: 273.75 seconds (273.62 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"data_params\": {\n",
    "        \"data_train_dir\": \"/home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Restaurants_Train.csv\",\n",
    "        \"data_test_dir\": \"/home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Restaurants_Test.csv\",\n",
    "        \"transformation\": tune.choice(['QA_M', 'QA_B', 'MLI_M', 'MLI_B']),\n",
    "        \"num_classes\": 3,\n",
    "        \"batch_size\": 128,\n",
    "        \"bert_name\": \"bert-base-uncased\",\n",
    "        \"max_length\": 128,\n",
    "        \"seed\": 12345\n",
    "    },\n",
    "    \n",
    "    \"model_params\": {\n",
    "        \"pretrained_bert_name\": \"bert-base-uncased\",\n",
    "        \"hidden_size\": tune.choice([128, 256]),\n",
    "        \"hidden_dropout_prob\": 0.1,\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    },\n",
    "    \"trainer_params\": {\n",
    "        \"checkpoint_dir\": \"../model/restaurants\",\n",
    "        \"top_k\": 3,\n",
    "        \"max_epochs\": 100,\n",
    "        \"metric\": \"val_loss\",\n",
    "        \"patience\": 10,\n",
    "        \"mode\": \"min\",\n",
    "        \"tune\": True,\n",
    "        \"train\": False,\n",
    "        \"test\": False,\n",
    "    }\n",
    "}\n",
    "\n",
    "experiment_name = 'laptop'\n",
    "experiment_dir = str(PROJ_PATH / 'experiment' / experiment_name )\n",
    "\n",
    "if not os.path.exists(experiment_dir): os.mkdir(experiment_dir)\n",
    "    \n",
    "analysis = tune.run(\n",
    "    tune.with_parameters(execute, phase='tune'),\n",
    "    config=config,\n",
    "    resources_per_trial={'gpu': 1},\n",
    "    local_dir=experiment_dir,\n",
    "    stop={'training_iteration': 20},\n",
    "    scheduler=MedianStoppingRule(metric='acc'),\n",
    "    num_samples=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75907fa9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# tensorboard --logdir=./execute_2021-11-21_14-34-48 --port 1234"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
