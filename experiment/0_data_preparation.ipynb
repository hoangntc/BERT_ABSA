{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ea9b30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJ_PATH=/home/hoang/github/BERT_ABSA\n"
     ]
    }
   ],
   "source": [
    "import os, sys, re, datetime, random, gzip, json\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from itertools import accumulate\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "\n",
    "from time import time\n",
    "from math import ceil\n",
    "from multiprocessing import Pool\n",
    "from sentence_transformers import SentenceTransformer, models, losses, InputExample\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.trainer.trainer import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "\n",
    "PROJ_PATH = Path(os.path.join(re.sub(\"/BERT_ABSA.*$\", '', os.getcwd()), 'BERT_ABSA'))\n",
    "print(f'PROJ_PATH={PROJ_PATH}')\n",
    "sys.path.insert(1, str(PROJ_PATH))\n",
    "sys.path.insert(1, str(PROJ_PATH/'src'))\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21cf9c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../model/restaurants/epoch=4-val_loss=0.6392-val_acc=0.7826-val_macro_f1=0.6944-val_micro_f1=0.7826.ckpt',\n",
       " '../model/restaurants/epoch=2-val_loss=0.6676-val_acc=0.7673-val_macro_f1=0.6605-val_micro_f1=0.7673.ckpt',\n",
       " '../model/restaurants/epoch=8-val_loss=0.6323-val_acc=0.8034-val_macro_f1=0.7262-val_micro_f1=0.8034.ckpt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob('../model/restaurants/*.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80d8522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3851e49",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "763739db",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caca32d9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def parseXML(data_path):\n",
    "    tree = ET.ElementTree(file=data_path)\n",
    "    objs = list()\n",
    "    for sentence in tree.getroot():\n",
    "        obj = dict()\n",
    "        obj['id'] = sentence.attrib['id']\n",
    "        for item in sentence:\n",
    "            if item.tag == 'text':\n",
    "                obj['text'] = item.text\n",
    "            elif item.tag == 'aspectTerms':\n",
    "                obj['aspects'] = list()\n",
    "                for aspectTerm in item:\n",
    "                    if aspectTerm.attrib['polarity'] != 'conflict':\n",
    "                        obj['aspects'].append(aspectTerm.attrib)\n",
    "            elif item.tag == 'aspectCategories':\n",
    "                obj['category'] = list()\n",
    "                for category in item:\n",
    "                    obj['category'].append(category.attrib)\n",
    "        if 'aspects' in obj and len(obj['aspects']):\n",
    "            objs.append(obj)\n",
    "    return objs\n",
    "\n",
    "def convert_to_dataframe(objs):\n",
    "    output = []\n",
    "    for sentence in objs:\n",
    "        id = sentence['id']\n",
    "        text = sentence['text']\n",
    "        aspects = sentence['aspects']\n",
    "        for aspect in aspects:\n",
    "            term = aspect['term']\n",
    "            label = aspect['polarity']\n",
    "            output.append([id, text, term, label])\n",
    "    output = sorted(output, key=lambda x: x[0])\n",
    "    df = pd.DataFrame(output, columns=['id', 'text', 'term', 'label'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3edac0f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_files = {\n",
    "    'restaurant': {\n",
    "        'train': 'Restaurants_Train.xml',\n",
    "        'test': 'Restaurants_Test.xml',\n",
    "        'trial': 'Restaurants_Trial.xml'\n",
    "    },\n",
    "    'laptop': {\n",
    "        'train': 'Laptops_Train.xml',\n",
    "        'test': 'Laptops_Test.xml',\n",
    "        'trial': 'Laptops_Trial.xml'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4332fd37",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load: /home/hoang/github/BERT_ABSA/dataset/raw_data/Restaurants_Train.xml\n",
      "Save: /home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Restaurants_Train.pkl\n",
      "\n",
      "Load: /home/hoang/github/BERT_ABSA/dataset/raw_data/Restaurants_Test.xml\n",
      "Save: /home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Restaurants_Test.pkl\n",
      "\n",
      "Load: /home/hoang/github/BERT_ABSA/dataset/raw_data/Restaurants_Trial.xml\n",
      "Save: /home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Restaurants_Trial.pkl\n",
      "\n",
      "Load: /home/hoang/github/BERT_ABSA/dataset/raw_data/Laptops_Train.xml\n",
      "Save: /home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Laptops_Train.pkl\n",
      "\n",
      "Load: /home/hoang/github/BERT_ABSA/dataset/raw_data/Laptops_Test.xml\n",
      "Save: /home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Laptops_Test.pkl\n",
      "\n",
      "Load: /home/hoang/github/BERT_ABSA/dataset/raw_data/Laptops_Trial.xml\n",
      "Save: /home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Laptops_Trial.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dsname, fnames in dataset_files.items():\n",
    "    for g, fname in fnames.items():\n",
    "        input_path = str(PROJ_PATH/ 'dataset/raw_data' / fname)\n",
    "        output_path01 = str(PROJ_PATH/ 'dataset/preprocessed_data' / fname.replace('.xml', '.pkl'))\n",
    "        output_path02 = str(PROJ_PATH/ 'dataset/preprocessed_data' / fname.replace('.xml', '.csv'))\n",
    "        print(f'Load: {input_path}')\n",
    "        print(f'Save: {output_path01}\\n')\n",
    "        objs = parseXML(input_path)\n",
    "        df = convert_to_dataframe(objs)\n",
    "        pd.to_pickle(objs, output_path01)\n",
    "        df.to_csv(output_path02, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6bb73f79",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>term</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>The food is good, especially their more basic ...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>The food is good, especially their more basic ...</td>\n",
       "      <td>dishes</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>The food is good, especially their more basic ...</td>\n",
       "      <td>drinks</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1002</td>\n",
       "      <td>The view is spectacular, and the food is great.</td>\n",
       "      <td>view</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1002</td>\n",
       "      <td>The view is spectacular, and the food is great.</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text    term     label\n",
       "0  1000  The food is good, especially their more basic ...    food  positive\n",
       "1  1000  The food is good, especially their more basic ...  dishes  positive\n",
       "2  1000  The food is good, especially their more basic ...  drinks  positive\n",
       "3  1002    The view is spectacular, and the food is great.    view  positive\n",
       "4  1002    The view is spectacular, and the food is great.    food  positive"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/hoang/github/BERT_ABSA/dataset/preprocessed_data/Restaurants_Train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afc0b4b",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc6f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(pd.read_csv('../dataset/preprocessed_data/Laptops_Train.csv').T.to_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "212c8aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, data_dir, transformation='QA_M', num_classes=3, bert_tokenizer=None, max_length=0, seed=0):\n",
    "        random.seed(seed)\n",
    "        assert transformation in ['QA_M', 'QA_B', 'MLI_M', 'MLI_B'], 'Invalid transformation method'\n",
    "        assert num_classes in [2, 3], 'Invalid num_classes'\n",
    "        \n",
    "        self.transformation = transformation\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.polarity_dict = {'positive': 0, 'negative': 1, 'neutral': 2}\n",
    "        \n",
    "        # load data\n",
    "        self.data = list(pd.read_csv(data_dir).T.to_dict().values())\n",
    "        if num_classes == 2:\n",
    "            self.data = [d for d in self.data if d['label'] != 'neutral']\n",
    "    \n",
    "    def transform(self, sample):\n",
    "        seq1 = sample['text'].lower()\n",
    "        term = sample['term'].lower()\n",
    "        \n",
    "        if self.transformation == 'QA_M':\n",
    "            seq2 = f'what is the polarity of {term} ?'\n",
    "            label = self.polarity_dict[sample['label']]\n",
    "        elif self.transformation == 'MLI_M':\n",
    "            seq2 = term.lower()\n",
    "            label = self.polarity_dict[sample['label']]\n",
    "#         elif self.transformation == 'QA_B':\n",
    "#         elif self.transformation == 'MLI_B':\n",
    "        \n",
    "        return seq1, seq2, label\n",
    "        \n",
    "    def encode_text(self, seq1, seq2):\n",
    "        # encode\n",
    "        encoded_text = self.bert_tokenizer.encode_plus(\n",
    "            seq1,\n",
    "            seq2,\n",
    "            add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "            max_length=self.max_length,  # maximum length of a sentence\n",
    "            padding='max_length',  # Add [PAD]s\n",
    "            truncation=True, # Truncate up to maximum length\n",
    "            return_attention_mask=True,  # Generate the attention mask\n",
    "            return_tensors='pt',  # Ask the function to return PyTorch tensors\n",
    "        )\n",
    "        return encoded_text\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        '''\n",
    "        example = {\n",
    "            'id': 1000,\n",
    "            'text': 'The food is good, especially their more basic dishes, and the drinks are delicious.',\n",
    "            'term': 'food',\n",
    "            'label': 'positive',\n",
    "            }\n",
    "        '''\n",
    "            \n",
    "        # encoder\n",
    "        sample = self.data[item]\n",
    "        seq1, seq2, label = self.transform(sample)\n",
    "        encoded_text = self.encode_text(seq1, seq2)\n",
    "\n",
    "        single_input = {\n",
    "            'seq1': seq1,\n",
    "            'seq2': seq2,\n",
    "            'term': sample['term'],\n",
    "            'label': label, \n",
    "            'input_ids': encoded_text['input_ids'].flatten(),\n",
    "            'token_type_ids': encoded_text['token_type_ids'].flatten(),\n",
    "            'attention_mask': encoded_text['attention_mask'].flatten(),\n",
    "        }\n",
    "        return single_input\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(params)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        bert_tokenizer = BertTokenizer.from_pretrained(self.hparams.bert_name)\n",
    "        \n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            data_fit = Dataset(\n",
    "                data_dir=self.hparams.data_train_dir,\n",
    "                transformation=self.hparams.transformation,\n",
    "                num_classes=self.hparams.num_classes,\n",
    "                bert_tokenizer=bert_tokenizer,\n",
    "                max_length=self.hparams.max_length,\n",
    "                seed=self.hparams.seed)\n",
    "            \n",
    "            total_samples = data_fit.__len__()\n",
    "            train_samples = int(data_fit.__len__() * 0.8)\n",
    "            val_samples = total_samples - train_samples\n",
    "            self.data_train, self.data_val = random_split(\n",
    "                data_fit, [train_samples, val_samples], generator=torch.Generator().manual_seed(self.hparams.seed))\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.data_test = Dataset(\n",
    "                data_dir=self.hparams.data_test_dir,\n",
    "                transformation=self.hparams.transformation,\n",
    "                num_classes=self.hparams.num_classes,\n",
    "                bert_tokenizer=bert_tokenizer,\n",
    "                max_length=self.hparams.max_length,\n",
    "                seed=self.hparams.seed)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.data_train, \n",
    "            batch_size=self.hparams.batch_size, \n",
    "            num_workers=4, \n",
    "            shuffle=False, # Already shuffle in random_split() \n",
    "            drop_last=True, \n",
    "#             collate_fn=lambda x: x,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.data_val, \n",
    "            batch_size=self.hparams.batch_size, \n",
    "            num_workers=4, \n",
    "            shuffle=False,\n",
    "#             drop_last=True, \n",
    "#             collate_fn=lambda x: x,\n",
    "        )\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.data_test, \n",
    "            batch_size=self.hparams.batch_size, \n",
    "            num_workers=4, \n",
    "            shuffle=False,\n",
    "#             drop_last=True, \n",
    "#             collate_fn=lambda x: x,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e72295",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2afe3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(pl.LightningModule):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(params)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.hparams.pretrained_bert_name)\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(\n",
    "            self.hparams.pretrained_bert_name, num_labels=3, output_hidden_states=True, output_attentions=True, return_dict=False)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        return optimizer  \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        loss, logits, hidden, _ = self.bert(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels,\n",
    "        )\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def margin_loss(self,  embedding_query, embedding_pos, embedding_neg):\n",
    "        scores_pos = (embeddings_query * embeddings_pos).sum(dim=-1)\n",
    "        scores_neg = (embeddings_query * embeddings_neg).sum(dim=-1) * self.scale\n",
    "        return scores_pos - scores_neg\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # ['seq1', 'seq2', 'term', 'label', 'input_ids', 'token_type_ids', 'attention_mask']\n",
    "        logits = self.forward(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            token_type_ids=batch['token_type_ids'],\n",
    "            labels=batch['label'],\n",
    "        )\n",
    "        \n",
    "        labels = batch['label']\n",
    "        ce_loss = self.cross_entropy_loss(logits, labels)        \n",
    "#         acc = utils.calc_accuracy(logits, labels).squeeze()\n",
    "#         logs = {\n",
    "#             'loss': ce_loss,\n",
    "#             'acc': acc,\n",
    "#         }\n",
    "#         self.log_dict(logs, prog_bar=True)\n",
    "        return ce_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logits = self.forward(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            token_type_ids=batch['token_type_ids'],\n",
    "            labels=batch['label'],\n",
    "        )\n",
    "        \n",
    "        labels = batch['label']\n",
    "        ce_loss = self.cross_entropy_loss(logits, labels)        \n",
    "        acc = utils.calc_accuracy(logits, labels).squeeze()\n",
    "        macro_f1 = utils.calc_f1(logits, labels, avg_type='macro').squeeze()\n",
    "        micro_f1 = utils.calc_f1(logits, labels, avg_type='micro').squeeze()\n",
    "\n",
    "        logs = {\n",
    "            'loss': ce_loss, \n",
    "            'acc': acc,\n",
    "            'macro_f1': macro_f1,\n",
    "            'micro_f1': micro_f1\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "        return logs\n",
    "    \n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_acc = torch.stack([x['acc'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_macro_f1 = torch.stack([x['macro_f1'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_micro_f1 = torch.stack([x['micro_f1'] for x in val_step_outputs]).mean().cpu()\n",
    "        logs = {\n",
    "            'val_loss': avg_loss, \n",
    "            'val_acc': avg_acc,\n",
    "            'val_macro_f1': avg_macro_f1,\n",
    "            'val_micro_f1': avg_micro_f1,\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "     \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        logits = self.forward(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            token_type_ids=batch['token_type_ids'],\n",
    "            labels=batch['label'],\n",
    "        )\n",
    "        \n",
    "        labels = batch['label']\n",
    "        ce_loss = self.cross_entropy_loss(logits, labels)        \n",
    "        acc = utils.calc_accuracy(logits, labels).squeeze()\n",
    "        macro_f1 = utils.calc_f1(logits, labels, avg_type='macro').squeeze()\n",
    "        micro_f1 = utils.calc_f1(logits, labels, avg_type='micro').squeeze()\n",
    "\n",
    "        logs = {\n",
    "            'loss': ce_loss, \n",
    "            'acc': acc,\n",
    "            'macro_f1': macro_f1,\n",
    "            'micro_f1': micro_f1\n",
    "        }\n",
    "        return logs\n",
    "    \n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in test_step_outputs]).mean().cpu()\n",
    "        avg_acc = torch.stack([x['acc'] for x in test_step_outputs]).mean().cpu()\n",
    "        avg_macro_f1 = torch.stack([x['macro_f1'] for x in test_step_outputs]).mean().cpu()\n",
    "        avg_micro_f1 = torch.stack([x['micro_f1'] for x in test_step_outputs]).mean().cpu()\n",
    "\n",
    "        logs = {\n",
    "            'test_loss': avg_loss, \n",
    "            'test_acc': avg_acc,\n",
    "            'test_macro_f1': avg_macro_f1,\n",
    "            'test_micro_f1': avg_micro_f1,\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "        return logs\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea4229ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import commentjson\n",
    "from collections import OrderedDict\n",
    "\n",
    "def read_json(fname):\n",
    "    '''\n",
    "    Read in the json file specified by 'fname'\n",
    "    '''\n",
    "    with open(fname, 'rt') as handle:\n",
    "        return commentjson.load(handle, object_hook=OrderedDict)\n",
    "\n",
    "def build_model(config):\n",
    "    data_params, model_params = config['data_params'], config['model_params']\n",
    "    data = DataModule(data_params)\n",
    "    model = SentimentClassifier(model_params)\n",
    "    return data, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db53d7c9",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def build_trainder(config):\n",
    "    trainer_params = config['trainer_params']\n",
    "    data_params = config['data_params']\n",
    "    \n",
    "    # callbacks\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        dirpath=trainer_params['checkpoint_dir'], \n",
    "        filename='{epoch}-{val_loss:.4f}-{val_acc:.4f}-{val_macro_f1:.4f}-{val_micro_f1:.4f}',\n",
    "        save_top_k=trainer_params['top_k'],\n",
    "        verbose=True,\n",
    "        monitor=trainer_params['metric'],\n",
    "        mode=trainer_params['mode'],\n",
    "    )\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        min_delta=0.00, \n",
    "        patience=trainer_params['patience'],\n",
    "        verbose=False,\n",
    "        mode=trainer_params['mode'],\n",
    "    )\n",
    "    callbacks = [checkpoint, early_stopping]\n",
    "    \n",
    "    # trainer_kwargs\n",
    "    trainer_kwargs = {\n",
    "        'max_epochs': trainer_params['max_epochs'],\n",
    "        'gpus': 1 if torch.cuda.is_available() else 0,\n",
    "    #     \"progress_bar_refresh_rate\":p_refresh,\n",
    "    #     'gradient_clip_val': hyperparameters['grad_clip'],\n",
    "        'weights_summary': 'full',\n",
    "        'deterministic': True,\n",
    "        'callbacks': callbacks,\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(**trainer_kwargs)\n",
    "    return trainer, trainer_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7fe421a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='Training.')\n",
    "\n",
    "# parser.add_argument('-config_file', help='config file path', default='../src/restaurant_config.json', type=str)\n",
    "# parser.add_argument('-f', '--fff', help='a dummy argument to fool ipython', default='1')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# args.config = read_json(args.config_file)\n",
    "# seed_everything(args.config['data_params']['seed'], workers=True)\n",
    "# data, clf = build_model(args.config)\n",
    "# trainer, trainer_kwargs = build_trainder(args.config)\n",
    "# trainer.fit(clf, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2273a921",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb62ac8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 12345\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory ../model/restaurants exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Training.')\n",
    "\n",
    "parser.add_argument('-config_file', help='config file path', default='../src/restaurant_config.json', type=str)\n",
    "parser.add_argument('-f', '--fff', help='a dummy argument to fool ipython', default='1')\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.config = read_json(args.config_file)\n",
    "seed_everything(args.config['data_params']['seed'], workers=True)\n",
    "data, clf = build_model(args.config)\n",
    "trainer, trainer_kwargs = build_trainder(args.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c18f8bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7770c45615fa4e92adb87d860b8c70bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'loss': tensor(0.4673, device='cuda:0'), 'acc': tensor(0.8203, dtype=torch.float64), 'macro_f1': tensor(0.7241, dtype=torch.float64), 'micro_f1': tensor(0.8203, dtype=torch.float64)}, {'loss': tensor(0.4060, device='cuda:0'), 'acc': tensor(0.8906, dtype=torch.float64), 'macro_f1': tensor(0.7525, dtype=torch.float64), 'micro_f1': tensor(0.8906, dtype=torch.float64)}, {'loss': tensor(0.4146, device='cuda:0'), 'acc': tensor(0.8359, dtype=torch.float64), 'macro_f1': tensor(0.6356, dtype=torch.float64), 'micro_f1': tensor(0.8359, dtype=torch.float64)}, {'loss': tensor(0.6661, device='cuda:0'), 'acc': tensor(0.7734, dtype=torch.float64), 'macro_f1': tensor(0.6064, dtype=torch.float64), 'micro_f1': tensor(0.7734, dtype=torch.float64)}, {'loss': tensor(0.7138, device='cuda:0'), 'acc': tensor(0.7578, dtype=torch.float64), 'macro_f1': tensor(0.5623, dtype=torch.float64), 'micro_f1': tensor(0.7578, dtype=torch.float64)}, {'loss': tensor(0.7445, device='cuda:0'), 'acc': tensor(0.7500, dtype=torch.float64), 'macro_f1': tensor(0.5685, dtype=torch.float64), 'micro_f1': tensor(0.7500, dtype=torch.float64)}, {'loss': tensor(0.7893, device='cuda:0'), 'acc': tensor(0.7266, dtype=torch.float64), 'macro_f1': tensor(0.5635, dtype=torch.float64), 'micro_f1': tensor(0.7266, dtype=torch.float64)}, {'loss': tensor(0.7322, device='cuda:0'), 'acc': tensor(0.7578, dtype=torch.float64), 'macro_f1': tensor(0.5861, dtype=torch.float64), 'micro_f1': tensor(0.7578, dtype=torch.float64)}, {'loss': tensor(0.8118, device='cuda:0'), 'acc': tensor(0.7708, dtype=torch.float64), 'macro_f1': tensor(0.5916, dtype=torch.float64), 'micro_f1': tensor(0.7708, dtype=torch.float64)}]\n",
      "tensor(0.6384)\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.7870370149612427,\n",
      " 'test_loss': 0.6384071707725525,\n",
      " 'test_macro_f1': 0.6211695671081543,\n",
      " 'test_micro_f1': 0.7870370149612427}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "paths = sorted(glob.glob('/home/hoang/github/BERT_ABSA/model/restaurants/*.ckpt'))\n",
    "model_test = SentimentClassifier.load_from_checkpoint(paths[0])\n",
    "result = trainer.test(model_test, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc1d533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
