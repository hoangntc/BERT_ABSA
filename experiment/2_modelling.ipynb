{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ea9b30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_235065/2553875025.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJ_PATH=/home/hoang/github/BERT_ABSA\n",
      "PROJ_PATH=/home/hoang/github/BERT_ABSA\n"
     ]
    }
   ],
   "source": [
    "import os, sys, re, datetime, random, gzip, json, copy\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from itertools import accumulate\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "\n",
    "from time import time\n",
    "from math import ceil\n",
    "from multiprocessing import Pool\n",
    "from sentence_transformers import SentenceTransformer, models, losses, InputExample\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.trainer.trainer import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "\n",
    "from torch_geometric.nn import Sequential, HeteroConv, GINConv, GCNConv, SAGEConv, GATConv\n",
    "\n",
    "PROJ_PATH = Path(os.path.join(re.sub(\"/BERT_ABSA.*$\", '', os.getcwd()), 'BERT_ABSA'))\n",
    "print(f'PROJ_PATH={PROJ_PATH}')\n",
    "sys.path.insert(1, str(PROJ_PATH))\n",
    "sys.path.insert(1, str(PROJ_PATH/'src'))\n",
    "import utils\n",
    "from utils import *\n",
    "from dataset import DataModule\n",
    "from attention import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21cf9c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glob.glob('../model/restaurants/*.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80d8522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1afc0b4b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc6f384",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'text', 'term', 'from', 'to', 'source_dep', 'target_dep', 'edge_type', 'label'])\n",
      "{'id': '813', 'text': 'All the appetizers and salads were fabulous, the steak was mouth watering and the pasta was delicious!!!', 'term': 'appetizers', 'from': '8', 'to': '18', 'source_dep': [2, 2, 2, 2, 2, 2, 9, 2, 2, 6, 6, 2, 2, 2, 6, 9, 12, 15, 15, 15, 9, 20, 18, 20, 20, 9, 9], 'target_dep': [0, 3, 4, 1, 3, 4, 2, 3, 4, 5, 7, 6, 3, 4, 7, 8, 11, 12, 13, 14, 15, 16, 11, 18, 19, 20, 21], 'edge_type': ['dep', 'sprwrd', 'sprwrd', 'det', 'sprwrd', 'sprwrd', 'nsubj', 'sprwrd', 'sprwrd', 'cc', 'sprwrd', 'conj', 'sprwrd', 'sprwrd', 'sprwrd', 'cop', 'det', 'nsubj', 'cop', 'compound', 'ccomp', 'cc', 'det', 'nsubj', 'cop', 'conj', 'punct'], 'label': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle('../dataset/preprocessed_data/Restaurants_Trial_data.pkl')\n",
    "sample = data[0]\n",
    "print(sample.keys())\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "212c8aef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# class Dataset(Dataset):\n",
    "#     def __init__(self, data_dir, transformation='KW_M', num_classes=3, bert_tokenizer=None, max_length=0, seed=0):\n",
    "#         random.seed(seed)\n",
    "#         assert transformation in ['QA_M', 'MLI_M', 'KW_M'], 'Invalid transformation method'\n",
    "#         assert num_classes in [2, 3], 'Invalid num_classes'\n",
    "        \n",
    "#         self.transformation = transformation\n",
    "#         self.bert_tokenizer = bert_tokenizer\n",
    "#         self.max_length = max_length\n",
    "#         self.polarity_dict = {'positive': 0, 'negative': 1, 'neutral': 2}\n",
    "        \n",
    "#         # load data\n",
    "#         self.data = pd.read_pickle(data_dir)\n",
    "    \n",
    "#         if num_classes == 2:\n",
    "#             self.data = [d for d in self.data if d['label'] != 'neutral']\n",
    "    \n",
    "#     def transform(self, sample):\n",
    "#         # Transform input text to \n",
    "#         seq1 = sample['text'].lower()\n",
    "#         term = sample['term'].lower()\n",
    "        \n",
    "#         if self.transformation == 'QA_M':\n",
    "#             seq2 = f'what is the polarity of {term} ?'\n",
    "#             label = self.polarity_dict[sample['label']]\n",
    "#         elif self.transformation == 'MLI_M':\n",
    "#             seq2 = term.lower()\n",
    "#             label = self.polarity_dict[sample['label']]\n",
    "#         elif self.transformation == 'KW_M':\n",
    "#             seq2 = term\n",
    "#             label = self.polarity_dict[sample['label']]\n",
    "        \n",
    "#         return seq1, seq2, label\n",
    "        \n",
    "#     def encode_text(self, seq1, seq2):\n",
    "#         # Encode text for BERT model\n",
    "#         encoded_text = self.bert_tokenizer.encode_plus(\n",
    "#             seq1,\n",
    "#             seq2,\n",
    "#             add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "#             max_length=self.max_length,  # maximum length of a sentence\n",
    "#             padding='max_length',  # Add [PAD]s\n",
    "#             truncation=True, # Truncate up to maximum length\n",
    "#             return_attention_mask=True,  # Generate the attention mask\n",
    "#             return_tensors='pt',  # Ask the function to return PyTorch tensors\n",
    "#         )\n",
    "#         return encoded_text\n",
    "\n",
    "#     def padding_adj(self, adj):\n",
    "#         # Padding adj to fixed size\n",
    "#         pad_size = self.max_length - adj.shape[0]\n",
    "#         return np.pad(adj, [(0, pad_size), (0, pad_size)], 'constant')\n",
    "    \n",
    "#     def get_adj_of_dependency_graph(self, edge_index):\n",
    "#         edge_reindex = torch.tensor(self.reindex_edge_index(edge_index))\n",
    "#         dense_adj = to_dense_adj(edge_reindex).squeeze().numpy()\n",
    "#         return self.padding_adj(dense_adj)\n",
    "    \n",
    "#     def reindex_edge_index(self, edge_index):\n",
    "#         '''\n",
    "#         Reindex for special token id in BERT\n",
    "#         '''\n",
    "#         edge_reindex = [\n",
    "#             [i+1 for i in edge_index[0]], \n",
    "#             [i+1 for i in edge_index[1]],\n",
    "#         ]\n",
    "#         return edge_reindex\n",
    "    \n",
    "#     def __getitem__(self, item):\n",
    "#         '''\n",
    "#         sample = {\n",
    "#         'id': '813', \n",
    "#         'text': 'All the appetizers and salads were fabulous, \\\\\n",
    "#         the steak was mouth watering and the pasta was delicious!!!', \n",
    "#         'term': 'appetizers', \n",
    "#         'from': '8', \n",
    "#         'to': '18', \n",
    "#         'source_dep': [2, 2, 2, 2, 2, 2, 9, 2, 2, 6, 6, 2, 2, 2, 6, 9, 12, 15, 15, 15, 9, 20, 18, 20, 20, 9, 9], \n",
    "#         'target_dep': [0, 3, 4, 1, 3, 4, 2, 3, 4, 5, 7, 6, 3, 4, 7, 8, 11, 12, 13, 14, 15, 16, 11, 18, 19, 20, 21], \n",
    "#         'edge_type': ['dep', 'sprwrd', 'sprwrd', 'det', 'sprwrd', 'sprwrd', 'nsubj',\\\\\n",
    "#         'sprwrd', 'sprwrd', 'cc', 'sprwrd', 'conj', 'sprwrd', 'sprwrd', 'sprwrd', 'cop',\\\\\n",
    "#         'det', 'nsubj', 'cop', 'compound', 'ccomp', 'cc', 'det', 'nsubj', 'cop', 'conj', 'punct'],\n",
    "#         'label': 'positive'}\n",
    "#         '''\n",
    "            \n",
    "#         # BERT Encoder\n",
    "#         sample = self.data[item]\n",
    "#         seq1, seq2, label = self.transform(sample)\n",
    "#         encoded_text = self.encode_text(seq1, seq2)\n",
    "#         edge_index = [sample['source_dep'], sample['target_dep']]\n",
    "#         edge_reindex = self.reindex_edge_index(edge_index)\n",
    "        \n",
    "#         single_input = {\n",
    "#             'id': sample['id'],\n",
    "#             'text': sample['text'],\n",
    "#             'seq1': seq1,\n",
    "#             'seq2': seq2,\n",
    "#             'term': sample['term'],\n",
    "#             'label': label, \n",
    "#             'input_ids': encoded_text['input_ids'].flatten(),\n",
    "#             'token_type_ids': encoded_text['token_type_ids'].flatten(),\n",
    "#             'attention_mask': encoded_text['attention_mask'].flatten(),\n",
    "#             'edge_index': edge_reindex, \n",
    "#         }\n",
    "#         return single_input\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "# class DataModule(pl.LightningDataModule):\n",
    "#     def __init__(self, params):\n",
    "#         super().__init__()\n",
    "#         self.save_hyperparameters(params)\n",
    "\n",
    "#     def setup(self, stage=None):\n",
    "#         bert_tokenizer = BertTokenizer.from_pretrained(self.hparams.bert_name)\n",
    "        \n",
    "#         # Assign train/val datasets for use in dataloaders\n",
    "#         if stage == \"fit\" or stage is None:\n",
    "#             data_fit = Dataset(\n",
    "#                 data_dir=self.hparams.data_train_dir,\n",
    "#                 transformation=self.hparams.transformation,\n",
    "#                 num_classes=self.hparams.num_classes,\n",
    "#                 bert_tokenizer=bert_tokenizer,\n",
    "#                 max_length=self.hparams.max_length,\n",
    "#                 seed=self.hparams.seed)\n",
    "            \n",
    "#             total_samples = data_fit.__len__()\n",
    "#             train_samples = int(data_fit.__len__() * 0.8)\n",
    "#             val_samples = total_samples - train_samples\n",
    "#             self.data_train, self.data_val = random_split(\n",
    "#                 data_fit, [train_samples, val_samples], generator=torch.Generator().manual_seed(self.hparams.seed))\n",
    "\n",
    "#         # Assign test dataset for use in dataloader(s)\n",
    "#         if stage == \"test\" or stage is None:\n",
    "#             self.data_test = Dataset(\n",
    "#                 data_dir=self.hparams.data_test_dir,\n",
    "#                 transformation=self.hparams.transformation,\n",
    "#                 num_classes=self.hparams.num_classes,\n",
    "#                 bert_tokenizer=bert_tokenizer,\n",
    "#                 max_length=self.hparams.max_length,\n",
    "#                 seed=self.hparams.seed)\n",
    "    \n",
    "#     def reindex_node_by_graph(self, nodes, graph_order):\n",
    "#         new_nodes = [n + graph_order*self.hparams.max_length for n in nodes]\n",
    "#         return new_nodes\n",
    "    \n",
    "#     def collate_fn(self, batch):        \n",
    "#         label = torch.tensor([i['label'] for i in batch], dtype=torch.long)\n",
    "#         input_ids = torch.stack([i['input_ids'] for i in batch])\n",
    "#         token_type_ids = torch.stack([i['token_type_ids'] for i in batch])\n",
    "#         attention_mask = torch.stack([i['attention_mask'] for i in batch])\n",
    "#         source = [self.reindex_node_by_graph(j['edge_index'][0], i) for i,j in enumerate(batch)]\n",
    "#         target = [self.reindex_node_by_graph(j['edge_index'][1], i) for i,j in enumerate(batch)]\n",
    "#         edge_index = torch.tensor([\n",
    "#             [item for sublist in source for item in sublist], \n",
    "#             [item for sublist in target for item in sublist], \n",
    "#         ], dtype=torch.long)\n",
    "#         adj = to_dense_adj(edge_index).squeeze() \n",
    "#         return {\n",
    "#             'label': label,\n",
    "#             'input_ids': input_ids,\n",
    "#             'token_type_ids': token_type_ids,\n",
    "#             'attention_mask': attention_mask,\n",
    "#             'edge_index': edge_index,\n",
    "#             'adj': adj,\n",
    "#         }\n",
    "    \n",
    "#     def train_dataloader(self):\n",
    "#         return DataLoader(\n",
    "#             self.data_train, \n",
    "#             batch_size=self.hparams.batch_size, \n",
    "#             num_workers=4, \n",
    "#             shuffle=False, # Already shuffle in random_split() \n",
    "#             drop_last=True, \n",
    "#             collate_fn=self.collate_fn,\n",
    "#         )\n",
    "    \n",
    "#     def mytrain_dataloader(self):\n",
    "#         return DataLoader(\n",
    "#             self.data_train, \n",
    "#             batch_size=self.hparams.batch_size, \n",
    "#             num_workers=4, \n",
    "#             shuffle=False, # Already shuffle in random_split() \n",
    "#             drop_last=False, \n",
    "#             collate_fn=self.collate_fn,\n",
    "#         )\n",
    "    \n",
    "#     def val_dataloader(self):\n",
    "#         return DataLoader(\n",
    "#             self.data_val, \n",
    "#             batch_size=self.hparams.batch_size, \n",
    "#             num_workers=4, \n",
    "#             shuffle=False,\n",
    "#             collate_fn=self.collate_fn,\n",
    "#         )\n",
    "#     def test_dataloader(self):\n",
    "#         return DataLoader(\n",
    "#             self.data_test, \n",
    "#             batch_size=self.hparams.batch_size, \n",
    "#             num_workers=4, \n",
    "#             shuffle=False,\n",
    "#             collate_fn=self.collate_fn,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d72e144",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# convert adj to index: https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/utils/sparse.html#dense_to_sparse\n",
    "# write collate_fn for adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fd043e1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "config_file = '../src/config/restaurant_config.json'\n",
    "config = read_json(config_file)\n",
    "data_params, model_params = config['data_params'], config['model_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "701ccd31",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_params = config['data_params']\n",
    "data = DataModule(data_params)\n",
    "data.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29489309",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in data.train_dataloader():\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e72295",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2afe3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(pl.LightningModule):\n",
    "    '''\n",
    "    Bert-based sentiment classifier\n",
    "    '''\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(params)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.hparams.pretrained_bert_name)\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(\n",
    "            self.hparams.pretrained_bert_name, num_labels=3, output_hidden_states=True, output_attentions=True, return_dict=False)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        return optimizer  \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        loss, logits, hidden, _ = self.bert(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels,\n",
    "        )\n",
    "        \n",
    "        return logits\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # ['seq1', 'seq2', 'term', 'label', 'input_ids', 'token_type_ids', 'attention_mask']\n",
    "        logits = self.forward(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            token_type_ids=batch['token_type_ids'],\n",
    "            labels=batch['label'],\n",
    "        )\n",
    "        \n",
    "        labels = batch['label']\n",
    "        ce_loss = self.cross_entropy_loss(logits, labels)        \n",
    "#         acc = utils.calc_accuracy(logits, labels).squeeze()\n",
    "#         logs = {\n",
    "#             'loss': ce_loss,\n",
    "#             'acc': acc,\n",
    "#         }\n",
    "#         self.log_dict(logs, prog_bar=True)\n",
    "        return ce_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logits = self.forward(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            token_type_ids=batch['token_type_ids'],\n",
    "            labels=batch['label'],\n",
    "        )\n",
    "        \n",
    "        labels = batch['label']\n",
    "        ce_loss = self.cross_entropy_loss(logits, labels)        \n",
    "        acc = utils.calc_accuracy(logits, labels).squeeze()\n",
    "        auc = roc_auc_score(labels.cpu(), F.softmax(logits, dim=1).cpu(), multi_class='ovr')\n",
    "        macro_f1 = utils.calc_f1(logits, labels, avg_type='macro').squeeze()\n",
    "        micro_f1 = utils.calc_f1(logits, labels, avg_type='micro').squeeze()\n",
    "\n",
    "        logs = {\n",
    "            'loss': ce_loss, \n",
    "            'acc': acc,\n",
    "            'auc': auc,\n",
    "            'macro_f1': macro_f1,\n",
    "            'micro_f1': micro_f1\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "        return logs\n",
    "    \n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_acc = torch.stack([x['acc'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_auc = sum([x['acc'] for x in val_step_outputs])/ len([x['acc'] for x in val_step_outputs])\n",
    "        avg_macro_f1 = torch.stack([x['macro_f1'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_micro_f1 = torch.stack([x['micro_f1'] for x in val_step_outputs]).mean().cpu()\n",
    "        logs = {\n",
    "            'val_loss': avg_loss, \n",
    "            'val_acc': avg_acc,\n",
    "            'val_auc': avg_auc,\n",
    "            'val_macro_f1': avg_macro_f1,\n",
    "            'val_micro_f1': avg_micro_f1,\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "     \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        logits = self.forward(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            token_type_ids=batch['token_type_ids'],\n",
    "            labels=batch['label'],\n",
    "        )\n",
    "        \n",
    "        labels = batch['label']\n",
    "        ce_loss = self.cross_entropy_loss(logits, labels)        \n",
    "        acc = utils.calc_accuracy(logits, labels).squeeze()\n",
    "        auc = roc_auc_score(labels.cpu(), F.softmax(logits, dim=1).cpu(), multi_class='ovr')\n",
    "        macro_f1 = utils.calc_f1(logits, labels, avg_type='macro').squeeze()\n",
    "        micro_f1 = utils.calc_f1(logits, labels, avg_type='micro').squeeze()\n",
    "\n",
    "        logs = {\n",
    "            'loss': ce_loss, \n",
    "            'acc': acc,\n",
    "            'macro_f1': macro_f1,\n",
    "            'micro_f1': micro_f1\n",
    "        }\n",
    "        return logs\n",
    "    \n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in test_step_outputs]).mean().cpu()\n",
    "        avg_acc = torch.stack([x['acc'] for x in test_step_outputs]).mean().cpu()\n",
    "        avg_auc = sum([x['acc'] for x in test_step_outputs])/ len([x['acc'] for x in test_step_outputs])\n",
    "        avg_macro_f1 = torch.stack([x['macro_f1'] for x in test_step_outputs]).mean().cpu()\n",
    "        avg_micro_f1 = torch.stack([x['micro_f1'] for x in test_step_outputs]).mean().cpu()\n",
    "        logs = {\n",
    "            'test_loss': avg_loss, \n",
    "            'test_acc': avg_acc,\n",
    "            'test_auc': avg_auc,\n",
    "            'test_macro_f1': avg_macro_f1,\n",
    "            'test_micro_f1': avg_micro_f1,\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "        return logs\n",
    "\n",
    "class SynSentimentClassifier(pl.LightningModule):\n",
    "    '''\n",
    "    Bert-based sentiment classifier with Syntax-aware from Dependency Tree\n",
    "    '''\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(params)\n",
    "        # Bert\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.hparams.pretrained_bert_name)\n",
    "        self.bert = BertModel.from_pretrained(\n",
    "            self.hparams.pretrained_bert_name, output_hidden_states=True, output_attentions=True, return_dict=False)\n",
    "        self.bert_size = self.bert.config.hidden_size\n",
    "        self.hidden_size =  self.hparams.hidden_size\n",
    "        \n",
    "        # GNN\n",
    "        heads = 9\n",
    "        self.convs = Sequential('x, edge_index', [\n",
    "            (GATConv(self.bert_size, self.hidden_size, heads=heads), 'x, edge_index -> x'),\n",
    "            nn.ReLU(inplace=True),\n",
    "            (GATConv(heads * self.hidden_size, self.hidden_size, heads=heads), 'x, edge_index -> x'),\n",
    "            nn.ReLU(inplace=True),\n",
    "            (GATConv(heads * self.hidden_size, self.hidden_size, heads=1), 'x, edge_index -> x'),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ])\n",
    "#         self.convs = torch.nn.ModuleList()\n",
    "#         heads = 9\n",
    "#         self.convs.append(GATConv(self.bert_size, self.hidden_size, heads=heads))\n",
    "#         for _ in range(self.n_layers - 2):\n",
    "#             self.convs.append(GATConv(heads * self.hidden_size, self.hidden_size, heads=heads))\n",
    "#         self.convs.append(GATConv(heads * self.hidden_size, self.hidden_size, heads=1))\n",
    "        \n",
    "        # Attention\n",
    "        self.attn_vector = Parameter(torch.zeros((self.hidden_size, 1), dtype=torch.float), requires_grad=True)   \n",
    "        nn.init.xavier_uniform_(self.attn_vector)\n",
    "        self.attention = AdditiveAttention(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "        # \n",
    "        self.lin =  nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(p=self.hparams.hidden_dropout_prob)\n",
    "        self.lin1 = nn.Linear(self.hidden_size, 64)\n",
    "        self.dropout1 = nn.Dropout(p=self.hparams.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(self.hidden_size, 3)\n",
    "        \n",
    "        # Loss\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        return optimizer  \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, edge_index, labels):\n",
    "        bsize = input_ids.shape[0]\n",
    "        bert_features = self.bert(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids,\n",
    "        )[0]\n",
    "        \n",
    "        features = torch.reshape(\n",
    "            bert_features, (bert_features.shape[0]*bert_features.shape[1], bert_features.shape[2]))\n",
    "        \n",
    "        h1 = self.convs(features, edge_index)\n",
    "        batch_graph_shape = (bert_features.shape[0], bert_features.shape[1], h1.shape[1])\n",
    "        h2 = torch.reshape(h1, batch_graph_shape)\n",
    "        mask = copy.deepcopy(attention_mask).reshape(-1).tile(self.hidden_size, 1).T.reshape(batch_graph_shape)\n",
    "        \n",
    "        h3 = mask * h2\n",
    "        graph_attn = self.attn_vector.squeeze().unsqueeze(0).repeat(bsize, 1)\n",
    "        attn_weights = self.attention(graph_attn, h3) \n",
    "        h4 = weighted_sum(h3, attn_weights)\n",
    "        h5 = F.relu(self.lin(h4))\n",
    "        h5 = self.dropout(h5)\n",
    "        h6 = F.relu(self.lin1(h6))\n",
    "        graph_embedding = self.dropout1(h6)\n",
    "        logits = self.classifier(graph_embedding)\n",
    "        return logits\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        logits = self.forward(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            token_type_ids=batch['token_type_ids'],\n",
    "            edge_index=batch['edge_index'],\n",
    "            labels=batch['label'],\n",
    "        )\n",
    "        \n",
    "        labels = batch['label']\n",
    "        ce_loss = self.cross_entropy_loss(logits, labels)        \n",
    "        return ce_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logits = self.forward(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            token_type_ids=batch['token_type_ids'],\n",
    "            edge_index=batch['edge_index'],\n",
    "            labels=batch['label'],\n",
    "        )\n",
    "        \n",
    "        labels = batch['label']\n",
    "        ce_loss = self.cross_entropy_loss(logits, labels)        \n",
    "        acc = utils.calc_accuracy(logits, labels).squeeze()\n",
    "        auc = roc_auc_score(labels.cpu(), F.softmax(logits, dim=1).cpu(), multi_class='ovr')\n",
    "        macro_f1 = utils.calc_f1(logits, labels, avg_type='macro').squeeze()\n",
    "        micro_f1 = utils.calc_f1(logits, labels, avg_type='micro').squeeze()\n",
    "\n",
    "        logs = {\n",
    "            'loss': ce_loss, \n",
    "            'acc': acc,\n",
    "            'auc': auc,\n",
    "            'macro_f1': macro_f1,\n",
    "            'micro_f1': micro_f1\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "        return logs\n",
    "    \n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_acc = torch.stack([x['acc'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_auc = sum([x['acc'] for x in val_step_outputs])/ len([x['acc'] for x in val_step_outputs])\n",
    "        avg_macro_f1 = torch.stack([x['macro_f1'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_micro_f1 = torch.stack([x['micro_f1'] for x in val_step_outputs]).mean().cpu()\n",
    "        logs = {\n",
    "            'val_loss': avg_loss, \n",
    "            'val_acc': avg_acc,\n",
    "            'val_auc': avg_auc,\n",
    "            'val_macro_f1': avg_macro_f1,\n",
    "            'val_micro_f1': avg_micro_f1,\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "     \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        logits = self.forward(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            token_type_ids=batch['token_type_ids'],\n",
    "            edge_index=batch['edge_index'],\n",
    "            labels=batch['label'],\n",
    "        )\n",
    "        \n",
    "        labels = batch['label']\n",
    "        ce_loss = self.cross_entropy_loss(logits, labels)        \n",
    "        acc = utils.calc_accuracy(logits, labels).squeeze()\n",
    "        auc = roc_auc_score(labels.cpu(), F.softmax(logits, dim=1).cpu(), multi_class='ovr')\n",
    "        macro_f1 = utils.calc_f1(logits, labels, avg_type='macro').squeeze()\n",
    "        micro_f1 = utils.calc_f1(logits, labels, avg_type='micro').squeeze()\n",
    "\n",
    "        logs = {\n",
    "            'loss': ce_loss, \n",
    "            'acc': acc,\n",
    "            'auc': auc,\n",
    "            'macro_f1': macro_f1,\n",
    "            'micro_f1': micro_f1\n",
    "        }\n",
    "        return logs\n",
    "    \n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in test_step_outputs]).mean().cpu()\n",
    "        avg_acc = torch.stack([x['acc'] for x in test_step_outputs]).mean().cpu()\n",
    "        avg_auc = sum([x['acc'] for x in test_step_outputs])/ len([x['acc'] for x in test_step_outputs])\n",
    "        avg_macro_f1 = torch.stack([x['macro_f1'] for x in test_step_outputs]).mean().cpu()\n",
    "        avg_micro_f1 = torch.stack([x['micro_f1'] for x in test_step_outputs]).mean().cpu()\n",
    "\n",
    "        logs = {\n",
    "            'test_loss': avg_loss, \n",
    "            'test_acc': avg_acc,\n",
    "            'test_auc': avg_auc,\n",
    "            'test_macro_f1': avg_macro_f1,\n",
    "            'test_micro_f1': avg_micro_f1,\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "        return logs\n",
    "\n",
    "class SynSemSentimentClassifier(pl.LightningModule):\n",
    "    '''\n",
    "    Bert-based sentiment classifier with Syntax-aware from Dependency Tree and Semantics-aware from BERT\n",
    "    '''\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(params)\n",
    "        # Bert\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.hparams.pretrained_bert_name)\n",
    "        self.bert = BertModel.from_pretrained(\n",
    "            self.hparams.pretrained_bert_name, output_hidden_states=True, output_attentions=True, return_dict=False)\n",
    "        self.bert_size = self.bert.config.hidden_size\n",
    "        self.hidden_size =  self.hparams.hidden_size\n",
    "        \n",
    "        # Semantic\n",
    "        self.lin = nn.Linear(self.bert_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(p=self.hparams.hidden_dropout_prob)\n",
    "        \n",
    "        # Syntactic\n",
    "        heads = 9\n",
    "        self.convs = Sequential('x, edge_index', [\n",
    "            (GATConv(self.bert_size, self.hidden_size, heads=heads), 'x, edge_index -> x'),\n",
    "            nn.ReLU(inplace=True),\n",
    "            (GATConv(heads * self.hidden_size, self.hidden_size, heads=heads), 'x, edge_index -> x'),\n",
    "            nn.ReLU(inplace=True),\n",
    "            (GATConv(heads * self.hidden_size, self.hidden_size, heads=1), 'x, edge_index -> x'),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ])\n",
    "        \n",
    "        # Attention\n",
    "        self.attn_vector = Parameter(torch.zeros((self.hidden_size, 1), dtype=torch.float), requires_grad=True)   \n",
    "        nn.init.xavier_uniform_(self.attn_vector)\n",
    "        self.attention = AdditiveAttention(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "        # \n",
    "        self.lin1 =  nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.dropout1 = nn.Dropout(p=self.hparams.hidden_dropout_prob)\n",
    "        self.lin2 = nn.Linear(self.hidden_size, 64)\n",
    "        self.dropout2 = nn.Dropout(p=self.hparams.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(self.hidden_size, 3)\n",
    "        \n",
    "        # Loss\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        return optimizer  \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, edge_index, labels):\n",
    "        bsize = input_ids.shape[0]\n",
    "        bert_features, pooler_output, _, _, _ = self.bert(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        \n",
    "        # Semantic\n",
    "        sem_h = F.relu(self.lin(pooler_output))\n",
    "        sem_h = self.dropout(sem_h)\n",
    "        \n",
    "        # Syntactic\n",
    "        features = torch.reshape(\n",
    "            bert_features, (bert_features.shape[0]*bert_features.shape[1], bert_features.shape[2]))\n",
    "        syn_h1 = self.convs(features, edge_index)\n",
    "        batch_graph_shape = (bert_features.shape[0], bert_features.shape[1], syn_h1.shape[1])\n",
    "        syn_h2 = torch.reshape(syn_h1, batch_graph_shape)\n",
    "        mask = copy.deepcopy(attention_mask).reshape(-1).tile(self.hidden_size, 1).T.reshape(batch_graph_shape)\n",
    "        syn_h3 = mask * syn_h1\n",
    "        graph_attn = self.attn_vector.squeeze().unsqueeze(0).repeat(bsize, 1)\n",
    "        attn_weights = self.attention(graph_attn, syn_h3) \n",
    "        syn_h = weighted_sum(syn_h3, attn_weights)\n",
    "        \n",
    "        # Combine semantic and Syntactic features\n",
    "        h = torch.cat((sem_h, syn_h), dim=1)\n",
    "        h1 = F.relu(self.lin1(h))\n",
    "        h1 = self.dropout1(h1)\n",
    "        h2 = F.relu(self.lin2(h2))\n",
    "        graph_embedding = self.dropout2(h2)\n",
    "        \n",
    "        # Get logits\n",
    "        logits = self.classifier(graph_embedding)\n",
    "        return logits\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        logits = self.forward(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            token_type_ids=batch['token_type_ids'],\n",
    "            edge_index=batch['edge_index'],\n",
    "            labels=batch['label'],\n",
    "        )\n",
    "        \n",
    "        labels = batch['label']\n",
    "        ce_loss = self.cross_entropy_loss(logits, labels)        \n",
    "        return ce_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logits = self.forward(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            token_type_ids=batch['token_type_ids'],\n",
    "            edge_index=batch['edge_index'],\n",
    "            labels=batch['label'],\n",
    "        )\n",
    "        \n",
    "        labels = batch['label']\n",
    "        ce_loss = self.cross_entropy_loss(logits, labels)        \n",
    "        acc = utils.calc_accuracy(logits, labels).squeeze()\n",
    "        auc = roc_auc_score(labels.cpu(), F.softmax(logits, dim=1).cpu(), multi_class='ovr')\n",
    "        macro_f1 = utils.calc_f1(logits, labels, avg_type='macro').squeeze()\n",
    "        micro_f1 = utils.calc_f1(logits, labels, avg_type='micro').squeeze()\n",
    "\n",
    "        logs = {\n",
    "            'loss': ce_loss, \n",
    "            'acc': acc,\n",
    "            'auc': auc,\n",
    "            'macro_f1': macro_f1,\n",
    "            'micro_f1': micro_f1\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "        return logs\n",
    "    \n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_acc = torch.stack([x['acc'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_auc = sum([x['acc'] for x in val_step_outputs])/ len([x['acc'] for x in val_step_outputs])\n",
    "        avg_macro_f1 = torch.stack([x['macro_f1'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_micro_f1 = torch.stack([x['micro_f1'] for x in val_step_outputs]).mean().cpu()\n",
    "        logs = {\n",
    "            'val_loss': avg_loss, \n",
    "            'val_acc': avg_acc,\n",
    "            'val_auc': avg_auc,\n",
    "            'val_macro_f1': avg_macro_f1,\n",
    "            'val_micro_f1': avg_micro_f1,\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "     \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        logits = self.forward(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            token_type_ids=batch['token_type_ids'],\n",
    "            edge_index=batch['edge_index'],\n",
    "            labels=batch['label'],\n",
    "        )\n",
    "        \n",
    "        labels = batch['label']\n",
    "        ce_loss = self.cross_entropy_loss(logits, labels)        \n",
    "        acc = utils.calc_accuracy(logits, labels).squeeze()\n",
    "        auc = roc_auc_score(labels.cpu(), F.softmax(logits, dim=1).cpu(), multi_class='ovr')\n",
    "        macro_f1 = utils.calc_f1(logits, labels, avg_type='macro').squeeze()\n",
    "        micro_f1 = utils.calc_f1(logits, labels, avg_type='micro').squeeze()\n",
    "\n",
    "        logs = {\n",
    "            'loss': ce_loss, \n",
    "            'acc': acc,\n",
    "            'auc': auc,\n",
    "            'macro_f1': macro_f1,\n",
    "            'micro_f1': micro_f1\n",
    "        }\n",
    "        return logs\n",
    "    \n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in test_step_outputs]).mean().cpu()\n",
    "        avg_acc = torch.stack([x['acc'] for x in test_step_outputs]).mean().cpu()\n",
    "        avg_auc = sum([x['acc'] for x in test_step_outputs])/ len([x['acc'] for x in test_step_outputs])\n",
    "        avg_macro_f1 = torch.stack([x['macro_f1'] for x in test_step_outputs]).mean().cpu()\n",
    "        avg_micro_f1 = torch.stack([x['micro_f1'] for x in test_step_outputs]).mean().cpu()\n",
    "\n",
    "        logs = {\n",
    "            'test_loss': avg_loss, \n",
    "            'test_acc': avg_acc,\n",
    "            'test_auc': avg_auc,\n",
    "            'test_macro_f1': avg_macro_f1,\n",
    "            'test_micro_f1': avg_micro_f1,\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "        return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea4229ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import commentjson\n",
    "from collections import OrderedDict\n",
    "\n",
    "def read_json(fname):\n",
    "    '''\n",
    "    Read in the json file specified by 'fname'\n",
    "    '''\n",
    "    with open(fname, 'rt') as handle:\n",
    "        return commentjson.load(handle, object_hook=OrderedDict)\n",
    "\n",
    "def build_model(config):\n",
    "    data_params, model_params = config['data_params'], config['model_params']\n",
    "    data = DataModule(data_params)\n",
    "    model = SynSentimentClassifier(model_params)\n",
    "    return data, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db53d7c9",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def build_trainder(config):\n",
    "    trainer_params = config['trainer_params']\n",
    "    data_params = config['data_params']\n",
    "    \n",
    "    # callbacks\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        dirpath=trainer_params['checkpoint_dir'], \n",
    "        filename='{epoch}-{val_loss:.4f}-{val_acc:.4f}-{val_macro_f1:.4f}-{val_micro_f1:.4f}',\n",
    "        save_top_k=trainer_params['top_k'],\n",
    "        verbose=True,\n",
    "        monitor=trainer_params['metric'],\n",
    "        mode=trainer_params['mode'],\n",
    "    )\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        min_delta=0.00, \n",
    "        patience=trainer_params['patience'],\n",
    "        verbose=False,\n",
    "        mode=trainer_params['mode'],\n",
    "    )\n",
    "    callbacks = [checkpoint, early_stopping]\n",
    "    \n",
    "    # trainer_kwargs\n",
    "    trainer_kwargs = {\n",
    "        'max_epochs': trainer_params['max_epochs'],\n",
    "        'gpus': 1 if torch.cuda.is_available() else 0,\n",
    "    #     \"progress_bar_refresh_rate\":p_refresh,\n",
    "    #     'gradient_clip_val': hyperparameters['grad_clip'],\n",
    "        'weights_summary': 'full',\n",
    "        'deterministic': True,\n",
    "        'callbacks': callbacks,\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(**trainer_kwargs)\n",
    "    return trainer, trainer_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fe421a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='Training.')\n",
    "\n",
    "# parser.add_argument('-config_file', help='config file path', default='../src/config/laptop_config.json', type=str)\n",
    "# parser.add_argument('-f', '--fff', help='a dummy argument to fool ipython', default='1')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# args.config = read_json(args.config_file)\n",
    "# seed_everything(args.config['data_params']['seed'], workers=True)\n",
    "# data, clf = build_model(args.config)\n",
    "# trainer, trainer_kwargs = build_trainder(args.config)\n",
    "# trainer.fit(clf, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad00c622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9246647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d4cae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2273a921",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb62ac8d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 12345\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory ../model/restaurants exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Training.')\n",
    "\n",
    "parser.add_argument('-config_file', help='config file path', default='../src/restaurant_config.json', type=str)\n",
    "parser.add_argument('-f', '--fff', help='a dummy argument to fool ipython', default='1')\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.config = read_json(args.config_file)\n",
    "seed_everything(args.config['data_params']['seed'], workers=True)\n",
    "data, clf = build_model(args.config)\n",
    "trainer, trainer_kwargs = build_trainder(args.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c18f8bad",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7770c45615fa4e92adb87d860b8c70bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'loss': tensor(0.4673, device='cuda:0'), 'acc': tensor(0.8203, dtype=torch.float64), 'macro_f1': tensor(0.7241, dtype=torch.float64), 'micro_f1': tensor(0.8203, dtype=torch.float64)}, {'loss': tensor(0.4060, device='cuda:0'), 'acc': tensor(0.8906, dtype=torch.float64), 'macro_f1': tensor(0.7525, dtype=torch.float64), 'micro_f1': tensor(0.8906, dtype=torch.float64)}, {'loss': tensor(0.4146, device='cuda:0'), 'acc': tensor(0.8359, dtype=torch.float64), 'macro_f1': tensor(0.6356, dtype=torch.float64), 'micro_f1': tensor(0.8359, dtype=torch.float64)}, {'loss': tensor(0.6661, device='cuda:0'), 'acc': tensor(0.7734, dtype=torch.float64), 'macro_f1': tensor(0.6064, dtype=torch.float64), 'micro_f1': tensor(0.7734, dtype=torch.float64)}, {'loss': tensor(0.7138, device='cuda:0'), 'acc': tensor(0.7578, dtype=torch.float64), 'macro_f1': tensor(0.5623, dtype=torch.float64), 'micro_f1': tensor(0.7578, dtype=torch.float64)}, {'loss': tensor(0.7445, device='cuda:0'), 'acc': tensor(0.7500, dtype=torch.float64), 'macro_f1': tensor(0.5685, dtype=torch.float64), 'micro_f1': tensor(0.7500, dtype=torch.float64)}, {'loss': tensor(0.7893, device='cuda:0'), 'acc': tensor(0.7266, dtype=torch.float64), 'macro_f1': tensor(0.5635, dtype=torch.float64), 'micro_f1': tensor(0.7266, dtype=torch.float64)}, {'loss': tensor(0.7322, device='cuda:0'), 'acc': tensor(0.7578, dtype=torch.float64), 'macro_f1': tensor(0.5861, dtype=torch.float64), 'micro_f1': tensor(0.7578, dtype=torch.float64)}, {'loss': tensor(0.8118, device='cuda:0'), 'acc': tensor(0.7708, dtype=torch.float64), 'macro_f1': tensor(0.5916, dtype=torch.float64), 'micro_f1': tensor(0.7708, dtype=torch.float64)}]\n",
      "tensor(0.6384)\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.7870370149612427,\n",
      " 'test_loss': 0.6384071707725525,\n",
      " 'test_macro_f1': 0.6211695671081543,\n",
      " 'test_micro_f1': 0.7870370149612427}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "paths = sorted(glob.glob('/home/hoang/github/BERT_ABSA/model/restaurants/*.ckpt'))\n",
    "model_test = SentimentClassifier.load_from_checkpoint(paths[0])\n",
    "result = trainer.test(model_test, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc1d533",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
