{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ea9b30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_235065/2553875025.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJ_PATH=/home/hoang/github/BERT_ABSA\n",
      "PROJ_PATH=/home/hoang/github/BERT_ABSA\n"
     ]
    }
   ],
   "source": [
    "import os, sys, re, datetime, random, gzip, json, copy\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from itertools import accumulate\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "\n",
    "from time import time\n",
    "from math import ceil\n",
    "from multiprocessing import Pool\n",
    "from sentence_transformers import SentenceTransformer, models, losses, InputExample\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.trainer.trainer import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "\n",
    "from torch_geometric.nn import Sequential, HeteroConv, GINConv, GCNConv, SAGEConv, GATConv\n",
    "\n",
    "PROJ_PATH = Path(os.path.join(re.sub(\"/BERT_ABSA.*$\", '', os.getcwd()), 'BERT_ABSA'))\n",
    "print(f'PROJ_PATH={PROJ_PATH}')\n",
    "sys.path.insert(1, str(PROJ_PATH))\n",
    "sys.path.insert(1, str(PROJ_PATH/'src'))\n",
    "import utils\n",
    "from utils import *\n",
    "from dataset import DataModule\n",
    "from attention import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21cf9c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glob.glob('../model/restaurants/*.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80d8522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1afc0b4b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc6f384",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'text', 'term', 'from', 'to', 'source_dep', 'target_dep', 'edge_type', 'label'])\n",
      "{'id': '813', 'text': 'All the appetizers and salads were fabulous, the steak was mouth watering and the pasta was delicious!!!', 'term': 'appetizers', 'from': '8', 'to': '18', 'source_dep': [2, 2, 2, 2, 2, 2, 9, 2, 2, 6, 6, 2, 2, 2, 6, 9, 12, 15, 15, 15, 9, 20, 18, 20, 20, 9, 9], 'target_dep': [0, 3, 4, 1, 3, 4, 2, 3, 4, 5, 7, 6, 3, 4, 7, 8, 11, 12, 13, 14, 15, 16, 11, 18, 19, 20, 21], 'edge_type': ['dep', 'sprwrd', 'sprwrd', 'det', 'sprwrd', 'sprwrd', 'nsubj', 'sprwrd', 'sprwrd', 'cc', 'sprwrd', 'conj', 'sprwrd', 'sprwrd', 'sprwrd', 'cop', 'det', 'nsubj', 'cop', 'compound', 'ccomp', 'cc', 'det', 'nsubj', 'cop', 'conj', 'punct'], 'label': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle('../dataset/preprocessed_data/Restaurants_Trial_data.pkl')\n",
    "sample = data[0]\n",
    "print(sample.keys())\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "212c8aef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# class Dataset(Dataset):\n",
    "#     def __init__(self, data_dir, transformation='KW_M', num_classes=3, bert_tokenizer=None, max_length=0, seed=0):\n",
    "#         random.seed(seed)\n",
    "#         assert transformation in ['QA_M', 'MLI_M', 'KW_M'], 'Invalid transformation method'\n",
    "#         assert num_classes in [2, 3], 'Invalid num_classes'\n",
    "        \n",
    "#         self.transformation = transformation\n",
    "#         self.bert_tokenizer = bert_tokenizer\n",
    "#         self.max_length = max_length\n",
    "#         self.polarity_dict = {'positive': 0, 'negative': 1, 'neutral': 2}\n",
    "        \n",
    "#         # load data\n",
    "#         self.data = pd.read_pickle(data_dir)\n",
    "    \n",
    "#         if num_classes == 2:\n",
    "#             self.data = [d for d in self.data if d['label'] != 'neutral']\n",
    "    \n",
    "#     def transform(self, sample):\n",
    "#         # Transform input text to \n",
    "#         seq1 = sample['text'].lower()\n",
    "#         term = sample['term'].lower()\n",
    "        \n",
    "#         if self.transformation == 'QA_M':\n",
    "#             seq2 = f'what is the polarity of {term} ?'\n",
    "#             label = self.polarity_dict[sample['label']]\n",
    "#         elif self.transformation == 'MLI_M':\n",
    "#             seq2 = term.lower()\n",
    "#             label = self.polarity_dict[sample['label']]\n",
    "#         elif self.transformation == 'KW_M':\n",
    "#             seq2 = term\n",
    "#             label = self.polarity_dict[sample['label']]\n",
    "        \n",
    "#         return seq1, seq2, label\n",
    "        \n",
    "#     def encode_text(self, seq1, seq2):\n",
    "#         # Encode text for BERT model\n",
    "#         encoded_text = self.bert_tokenizer.encode_plus(\n",
    "#             seq1,\n",
    "#             seq2,\n",
    "#             add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "#             max_length=self.max_length,  # maximum length of a sentence\n",
    "#             padding='max_length',  # Add [PAD]s\n",
    "#             truncation=True, # Truncate up to maximum length\n",
    "#             return_attention_mask=True,  # Generate the attention mask\n",
    "#             return_tensors='pt',  # Ask the function to return PyTorch tensors\n",
    "#         )\n",
    "#         return encoded_text\n",
    "\n",
    "#     def padding_adj(self, adj):\n",
    "#         # Padding adj to fixed size\n",
    "#         pad_size = self.max_length - adj.shape[0]\n",
    "#         return np.pad(adj, [(0, pad_size), (0, pad_size)], 'constant')\n",
    "    \n",
    "#     def get_adj_of_dependency_graph(self, edge_index):\n",
    "#         edge_reindex = torch.tensor(self.reindex_edge_index(edge_index))\n",
    "#         dense_adj = to_dense_adj(edge_reindex).squeeze().numpy()\n",
    "#         return self.padding_adj(dense_adj)\n",
    "    \n",
    "#     def reindex_edge_index(self, edge_index):\n",
    "#         '''\n",
    "#         Reindex for special token id in BERT\n",
    "#         '''\n",
    "#         edge_reindex = [\n",
    "#             [i+1 for i in edge_index[0]], \n",
    "#             [i+1 for i in edge_index[1]],\n",
    "#         ]\n",
    "#         return edge_reindex\n",
    "    \n",
    "#     def __getitem__(self, item):\n",
    "#         '''\n",
    "#         sample = {\n",
    "#         'id': '813', \n",
    "#         'text': 'All the appetizers and salads were fabulous, \\\\\n",
    "#         the steak was mouth watering and the pasta was delicious!!!', \n",
    "#         'term': 'appetizers', \n",
    "#         'from': '8', \n",
    "#         'to': '18', \n",
    "#         'source_dep': [2, 2, 2, 2, 2, 2, 9, 2, 2, 6, 6, 2, 2, 2, 6, 9, 12, 15, 15, 15, 9, 20, 18, 20, 20, 9, 9], \n",
    "#         'target_dep': [0, 3, 4, 1, 3, 4, 2, 3, 4, 5, 7, 6, 3, 4, 7, 8, 11, 12, 13, 14, 15, 16, 11, 18, 19, 20, 21], \n",
    "#         'edge_type': ['dep', 'sprwrd', 'sprwrd', 'det', 'sprwrd', 'sprwrd', 'nsubj',\\\\\n",
    "#         'sprwrd', 'sprwrd', 'cc', 'sprwrd', 'conj', 'sprwrd', 'sprwrd', 'sprwrd', 'cop',\\\\\n",
    "#         'det', 'nsubj', 'cop', 'compound', 'ccomp', 'cc', 'det', 'nsubj', 'cop', 'conj', 'punct'],\n",
    "#         'label': 'positive'}\n",
    "#         '''\n",
    "            \n",
    "#         # BERT Encoder\n",
    "#         sample = self.data[item]\n",
    "#         seq1, seq2, label = self.transform(sample)\n",
    "#         encoded_text = self.encode_text(seq1, seq2)\n",
    "#         edge_index = [sample['source_dep'], sample['target_dep']]\n",
    "#         edge_reindex = self.reindex_edge_index(edge_index)\n",
    "        \n",
    "#         single_input = {\n",
    "#             'id': sample['id'],\n",
    "#             'text': sample['text'],\n",
    "#             'seq1': seq1,\n",
    "#             'seq2': seq2,\n",
    "#             'term': sample['term'],\n",
    "#             'label': label, \n",
    "#             'input_ids': encoded_text['input_ids'].flatten(),\n",
    "#             'token_type_ids': encoded_text['token_type_ids'].flatten(),\n",
    "#             'attention_mask': encoded_text['attention_mask'].flatten(),\n",
    "#             'edge_index': edge_reindex, \n",
    "#         }\n",
    "#         return single_input\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "# class DataModule(pl.LightningDataModule):\n",
    "#     def __init__(self, params):\n",
    "#         super().__init__()\n",
    "#         self.save_hyperparameters(params)\n",
    "\n",
    "#     def setup(self, stage=None):\n",
    "#         bert_tokenizer = BertTokenizer.from_pretrained(self.hparams.bert_name)\n",
    "        \n",
    "#         # Assign train/val datasets for use in dataloaders\n",
    "#         if stage == \"fit\" or stage is None:\n",
    "#             data_fit = Dataset(\n",
    "#                 data_dir=self.hparams.data_train_dir,\n",
    "#                 transformation=self.hparams.transformation,\n",
    "#                 num_classes=self.hparams.num_classes,\n",
    "#                 bert_tokenizer=bert_tokenizer,\n",
    "#                 max_length=self.hparams.max_length,\n",
    "#                 seed=self.hparams.seed)\n",
    "            \n",
    "#             total_samples = data_fit.__len__()\n",
    "#             train_samples = int(data_fit.__len__() * 0.8)\n",
    "#             val_samples = total_samples - train_samples\n",
    "#             self.data_train, self.data_val = random_split(\n",
    "#                 data_fit, [train_samples, val_samples], generator=torch.Generator().manual_seed(self.hparams.seed))\n",
    "\n",
    "#         # Assign test dataset for use in dataloader(s)\n",
    "#         if stage == \"test\" or stage is None:\n",
    "#             self.data_test = Dataset(\n",
    "#                 data_dir=self.hparams.data_test_dir,\n",
    "#                 transformation=self.hparams.transformation,\n",
    "#                 num_classes=self.hparams.num_classes,\n",
    "#                 bert_tokenizer=bert_tokenizer,\n",
    "#                 max_length=self.hparams.max_length,\n",
    "#                 seed=self.hparams.seed)\n",
    "    \n",
    "#     def reindex_node_by_graph(self, nodes, graph_order):\n",
    "#         new_nodes = [n + graph_order*self.hparams.max_length for n in nodes]\n",
    "#         return new_nodes\n",
    "    \n",
    "#     def collate_fn(self, batch):        \n",
    "#         label = torch.tensor([i['label'] for i in batch], dtype=torch.long)\n",
    "#         input_ids = torch.stack([i['input_ids'] for i in batch])\n",
    "#         token_type_ids = torch.stack([i['token_type_ids'] for i in batch])\n",
    "#         attention_mask = torch.stack([i['attention_mask'] for i in batch])\n",
    "#         source = [self.reindex_node_by_graph(j['edge_index'][0], i) for i,j in enumerate(batch)]\n",
    "#         target = [self.reindex_node_by_graph(j['edge_index'][1], i) for i,j in enumerate(batch)]\n",
    "#         edge_index = torch.tensor([\n",
    "#             [item for sublist in source for item in sublist], \n",
    "#             [item for sublist in target for item in sublist], \n",
    "#         ], dtype=torch.long)\n",
    "#         adj = to_dense_adj(edge_index).squeeze() \n",
    "#         return {\n",
    "#             'label': label,\n",
    "#             'input_ids': input_ids,\n",
    "#             'token_type_ids': token_type_ids,\n",
    "#             'attention_mask': attention_mask,\n",
    "#             'edge_index': edge_index,\n",
    "#             'adj': adj,\n",
    "#         }\n",
    "    \n",
    "#     def train_dataloader(self):\n",
    "#         return DataLoader(\n",
    "#             self.data_train, \n",
    "#             batch_size=self.hparams.batch_size, \n",
    "#             num_workers=4, \n",
    "#             shuffle=False, # Already shuffle in random_split() \n",
    "#             drop_last=True, \n",
    "#             collate_fn=self.collate_fn,\n",
    "#         )\n",
    "    \n",
    "#     def mytrain_dataloader(self):\n",
    "#         return DataLoader(\n",
    "#             self.data_train, \n",
    "#             batch_size=self.hparams.batch_size, \n",
    "#             num_workers=4, \n",
    "#             shuffle=False, # Already shuffle in random_split() \n",
    "#             drop_last=False, \n",
    "#             collate_fn=self.collate_fn,\n",
    "#         )\n",
    "    \n",
    "#     def val_dataloader(self):\n",
    "#         return DataLoader(\n",
    "#             self.data_val, \n",
    "#             batch_size=self.hparams.batch_size, \n",
    "#             num_workers=4, \n",
    "#             shuffle=False,\n",
    "#             collate_fn=self.collate_fn,\n",
    "#         )\n",
    "#     def test_dataloader(self):\n",
    "#         return DataLoader(\n",
    "#             self.data_test, \n",
    "#             batch_size=self.hparams.batch_size, \n",
    "#             num_workers=4, \n",
    "#             shuffle=False,\n",
    "#             collate_fn=self.collate_fn,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8b7174a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# convert adj to index: https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/utils/sparse.html#dense_to_sparse\n",
    "# write collate_fn for adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61f047b6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "config_file = '../src/config/restaurant_config.json'\n",
    "config = read_json(config_file)\n",
    "data_params, model_params = config['data_params'], config['model_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fd993f0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_params = config['data_params']\n",
    "data = DataModule(data_params)\n",
    "data.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5391c607",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in data.train_dataloader():\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e72295",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2afe3f4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SentimentClassifier(pl.LightningModule):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(params)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.hparams.pretrained_bert_name)\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(\n",
    "            self.hparams.pretrained_bert_name, num_labels=3, output_hidden_states=True, output_attentions=True, return_dict=False)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        return optimizer  \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        loss, logits, hidden, _ = self.bert(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels,\n",
    "        )\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def margin_loss(self,  embedding_query, embedding_pos, embedding_neg):\n",
    "        scores_pos = (embeddings_query * embeddings_pos).sum(dim=-1)\n",
    "        scores_neg = (embeddings_query * embeddings_neg).sum(dim=-1) * self.scale\n",
    "        return scores_pos - scores_neg\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # ['seq1', 'seq2', 'term', 'label', 'input_ids', 'token_type_ids', 'attention_mask']\n",
    "        logits = self.forward(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            token_type_ids=batch['token_type_ids'],\n",
    "            labels=batch['label'],\n",
    "        )\n",
    "        \n",
    "        labels = batch['label']\n",
    "        ce_loss = self.cross_entropy_loss(logits, labels)        \n",
    "#         acc = utils.calc_accuracy(logits, labels).squeeze()\n",
    "#         logs = {\n",
    "#             'loss': ce_loss,\n",
    "#             'acc': acc,\n",
    "#         }\n",
    "#         self.log_dict(logs, prog_bar=True)\n",
    "        return ce_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logits = self.forward(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            token_type_ids=batch['token_type_ids'],\n",
    "            labels=batch['label'],\n",
    "        )\n",
    "        \n",
    "        labels = batch['label']\n",
    "        ce_loss = self.cross_entropy_loss(logits, labels)        \n",
    "        acc = utils.calc_accuracy(logits, labels).squeeze()\n",
    "        macro_f1 = utils.calc_f1(logits, labels, avg_type='macro').squeeze()\n",
    "        micro_f1 = utils.calc_f1(logits, labels, avg_type='micro').squeeze()\n",
    "\n",
    "        logs = {\n",
    "            'loss': ce_loss, \n",
    "            'acc': acc,\n",
    "            'macro_f1': macro_f1,\n",
    "            'micro_f1': micro_f1\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "        return logs\n",
    "    \n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_acc = torch.stack([x['acc'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_macro_f1 = torch.stack([x['macro_f1'] for x in val_step_outputs]).mean().cpu()\n",
    "        avg_micro_f1 = torch.stack([x['micro_f1'] for x in val_step_outputs]).mean().cpu()\n",
    "        logs = {\n",
    "            'val_loss': avg_loss, \n",
    "            'val_acc': avg_acc,\n",
    "            'val_macro_f1': avg_macro_f1,\n",
    "            'val_micro_f1': avg_micro_f1,\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "     \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        logits = self.forward(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            token_type_ids=batch['token_type_ids'],\n",
    "            labels=batch['label'],\n",
    "        )\n",
    "        \n",
    "        labels = batch['label']\n",
    "        ce_loss = self.cross_entropy_loss(logits, labels)        \n",
    "        acc = utils.calc_accuracy(logits, labels).squeeze()\n",
    "        macro_f1 = utils.calc_f1(logits, labels, avg_type='macro').squeeze()\n",
    "        micro_f1 = utils.calc_f1(logits, labels, avg_type='micro').squeeze()\n",
    "\n",
    "        logs = {\n",
    "            'loss': ce_loss, \n",
    "            'acc': acc,\n",
    "            'macro_f1': macro_f1,\n",
    "            'micro_f1': micro_f1\n",
    "        }\n",
    "        return logs\n",
    "    \n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in test_step_outputs]).mean().cpu()\n",
    "        avg_acc = torch.stack([x['acc'] for x in test_step_outputs]).mean().cpu()\n",
    "        avg_macro_f1 = torch.stack([x['macro_f1'] for x in test_step_outputs]).mean().cpu()\n",
    "        avg_micro_f1 = torch.stack([x['micro_f1'] for x in test_step_outputs]).mean().cpu()\n",
    "\n",
    "        logs = {\n",
    "            'test_loss': avg_loss, \n",
    "            'test_acc': avg_acc,\n",
    "            'test_macro_f1': avg_macro_f1,\n",
    "            'test_micro_f1': avg_micro_f1,\n",
    "        }\n",
    "        self.log_dict(logs, prog_bar=True)\n",
    "        return logs\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea4229ea",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import commentjson\n",
    "from collections import OrderedDict\n",
    "\n",
    "def read_json(fname):\n",
    "    '''\n",
    "    Read in the json file specified by 'fname'\n",
    "    '''\n",
    "    with open(fname, 'rt') as handle:\n",
    "        return commentjson.load(handle, object_hook=OrderedDict)\n",
    "\n",
    "def build_model(config):\n",
    "    data_params, model_params = config['data_params'], config['model_params']\n",
    "    data = DataModule(data_params)\n",
    "    model = SynSentimentClassifier(model_params)\n",
    "    return data, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db53d7c9",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_trainder(config):\n",
    "    trainer_params = config['trainer_params']\n",
    "    data_params = config['data_params']\n",
    "    \n",
    "    # callbacks\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        dirpath=trainer_params['checkpoint_dir'], \n",
    "        filename='{epoch}-{val_loss:.4f}-{val_acc:.4f}-{val_macro_f1:.4f}-{val_micro_f1:.4f}',\n",
    "        save_top_k=trainer_params['top_k'],\n",
    "        verbose=True,\n",
    "        monitor=trainer_params['metric'],\n",
    "        mode=trainer_params['mode'],\n",
    "    )\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        min_delta=0.00, \n",
    "        patience=trainer_params['patience'],\n",
    "        verbose=False,\n",
    "        mode=trainer_params['mode'],\n",
    "    )\n",
    "    callbacks = [checkpoint, early_stopping]\n",
    "    \n",
    "    # trainer_kwargs\n",
    "    trainer_kwargs = {\n",
    "        'max_epochs': trainer_params['max_epochs'],\n",
    "        'gpus': 1 if torch.cuda.is_available() else 0,\n",
    "    #     \"progress_bar_refresh_rate\":p_refresh,\n",
    "    #     'gradient_clip_val': hyperparameters['grad_clip'],\n",
    "        'weights_summary': 'full',\n",
    "        'deterministic': True,\n",
    "        'callbacks': callbacks,\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(**trainer_kwargs)\n",
    "    return trainer, trainer_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fe421a1",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 12345\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory ../model/laptops exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "    | Name                                             | Type              | Params\n",
      "-----------------------------------------------------------------------------------------\n",
      "0   | bert                                             | BertModel         | 109 M \n",
      "1   | bert.embeddings                                  | BertEmbeddings    | 23.8 M\n",
      "2   | bert.embeddings.word_embeddings                  | Embedding         | 23.4 M\n",
      "3   | bert.embeddings.position_embeddings              | Embedding         | 393 K \n",
      "4   | bert.embeddings.token_type_embeddings            | Embedding         | 1.5 K \n",
      "5   | bert.embeddings.LayerNorm                        | LayerNorm         | 1.5 K \n",
      "6   | bert.embeddings.dropout                          | Dropout           | 0     \n",
      "7   | bert.encoder                                     | BertEncoder       | 85.1 M\n",
      "8   | bert.encoder.layer                               | ModuleList        | 85.1 M\n",
      "9   | bert.encoder.layer.0                             | BertLayer         | 7.1 M \n",
      "10  | bert.encoder.layer.0.attention                   | BertAttention     | 2.4 M \n",
      "11  | bert.encoder.layer.0.attention.self              | BertSelfAttention | 1.8 M \n",
      "12  | bert.encoder.layer.0.attention.self.query        | Linear            | 590 K \n",
      "13  | bert.encoder.layer.0.attention.self.key          | Linear            | 590 K \n",
      "14  | bert.encoder.layer.0.attention.self.value        | Linear            | 590 K \n",
      "15  | bert.encoder.layer.0.attention.self.dropout      | Dropout           | 0     \n",
      "16  | bert.encoder.layer.0.attention.output            | BertSelfOutput    | 592 K \n",
      "17  | bert.encoder.layer.0.attention.output.dense      | Linear            | 590 K \n",
      "18  | bert.encoder.layer.0.attention.output.LayerNorm  | LayerNorm         | 1.5 K \n",
      "19  | bert.encoder.layer.0.attention.output.dropout    | Dropout           | 0     \n",
      "20  | bert.encoder.layer.0.intermediate                | BertIntermediate  | 2.4 M \n",
      "21  | bert.encoder.layer.0.intermediate.dense          | Linear            | 2.4 M \n",
      "22  | bert.encoder.layer.0.output                      | BertOutput        | 2.4 M \n",
      "23  | bert.encoder.layer.0.output.dense                | Linear            | 2.4 M \n",
      "24  | bert.encoder.layer.0.output.LayerNorm            | LayerNorm         | 1.5 K \n",
      "25  | bert.encoder.layer.0.output.dropout              | Dropout           | 0     \n",
      "26  | bert.encoder.layer.1                             | BertLayer         | 7.1 M \n",
      "27  | bert.encoder.layer.1.attention                   | BertAttention     | 2.4 M \n",
      "28  | bert.encoder.layer.1.attention.self              | BertSelfAttention | 1.8 M \n",
      "29  | bert.encoder.layer.1.attention.self.query        | Linear            | 590 K \n",
      "30  | bert.encoder.layer.1.attention.self.key          | Linear            | 590 K \n",
      "31  | bert.encoder.layer.1.attention.self.value        | Linear            | 590 K \n",
      "32  | bert.encoder.layer.1.attention.self.dropout      | Dropout           | 0     \n",
      "33  | bert.encoder.layer.1.attention.output            | BertSelfOutput    | 592 K \n",
      "34  | bert.encoder.layer.1.attention.output.dense      | Linear            | 590 K \n",
      "35  | bert.encoder.layer.1.attention.output.LayerNorm  | LayerNorm         | 1.5 K \n",
      "36  | bert.encoder.layer.1.attention.output.dropout    | Dropout           | 0     \n",
      "37  | bert.encoder.layer.1.intermediate                | BertIntermediate  | 2.4 M \n",
      "38  | bert.encoder.layer.1.intermediate.dense          | Linear            | 2.4 M \n",
      "39  | bert.encoder.layer.1.output                      | BertOutput        | 2.4 M \n",
      "40  | bert.encoder.layer.1.output.dense                | Linear            | 2.4 M \n",
      "41  | bert.encoder.layer.1.output.LayerNorm            | LayerNorm         | 1.5 K \n",
      "42  | bert.encoder.layer.1.output.dropout              | Dropout           | 0     \n",
      "43  | bert.encoder.layer.2                             | BertLayer         | 7.1 M \n",
      "44  | bert.encoder.layer.2.attention                   | BertAttention     | 2.4 M \n",
      "45  | bert.encoder.layer.2.attention.self              | BertSelfAttention | 1.8 M \n",
      "46  | bert.encoder.layer.2.attention.self.query        | Linear            | 590 K \n",
      "47  | bert.encoder.layer.2.attention.self.key          | Linear            | 590 K \n",
      "48  | bert.encoder.layer.2.attention.self.value        | Linear            | 590 K \n",
      "49  | bert.encoder.layer.2.attention.self.dropout      | Dropout           | 0     \n",
      "50  | bert.encoder.layer.2.attention.output            | BertSelfOutput    | 592 K \n",
      "51  | bert.encoder.layer.2.attention.output.dense      | Linear            | 590 K \n",
      "52  | bert.encoder.layer.2.attention.output.LayerNorm  | LayerNorm         | 1.5 K \n",
      "53  | bert.encoder.layer.2.attention.output.dropout    | Dropout           | 0     \n",
      "54  | bert.encoder.layer.2.intermediate                | BertIntermediate  | 2.4 M \n",
      "55  | bert.encoder.layer.2.intermediate.dense          | Linear            | 2.4 M \n",
      "56  | bert.encoder.layer.2.output                      | BertOutput        | 2.4 M \n",
      "57  | bert.encoder.layer.2.output.dense                | Linear            | 2.4 M \n",
      "58  | bert.encoder.layer.2.output.LayerNorm            | LayerNorm         | 1.5 K \n",
      "59  | bert.encoder.layer.2.output.dropout              | Dropout           | 0     \n",
      "60  | bert.encoder.layer.3                             | BertLayer         | 7.1 M \n",
      "61  | bert.encoder.layer.3.attention                   | BertAttention     | 2.4 M \n",
      "62  | bert.encoder.layer.3.attention.self              | BertSelfAttention | 1.8 M \n",
      "63  | bert.encoder.layer.3.attention.self.query        | Linear            | 590 K \n",
      "64  | bert.encoder.layer.3.attention.self.key          | Linear            | 590 K \n",
      "65  | bert.encoder.layer.3.attention.self.value        | Linear            | 590 K \n",
      "66  | bert.encoder.layer.3.attention.self.dropout      | Dropout           | 0     \n",
      "67  | bert.encoder.layer.3.attention.output            | BertSelfOutput    | 592 K \n",
      "68  | bert.encoder.layer.3.attention.output.dense      | Linear            | 590 K \n",
      "69  | bert.encoder.layer.3.attention.output.LayerNorm  | LayerNorm         | 1.5 K \n",
      "70  | bert.encoder.layer.3.attention.output.dropout    | Dropout           | 0     \n",
      "71  | bert.encoder.layer.3.intermediate                | BertIntermediate  | 2.4 M \n",
      "72  | bert.encoder.layer.3.intermediate.dense          | Linear            | 2.4 M \n",
      "73  | bert.encoder.layer.3.output                      | BertOutput        | 2.4 M \n",
      "74  | bert.encoder.layer.3.output.dense                | Linear            | 2.4 M \n",
      "75  | bert.encoder.layer.3.output.LayerNorm            | LayerNorm         | 1.5 K \n",
      "76  | bert.encoder.layer.3.output.dropout              | Dropout           | 0     \n",
      "77  | bert.encoder.layer.4                             | BertLayer         | 7.1 M \n",
      "78  | bert.encoder.layer.4.attention                   | BertAttention     | 2.4 M \n",
      "79  | bert.encoder.layer.4.attention.self              | BertSelfAttention | 1.8 M \n",
      "80  | bert.encoder.layer.4.attention.self.query        | Linear            | 590 K \n",
      "81  | bert.encoder.layer.4.attention.self.key          | Linear            | 590 K \n",
      "82  | bert.encoder.layer.4.attention.self.value        | Linear            | 590 K \n",
      "83  | bert.encoder.layer.4.attention.self.dropout      | Dropout           | 0     \n",
      "84  | bert.encoder.layer.4.attention.output            | BertSelfOutput    | 592 K \n",
      "85  | bert.encoder.layer.4.attention.output.dense      | Linear            | 590 K \n",
      "86  | bert.encoder.layer.4.attention.output.LayerNorm  | LayerNorm         | 1.5 K \n",
      "87  | bert.encoder.layer.4.attention.output.dropout    | Dropout           | 0     \n",
      "88  | bert.encoder.layer.4.intermediate                | BertIntermediate  | 2.4 M \n",
      "89  | bert.encoder.layer.4.intermediate.dense          | Linear            | 2.4 M \n",
      "90  | bert.encoder.layer.4.output                      | BertOutput        | 2.4 M \n",
      "91  | bert.encoder.layer.4.output.dense                | Linear            | 2.4 M \n",
      "92  | bert.encoder.layer.4.output.LayerNorm            | LayerNorm         | 1.5 K \n",
      "93  | bert.encoder.layer.4.output.dropout              | Dropout           | 0     \n",
      "94  | bert.encoder.layer.5                             | BertLayer         | 7.1 M \n",
      "95  | bert.encoder.layer.5.attention                   | BertAttention     | 2.4 M \n",
      "96  | bert.encoder.layer.5.attention.self              | BertSelfAttention | 1.8 M \n",
      "97  | bert.encoder.layer.5.attention.self.query        | Linear            | 590 K \n",
      "98  | bert.encoder.layer.5.attention.self.key          | Linear            | 590 K \n",
      "99  | bert.encoder.layer.5.attention.self.value        | Linear            | 590 K \n",
      "100 | bert.encoder.layer.5.attention.self.dropout      | Dropout           | 0     \n",
      "101 | bert.encoder.layer.5.attention.output            | BertSelfOutput    | 592 K \n",
      "102 | bert.encoder.layer.5.attention.output.dense      | Linear            | 590 K \n",
      "103 | bert.encoder.layer.5.attention.output.LayerNorm  | LayerNorm         | 1.5 K \n",
      "104 | bert.encoder.layer.5.attention.output.dropout    | Dropout           | 0     \n",
      "105 | bert.encoder.layer.5.intermediate                | BertIntermediate  | 2.4 M \n",
      "106 | bert.encoder.layer.5.intermediate.dense          | Linear            | 2.4 M \n",
      "107 | bert.encoder.layer.5.output                      | BertOutput        | 2.4 M \n",
      "108 | bert.encoder.layer.5.output.dense                | Linear            | 2.4 M \n",
      "109 | bert.encoder.layer.5.output.LayerNorm            | LayerNorm         | 1.5 K \n",
      "110 | bert.encoder.layer.5.output.dropout              | Dropout           | 0     \n",
      "111 | bert.encoder.layer.6                             | BertLayer         | 7.1 M \n",
      "112 | bert.encoder.layer.6.attention                   | BertAttention     | 2.4 M \n",
      "113 | bert.encoder.layer.6.attention.self              | BertSelfAttention | 1.8 M \n",
      "114 | bert.encoder.layer.6.attention.self.query        | Linear            | 590 K \n",
      "115 | bert.encoder.layer.6.attention.self.key          | Linear            | 590 K \n",
      "116 | bert.encoder.layer.6.attention.self.value        | Linear            | 590 K \n",
      "117 | bert.encoder.layer.6.attention.self.dropout      | Dropout           | 0     \n",
      "118 | bert.encoder.layer.6.attention.output            | BertSelfOutput    | 592 K \n",
      "119 | bert.encoder.layer.6.attention.output.dense      | Linear            | 590 K \n",
      "120 | bert.encoder.layer.6.attention.output.LayerNorm  | LayerNorm         | 1.5 K \n",
      "121 | bert.encoder.layer.6.attention.output.dropout    | Dropout           | 0     \n",
      "122 | bert.encoder.layer.6.intermediate                | BertIntermediate  | 2.4 M \n",
      "123 | bert.encoder.layer.6.intermediate.dense          | Linear            | 2.4 M \n",
      "124 | bert.encoder.layer.6.output                      | BertOutput        | 2.4 M \n",
      "125 | bert.encoder.layer.6.output.dense                | Linear            | 2.4 M \n",
      "126 | bert.encoder.layer.6.output.LayerNorm            | LayerNorm         | 1.5 K \n",
      "127 | bert.encoder.layer.6.output.dropout              | Dropout           | 0     \n",
      "128 | bert.encoder.layer.7                             | BertLayer         | 7.1 M \n",
      "129 | bert.encoder.layer.7.attention                   | BertAttention     | 2.4 M \n",
      "130 | bert.encoder.layer.7.attention.self              | BertSelfAttention | 1.8 M \n",
      "131 | bert.encoder.layer.7.attention.self.query        | Linear            | 590 K \n",
      "132 | bert.encoder.layer.7.attention.self.key          | Linear            | 590 K \n",
      "133 | bert.encoder.layer.7.attention.self.value        | Linear            | 590 K \n",
      "134 | bert.encoder.layer.7.attention.self.dropout      | Dropout           | 0     \n",
      "135 | bert.encoder.layer.7.attention.output            | BertSelfOutput    | 592 K \n",
      "136 | bert.encoder.layer.7.attention.output.dense      | Linear            | 590 K \n",
      "137 | bert.encoder.layer.7.attention.output.LayerNorm  | LayerNorm         | 1.5 K \n",
      "138 | bert.encoder.layer.7.attention.output.dropout    | Dropout           | 0     \n",
      "139 | bert.encoder.layer.7.intermediate                | BertIntermediate  | 2.4 M \n",
      "140 | bert.encoder.layer.7.intermediate.dense          | Linear            | 2.4 M \n",
      "141 | bert.encoder.layer.7.output                      | BertOutput        | 2.4 M \n",
      "142 | bert.encoder.layer.7.output.dense                | Linear            | 2.4 M \n",
      "143 | bert.encoder.layer.7.output.LayerNorm            | LayerNorm         | 1.5 K \n",
      "144 | bert.encoder.layer.7.output.dropout              | Dropout           | 0     \n",
      "145 | bert.encoder.layer.8                             | BertLayer         | 7.1 M \n",
      "146 | bert.encoder.layer.8.attention                   | BertAttention     | 2.4 M \n",
      "147 | bert.encoder.layer.8.attention.self              | BertSelfAttention | 1.8 M \n",
      "148 | bert.encoder.layer.8.attention.self.query        | Linear            | 590 K \n",
      "149 | bert.encoder.layer.8.attention.self.key          | Linear            | 590 K \n",
      "150 | bert.encoder.layer.8.attention.self.value        | Linear            | 590 K \n",
      "151 | bert.encoder.layer.8.attention.self.dropout      | Dropout           | 0     \n",
      "152 | bert.encoder.layer.8.attention.output            | BertSelfOutput    | 592 K \n",
      "153 | bert.encoder.layer.8.attention.output.dense      | Linear            | 590 K \n",
      "154 | bert.encoder.layer.8.attention.output.LayerNorm  | LayerNorm         | 1.5 K \n",
      "155 | bert.encoder.layer.8.attention.output.dropout    | Dropout           | 0     \n",
      "156 | bert.encoder.layer.8.intermediate                | BertIntermediate  | 2.4 M \n",
      "157 | bert.encoder.layer.8.intermediate.dense          | Linear            | 2.4 M \n",
      "158 | bert.encoder.layer.8.output                      | BertOutput        | 2.4 M \n",
      "159 | bert.encoder.layer.8.output.dense                | Linear            | 2.4 M \n",
      "160 | bert.encoder.layer.8.output.LayerNorm            | LayerNorm         | 1.5 K \n",
      "161 | bert.encoder.layer.8.output.dropout              | Dropout           | 0     \n",
      "162 | bert.encoder.layer.9                             | BertLayer         | 7.1 M \n",
      "163 | bert.encoder.layer.9.attention                   | BertAttention     | 2.4 M \n",
      "164 | bert.encoder.layer.9.attention.self              | BertSelfAttention | 1.8 M \n",
      "165 | bert.encoder.layer.9.attention.self.query        | Linear            | 590 K \n",
      "166 | bert.encoder.layer.9.attention.self.key          | Linear            | 590 K \n",
      "167 | bert.encoder.layer.9.attention.self.value        | Linear            | 590 K \n",
      "168 | bert.encoder.layer.9.attention.self.dropout      | Dropout           | 0     \n",
      "169 | bert.encoder.layer.9.attention.output            | BertSelfOutput    | 592 K \n",
      "170 | bert.encoder.layer.9.attention.output.dense      | Linear            | 590 K \n",
      "171 | bert.encoder.layer.9.attention.output.LayerNorm  | LayerNorm         | 1.5 K \n",
      "172 | bert.encoder.layer.9.attention.output.dropout    | Dropout           | 0     \n",
      "173 | bert.encoder.layer.9.intermediate                | BertIntermediate  | 2.4 M \n",
      "174 | bert.encoder.layer.9.intermediate.dense          | Linear            | 2.4 M \n",
      "175 | bert.encoder.layer.9.output                      | BertOutput        | 2.4 M \n",
      "176 | bert.encoder.layer.9.output.dense                | Linear            | 2.4 M \n",
      "177 | bert.encoder.layer.9.output.LayerNorm            | LayerNorm         | 1.5 K \n",
      "178 | bert.encoder.layer.9.output.dropout              | Dropout           | 0     \n",
      "179 | bert.encoder.layer.10                            | BertLayer         | 7.1 M \n",
      "180 | bert.encoder.layer.10.attention                  | BertAttention     | 2.4 M \n",
      "181 | bert.encoder.layer.10.attention.self             | BertSelfAttention | 1.8 M \n",
      "182 | bert.encoder.layer.10.attention.self.query       | Linear            | 590 K \n",
      "183 | bert.encoder.layer.10.attention.self.key         | Linear            | 590 K \n",
      "184 | bert.encoder.layer.10.attention.self.value       | Linear            | 590 K \n",
      "185 | bert.encoder.layer.10.attention.self.dropout     | Dropout           | 0     \n",
      "186 | bert.encoder.layer.10.attention.output           | BertSelfOutput    | 592 K \n",
      "187 | bert.encoder.layer.10.attention.output.dense     | Linear            | 590 K \n",
      "188 | bert.encoder.layer.10.attention.output.LayerNorm | LayerNorm         | 1.5 K \n",
      "189 | bert.encoder.layer.10.attention.output.dropout   | Dropout           | 0     \n",
      "190 | bert.encoder.layer.10.intermediate               | BertIntermediate  | 2.4 M \n",
      "191 | bert.encoder.layer.10.intermediate.dense         | Linear            | 2.4 M \n",
      "192 | bert.encoder.layer.10.output                     | BertOutput        | 2.4 M \n",
      "193 | bert.encoder.layer.10.output.dense               | Linear            | 2.4 M \n",
      "194 | bert.encoder.layer.10.output.LayerNorm           | LayerNorm         | 1.5 K \n",
      "195 | bert.encoder.layer.10.output.dropout             | Dropout           | 0     \n",
      "196 | bert.encoder.layer.11                            | BertLayer         | 7.1 M \n",
      "197 | bert.encoder.layer.11.attention                  | BertAttention     | 2.4 M \n",
      "198 | bert.encoder.layer.11.attention.self             | BertSelfAttention | 1.8 M \n",
      "199 | bert.encoder.layer.11.attention.self.query       | Linear            | 590 K \n",
      "200 | bert.encoder.layer.11.attention.self.key         | Linear            | 590 K \n",
      "201 | bert.encoder.layer.11.attention.self.value       | Linear            | 590 K \n",
      "202 | bert.encoder.layer.11.attention.self.dropout     | Dropout           | 0     \n",
      "203 | bert.encoder.layer.11.attention.output           | BertSelfOutput    | 592 K \n",
      "204 | bert.encoder.layer.11.attention.output.dense     | Linear            | 590 K \n",
      "205 | bert.encoder.layer.11.attention.output.LayerNorm | LayerNorm         | 1.5 K \n",
      "206 | bert.encoder.layer.11.attention.output.dropout   | Dropout           | 0     \n",
      "207 | bert.encoder.layer.11.intermediate               | BertIntermediate  | 2.4 M \n",
      "208 | bert.encoder.layer.11.intermediate.dense         | Linear            | 2.4 M \n",
      "209 | bert.encoder.layer.11.output                     | BertOutput        | 2.4 M \n",
      "210 | bert.encoder.layer.11.output.dense               | Linear            | 2.4 M \n",
      "211 | bert.encoder.layer.11.output.LayerNorm           | LayerNorm         | 1.5 K \n",
      "212 | bert.encoder.layer.11.output.dropout             | Dropout           | 0     \n",
      "213 | bert.pooler                                      | BertPooler        | 590 K \n",
      "214 | bert.pooler.dense                                | Linear            | 590 K \n",
      "215 | bert.pooler.activation                           | Tanh              | 0     \n",
      "216 | convs                                            | Sequential_6560d8 | 2.4 M \n",
      "217 | convs.module_0                                   | GATConv           | 888 K \n",
      "218 | convs.module_0.lin_src                           | Linear            | 884 K \n",
      "219 | convs.module_1                                   | ReLU              | 0     \n",
      "220 | convs.module_2                                   | GATConv           | 1.3 M \n",
      "221 | convs.module_2.lin_src                           | Linear            | 1.3 M \n",
      "222 | convs.module_3                                   | ReLU              | 0     \n",
      "223 | convs.module_4                                   | GATConv           | 147 K \n",
      "224 | convs.module_4.lin_src                           | Linear            | 147 K \n",
      "225 | convs.module_5                                   | ReLU              | 0     \n",
      "226 | attention                                        | AdditiveAttention | 32.9 K\n",
      "227 | lin                                              | Linear            | 16.5 K\n",
      "228 | dropout                                          | Dropout           | 0     \n",
      "229 | classifier                                       | Linear            | 387   \n",
      "230 | cross_entropy_loss                               | CrossEntropyLoss  | 0     \n",
      "-----------------------------------------------------------------------------------------\n",
      "111 M     Trainable params\n",
      "0         Non-trainable params\n",
      "111 M     Total params\n",
      "447.595   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Global seed set to 12345\n",
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:326: UserWarning: The number of training samples (29) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adaa352455214bb88b56e82ea73708e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 0, global step 28: val_loss reached 0.60386 (best 0.60386), saving model to \"/home/hoang/github/BERT_ABSA/model/laptops/epoch=0-val_loss=0.6039-val_acc=0.7956-val_macro_f1=0.7313-val_micro_f1=0.7956.ckpt\" as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 1, global step 57: val_loss reached 0.60536 (best 0.60386), saving model to \"/home/hoang/github/BERT_ABSA/model/laptops/epoch=1-val_loss=0.6054-val_acc=0.7763-val_macro_f1=0.7356-val_micro_f1=0.7763.ckpt\" as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 2, global step 86: val_loss reached 0.60206 (best 0.60206), saving model to \"/home/hoang/github/BERT_ABSA/model/laptops/epoch=2-val_loss=0.6021-val_acc=0.7841-val_macro_f1=0.7471-val_micro_f1=0.7841.ckpt\" as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 3, global step 115: val_loss was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 4, global step 144: val_loss was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 5, global step 173: val_loss was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 6, global step 202: val_loss was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 7, global step 231: val_loss was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 8, global step 260: val_loss was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 9, global step 289: val_loss was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 10, global step 318: val_loss was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 11, global step 347: val_loss was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py:301: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  rank_zero_warn(\n",
      "Epoch 12, global step 376: val_loss was not in top 3\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Training.')\n",
    "\n",
    "parser.add_argument('-config_file', help='config file path', default='../src/config/laptop_config.json', type=str)\n",
    "parser.add_argument('-f', '--fff', help='a dummy argument to fool ipython', default='1')\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.config = read_json(args.config_file)\n",
    "seed_everything(args.config['data_params']['seed'], workers=True)\n",
    "data, clf = build_model(args.config)\n",
    "trainer, trainer_kwargs = build_trainder(args.config)\n",
    "trainer.fit(clf, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca77e7b0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d95f4b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d0a692",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2273a921",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb62ac8d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 12345\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/hoang/miniconda2/envs/py38/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory ../model/restaurants exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Training.')\n",
    "\n",
    "parser.add_argument('-config_file', help='config file path', default='../src/restaurant_config.json', type=str)\n",
    "parser.add_argument('-f', '--fff', help='a dummy argument to fool ipython', default='1')\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.config = read_json(args.config_file)\n",
    "seed_everything(args.config['data_params']['seed'], workers=True)\n",
    "data, clf = build_model(args.config)\n",
    "trainer, trainer_kwargs = build_trainder(args.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c18f8bad",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7770c45615fa4e92adb87d860b8c70bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'loss': tensor(0.4673, device='cuda:0'), 'acc': tensor(0.8203, dtype=torch.float64), 'macro_f1': tensor(0.7241, dtype=torch.float64), 'micro_f1': tensor(0.8203, dtype=torch.float64)}, {'loss': tensor(0.4060, device='cuda:0'), 'acc': tensor(0.8906, dtype=torch.float64), 'macro_f1': tensor(0.7525, dtype=torch.float64), 'micro_f1': tensor(0.8906, dtype=torch.float64)}, {'loss': tensor(0.4146, device='cuda:0'), 'acc': tensor(0.8359, dtype=torch.float64), 'macro_f1': tensor(0.6356, dtype=torch.float64), 'micro_f1': tensor(0.8359, dtype=torch.float64)}, {'loss': tensor(0.6661, device='cuda:0'), 'acc': tensor(0.7734, dtype=torch.float64), 'macro_f1': tensor(0.6064, dtype=torch.float64), 'micro_f1': tensor(0.7734, dtype=torch.float64)}, {'loss': tensor(0.7138, device='cuda:0'), 'acc': tensor(0.7578, dtype=torch.float64), 'macro_f1': tensor(0.5623, dtype=torch.float64), 'micro_f1': tensor(0.7578, dtype=torch.float64)}, {'loss': tensor(0.7445, device='cuda:0'), 'acc': tensor(0.7500, dtype=torch.float64), 'macro_f1': tensor(0.5685, dtype=torch.float64), 'micro_f1': tensor(0.7500, dtype=torch.float64)}, {'loss': tensor(0.7893, device='cuda:0'), 'acc': tensor(0.7266, dtype=torch.float64), 'macro_f1': tensor(0.5635, dtype=torch.float64), 'micro_f1': tensor(0.7266, dtype=torch.float64)}, {'loss': tensor(0.7322, device='cuda:0'), 'acc': tensor(0.7578, dtype=torch.float64), 'macro_f1': tensor(0.5861, dtype=torch.float64), 'micro_f1': tensor(0.7578, dtype=torch.float64)}, {'loss': tensor(0.8118, device='cuda:0'), 'acc': tensor(0.7708, dtype=torch.float64), 'macro_f1': tensor(0.5916, dtype=torch.float64), 'micro_f1': tensor(0.7708, dtype=torch.float64)}]\n",
      "tensor(0.6384)\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.7870370149612427,\n",
      " 'test_loss': 0.6384071707725525,\n",
      " 'test_macro_f1': 0.6211695671081543,\n",
      " 'test_micro_f1': 0.7870370149612427}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "paths = sorted(glob.glob('/home/hoang/github/BERT_ABSA/model/restaurants/*.ckpt'))\n",
    "model_test = SentimentClassifier.load_from_checkpoint(paths[0])\n",
    "result = trainer.test(model_test, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc1d533",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
